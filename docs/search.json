[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mobile Multimodal Lab",
    "section": "",
    "text": "MobileMultimodalLab (MMLab) is a project initiated by researchers at Donders Center for Cognition. It aims to provide a lab setup for anyone interested in studying multimodal interactive behaviour - including acoustics, body movement, muscle activity, eye movements, and so on.\nTo achieve this, we are working on a comprehensive coding library, accompanied by a practical manual, that shall help researchers to build their own MobileMultimodalLab. Our guiding principles are:\n\nOpen-source resources - All code and documentation is freely available to everyone\nLow-cost equipment - We want to build the setup with as little monetary cost as possible (i.e., less than 10K)\nPortable setup - The setup should be easily transportable across locations\n\n\n\n\nMobileMultimodalLab\n\n\nThe MML setup originally consists of\n\nmultiple frame-synced 2D cameras that allow for 3D motion tracking\nmultiple microphones for acoustic analysis\nmultiple physiological sensors for measuring heart rate, muscle activity, and respiration\n\nTo ensure that all the signals are synchronized, we use the Lab streaming layer (https://github.com/sccn/labstreaminglayer), a software that synchronizes different data streams with sub-millisecond precision, crucially simplifying the data collection process and subsequent processing.\n\n\n\nSetup\n\n\nAdditionally, the setup is build in a modular way, so that anyone can add or remove equipment and recording from the default setup as long as these devices are LSL compatible",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "5_ANIMATIONS/X_VISUALS/MT_animation.html",
    "href": "5_ANIMATIONS/X_VISUALS/MT_animation.html",
    "title": "PREPARATION",
    "section": "",
    "text": "import os\nimport glob\n\ncurfolder = os.getcwd()\nprint(curfolder)\n\n# this is our mt data C:\\Users\\kadava\\Documents\\Github\\FLESH_3Dtracking_new\\projectdata\n## MT\ndatafolder = curfolder + '/TS_motiontracking/'\nvidfolder = curfolder + '/Videos/'\nacfolder = curfolder + '/TS_acoustics/'\n\nmttotrack = glob.glob(datafolder + 'mt*.csv', recursive=True)\nbbtotrack = glob.glob(datafolder + 'bb*.csv', recursive=True)\nidtotrack = glob.glob(datafolder + 'id*.csv', recursive=True)\niktotrack = glob.glob(datafolder + 'ik*.csv', recursive=True)\nvidtotrack = glob.glob(vidfolder + '*.avi', recursive=True)\nenvtotrack = glob.glob(acfolder + 'env*.csv', recursive=True)\nf0totrack = glob.glob(acfolder + 'f0*.csv', recursive=True)\n\nprint(mttotrack)\nprint(idtotrack)\nprint(vidtotrack)\nprint(bbtotrack)\n\n\nc:\\Users\\Sarka Kadava\\Documents\\Github\\FLESH_ContinuousBodilyEffort\\TS_processing\n['c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_0_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_10_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_11_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_12_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_13_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_14_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_15_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_16_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_17_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_18_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_19_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_1_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_20_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_21_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_22_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_23_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_24_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_25_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_26_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_27_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_28_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_29_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_2_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_30_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_31_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_32_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_33_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_34_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_35_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_36_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_37_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_38_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_39_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_3_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_40_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_41_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_42_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_43_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_44_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_45_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_46_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_47_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_48_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_49_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_4_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_50_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_51_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_52_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_53_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_5_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_6_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_7_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_8_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_9_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_tpose_0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_1_tpose_1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_0_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_100_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_101_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_102_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_103_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_104_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_105_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_106_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_107_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_108_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_109_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_10_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_110_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_111_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_112_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_113_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_11_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_12_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_13_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_14_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_15_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_16_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_17_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_18_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_19_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_1_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_20_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_21_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_22_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_23_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_24_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_25_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_26_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_27_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_28_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_29_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_2_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_30_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_31_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_32_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_33_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_34_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_35_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_36_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_37_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_38_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_39_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_3_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_40_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_41_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_42_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_43_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_44_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_45_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_46_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_47_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_48_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_49_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_4_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_50_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_51_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_52_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_53_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_54_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_55_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_56_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_57_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_58_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_59_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_5_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_60_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_61_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_62_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_63_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_64_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_65_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_67_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_68_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_69_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_6_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_70_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_71_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_72_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_73_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_74_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_75_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_76_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_77_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_78_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_79_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_7_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_80_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_81_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_82_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_83_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_84_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_85_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_86_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_87_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_88_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_89_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_8_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_90_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_91_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_92_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_93_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_94_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_95_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_96_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_97_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_98_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_99_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_0_2_9_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_0_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_10_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_11_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_12_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_13_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_14_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_15_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_16_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_17_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_18_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_19_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_1_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_20_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_21_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_22_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_23_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_24_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_25_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_26_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_27_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_28_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_29_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_2_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_30_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_31_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_32_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_33_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_35_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_36_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_37_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_38_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_39_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_3_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_40_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_41_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_42_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_43_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_44_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_45_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_46_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_47_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_48_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_49_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_4_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_50_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_51_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_52_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_53_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_5_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_6_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_7_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_8_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_9_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_tpose_0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_1_tpose_1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_0_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_100_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_101_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_102_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_103_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_104_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_105_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_106_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_107_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_108_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_109_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_10_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_110_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_111_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_112_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_113_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_11_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_12_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_13_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_14_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_15_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_16_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_17_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_18_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_19_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_1_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_20_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_21_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_22_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_23_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_24_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_25_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_26_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_27_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_28_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_29_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_2_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_30_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_31_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_32_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_33_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_34_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_35_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_36_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_37_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_38_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_39_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_3_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_40_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_41_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_43_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_44_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_45_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_46_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_47_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_48_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_49_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_4_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_50_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_51_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_52_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_53_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_54_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_55_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_56_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_57_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_58_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_59_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_5_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_60_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_61_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_62_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_63_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_64_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_65_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_67_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_68_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_69_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_6_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_70_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_71_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_72_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_73_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_74_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_75_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_76_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_77_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_78_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_79_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_7_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_80_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_81_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_82_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_83_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_84_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_85_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_86_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_87_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_88_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_89_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_8_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_90_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_91_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_92_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_93_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_94_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_95_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_96_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_97_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_98_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_99_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\mt_centered_0_2_9_p0.csv']\n['c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_0_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_10_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_11_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_12_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_13_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_14_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_15_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_16_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_17_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_18_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_19_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_1_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_20_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_21_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_22_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_23_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_24_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_25_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_26_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_27_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_28_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_29_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_2_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_30_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_31_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_32_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_33_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_35_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_36_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_37_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_38_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_39_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_3_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_40_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_41_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_42_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_43_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_45_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_46_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_47_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_48_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_49_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_4_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_50_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_51_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_52_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_53_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_5_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_6_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_7_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_8_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_1_9_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_0_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_100_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_101_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_102_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_103_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_104_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_105_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_106_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_107_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_108_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_109_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_10_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_110_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_111_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_112_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_113_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_11_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_12_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_13_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_14_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_15_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_16_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_17_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_18_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_19_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_1_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_20_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_21_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_22_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_23_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_24_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_25_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_26_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_27_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_28_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_29_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_2_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_30_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_31_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_32_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_33_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_34_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_35_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_36_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_37_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_38_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_39_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_3_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_40_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_41_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_42_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_43_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_44_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_45_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_46_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_47_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_48_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_49_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_4_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_50_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_51_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_52_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_53_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_54_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_55_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_56_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_57_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_58_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_59_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_5_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_60_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_61_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_62_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_63_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_64_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_65_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_67_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_68_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_69_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_6_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_70_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_71_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_72_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_73_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_74_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_75_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_76_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_77_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_78_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_79_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_7_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_80_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_81_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_82_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_83_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_84_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_85_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_86_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_87_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_88_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_89_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_8_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_90_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_91_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_92_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_93_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_94_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_95_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_96_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_97_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_98_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_99_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\id_0_2_9_p0.csv']\n['c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_0_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_10_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_11_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_12_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_13_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_14_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_15_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_16_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_17_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_18_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_19_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_1_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_20_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_21_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_22_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_23_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_24_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_25_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_26_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_27_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_28_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_29_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_2_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_30_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_31_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_32_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_33_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_35_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_36_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_37_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_38_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_39_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_3_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_40_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_41_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_42_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_43_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_44_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_45_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_46_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_47_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_48_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_49_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_4_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_50_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_51_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_52_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_53_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_5_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_6_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_7_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_8_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_9_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_tpose_0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_1_tpose_1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_0_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_100_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_101_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_102_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_103_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_104_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_105_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_106_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_107_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_108_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_109_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_10_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_110_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_111_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_112_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_113_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_11_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_12_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_13_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_14_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_15_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_16_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_17_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_18_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_19_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_1_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_20_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_21_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_22_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_23_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_24_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_25_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_26_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_27_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_28_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_29_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_2_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_30_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_31_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_32_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_33_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_34_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_35_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_36_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_37_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_38_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_39_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_3_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_40_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_41_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_43_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_44_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_45_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_46_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_47_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_48_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_49_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_4_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_50_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_51_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_52_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_53_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_54_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_55_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_56_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_57_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_58_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_59_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_5_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_60_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_61_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_62_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_63_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_64_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_65_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_67_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_68_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_69_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_6_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_70_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_71_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_72_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_73_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_74_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_75_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_76_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_77_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_78_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_79_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_7_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_80_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_81_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_82_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_83_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_84_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_85_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_86_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_87_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_88_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_89_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_8_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_90_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_91_p0.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_92_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_93_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_94_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_95_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_96_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_97_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_98_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_99_p1.avi', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/Videos\\\\0_2_9_p0.avi']\n['c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_0_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_10_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_11_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_12_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_13_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_14_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_15_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_16_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_17_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_18_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_19_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_1_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_20_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_21_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_22_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_23_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_24_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_25_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_26_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_27_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_28_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_29_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_2_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_30_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_31_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_32_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_33_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_34_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_35_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_36_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_37_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_38_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_39_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_3_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_40_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_41_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_42_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_43_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_44_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_45_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_46_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_47_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_48_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_49_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_4_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_50_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_51_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_52_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_53_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_5_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_6_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_7_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_8_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_1_9_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_0_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_100_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_101_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_102_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_103_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_104_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_105_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_106_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_107_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_108_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_109_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_10_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_110_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_111_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_112_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_113_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_11_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_12_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_13_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_14_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_15_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_16_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_17_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_18_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_19_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_1_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_20_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_21_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_22_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_23_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_24_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_25_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_26_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_27_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_28_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_29_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_2_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_30_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_31_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_32_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_33_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_34_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_35_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_36_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_37_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_38_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_39_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_3_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_40_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_41_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_42_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_43_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_44_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_45_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_46_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_47_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_48_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_49_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_4_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_50_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_51_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_52_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_53_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_54_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_55_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_56_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_57_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_58_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_59_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_5_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_60_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_61_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_62_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_63_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_64_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_65_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_66_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_67_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_68_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_69_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_6_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_70_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_71_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_72_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_73_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_74_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_75_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_76_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_77_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_78_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_79_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_7_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_80_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_81_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_82_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_83_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_84_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_85_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_86_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_87_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_88_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_89_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_8_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_90_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_91_p0.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_92_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_93_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_94_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_95_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_96_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_97_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_98_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_99_p1.csv', 'c:\\\\Users\\\\Sarka Kadava\\\\Documents\\\\Github\\\\FLESH_ContinuousBodilyEffort\\\\TS_processing/TS_motiontracking\\\\bb_0_2_9_p0.csv']"
  },
  {
    "objectID": "5_ANIMATIONS/X_VISUALS/MT_animation.html#function-to-animate-multimodal-signals",
    "href": "5_ANIMATIONS/X_VISUALS/MT_animation.html#function-to-animate-multimodal-signals",
    "title": "PREPARATION",
    "section": "FUNCTION TO ANIMATE MULTIMODAL SIGNALS",
    "text": "FUNCTION TO ANIMATE MULTIMODAL SIGNALS\n\nimport tempfile\nimport shutil\nimport matplotlib.pyplot as plt\n\n\n# what is the window size in seconds\nwindow = 4\n\n# a function that generates a plot containing two panel time series of the envelope (panel 1) and the pose time series (panel 2) with a window of 4 seconds\ndef plot_body_signals(body, maxbody, minbody, env, maxenv, minenv, bb, maxbb, minbb, midpoint):\n    # make a temporary folder\n    tempfolder = tempfile.mkdtemp()\n    fig, ax = plt.subplots(3, 1, figsize=(14, 10))\n    # selection 4 seconds from midpoint\n    start = midpoint - window/2\n    end = midpoint + window/2\n    # if start is negative, set it to 0\n    if start &lt; 0:\n        start = 0\n    # subset the body time series and amplitude envelope based on the start and end\n    body = body[(body['time'] &gt;= start) & (body['time'] &lt;= end)]\n    env = env[(env['time'] &gt;= start) & (env['time'] &lt;= end)]\n    bb = bb[(bb['time'] &gt;= start) & (bb['time'] &lt;= end)]\n    # do a cross correlation between the envelope and the pose time series\n    # recenter time\n    body['time'] = body['time'] - midpoint\n    env['time'] = env['time'] - midpoint\n    bb['time'] = bb['time'] - midpoint\n    # Plot the amplitude envelope\n    ax[0].plot(env['time'], env['envelope'], label = 'envelope', linewidth=8, color='green')\n    ax[0].legend() # show labels in legend\n    ax[0].legend(prop={'size': 24})\n    # Plot the pose time series with a thick line semitransparent, but different colors (black & grey)\n    ax[1].plot(body['time'], body['arm_torque_sum_change'], label='Arm torque change', color='magenta', alpha=0.8, linewidth=4)\n    #ax[1].set_xlabel('Time (ms)', fontsize=24)\n    ax[1].legend() # show labels in legend\n    ax[1].legend(prop={'size': 24})\n    plt.tight_layout()\n    # plot the bb\n    ax[2].plot(bb['time'], bb['COPc'], label='Center of pressure (change)', color='black', alpha=0.8, linewidth=4)\n    ax[2].set_xlabel('Time (ms)', fontsize=24)\n    ax[2].legend() # show labels in legend\n    ax[2].legend(prop={'size': 24})\n    plt.tight_layout()\n    # set the x axes to centered by 0 minus and plus half the window\n    ax[0].set_xlim(-window/2, window/2)\n    ax[1].set_xlim(-window/2, window/2)\n    ax[2].set_xlim(-window/2, window/2)\n    # add a vertical line in the plot at 0\n    ax[0].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n    ax[1].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n    ax[2].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n\n    # ad horizontal lines at 15 for first plot\n    #ax[0].axhline(y=15, color='r', linestyle='--', linewidth=4, alpha=0.8)\n    # set the y axes \n    # what is the total max and min value in env\n    #maxenv = max(env['env'])\n    #minenv = min(env['env'])\n    ax[0].set_ylim(minenv, maxenv)\n    # speed according to max value within both wrists\n    #maxbody = max(body['RWrist_speed'].max(), body['LWrist_speed'].max())\n    ax[1].set_ylim(minbody, maxbody) #\n    # bb according to max value\n    #maxbb = max(bb['COPc'].max())\n    #minbb = min(bb['COPc'].min())\n    ax[2].set_ylim(minbb, maxbb)\n    \n    # increase font size\n    for a in ax:\n        a.tick_params(axis='both', which='major', labelsize=24)\n        a.tick_params(axis='both', which='major', width=6)\n    # save the plot\n    tpf = tempfolder + 'tempfig.png'\n    plt.savefig(tpf )\n    plt.close()\n    img = cv2.imread(tpf)\n    shutil.rmtree(tempfolder)\n    return img"
  },
  {
    "objectID": "5_ANIMATIONS/X_VISUALS/MT_animation.html#animation-of-multimodal-signals",
    "href": "5_ANIMATIONS/X_VISUALS/MT_animation.html#animation-of-multimodal-signals",
    "title": "PREPARATION",
    "section": "ANIMATION OF MULTIMODAL SIGNALS",
    "text": "ANIMATION OF MULTIMODAL SIGNALS\nFrom https://www.envisionbox.org/embedded_AnimatingSoundMovement.html\n\n\nimport tqdm\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom scipy.signal import savgol_filter\nimport scipy\nimport random\n\nresampl_rate = 200\nsmooth_hz = 8\n\nanifolder = curfolder + '/Animations/'\n\n# keep in idtotrack 5 random files\nrandom.shuffle(idtotrack)\nidtotrack = idtotrack[:5]\n\nfor file in idtotrack:\n    # get the trialid\n    trialid = file.split('\\\\')[-1].split('.')[0]\n    # get rid of the mt_\n    trialid = trialid[3:]\n    print(trialid)\n\n    # load it\n    mt = pd.read_csv(file)\n\n    # convert time to seconds\n    mt['time'] = mt['time']/1000\n\n    # get the max and min values for the speed\n    maxbody = max(mt['arm_torque_sum_change'])\n    minbody = 0\n\n    ###### PREPARING AUDIO\n\n    # get audio file from envtotracki with the same trialid\n    envfile = [x for x in envtotrack if trialid in x][0]\n\n    env = pd.read_csv(envfile)\n\n    # convert time to seconds\n    env['time'] = env['time']/1000\n\n    print(env)\n\n    # get the max and min values for the envelope\n    maxenv = max(env['envelope'])\n    minenv = min(env['envelope'])\n\n    ###### PREPARING BB\n\n    # get bb file bbtotrack with the same trialid\n    bbfile = [x for x in bbtotrack if trialid in x][0]\n\n    bb = pd.read_csv(bbfile)\n\n    # convert time to seconds\n    bb['time'] = bb['time']/1000\n\n    \n    print(bb)\n\n    # get the max and min values for the bb\n    maxbb = max(bb['COPc'])\n    minbb = min(bb['COPc'])\n\n    # PREPARE VIDEO\n    \n    # get the video file with the same trialid\n    videofile = [x for x in vidtotrack if trialid in x][0]\n    print(videofile)\n\n    #audiofilename = inputfol + vidf.replace(\".mp4\", \".wav\")\n    #videofilenamemasked = videofilename.replace(\".mp4\", \"_masked.mp4\")\n    #print(videofilenamemasked)\n    # Get the amplitude envelope\n    #ampv, sr = amp_envelope(audiofilename)\n    # get the raw audio\n    #rawaudio, sr = librosa.load(audiofilename, sr=None)\n\n\n    # load the video in opencv and prepare to loop over it\n    capture = cv2.VideoCapture(videofile)\n    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n    fps = capture.get(cv2.CAP_PROP_FPS)\n    # out\n    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n    out = cv2.VideoWriter(anifolder + '/' + trialid + '_sample_body_multimodal_animated.mp4', fourcc, fps, (int(frameWidth)*4, int(frameHeight)))\n    # loop over the video\n    # loop through the video and add the plot to the video on left upper corner in small inset\n    frame_number = 0\n    # with progress bar\n    for i in tqdm.tqdm(range(int(capture.get(cv2.CAP_PROP_FRAME_COUNT)))):\n        ret, frame = capture.read()\n        if ret == True:\n            img = plot_body_signals(mt, maxbody, minbody, env, maxenv, minenv, bb, maxbb, minbb, frame_number/fps)\n            # resize the image\n            img = cv2.resize(img, (int(frameWidth*3), int(frameHeight)))\n            # lets put the plot on the right side of the frame using concat\n            frame = np.concatenate([frame, img], axis=1)            \n            out.write(frame)\n            frame_number += 1\n            # also show the frame (optional)\n            #cv2.imshow('Frame', frame)\n            #if cv2.waitKey(1) & 0xFF == ord('q'):\n            #   break\n        else:\n            break\n    capture.release()\n    out.release()    \n    cv2.destroyAllWindows()\n\n0_1_9_p1\n            time  audio  envelope   trialID  envelope_change\n0       0.000000    0.0  0.021494  0_1_9_p1    -9.713033e-10\n1       0.000021    0.0  0.021494  0_1_9_p1    -9.688752e-10\n2       0.000042    0.0  0.021494  0_1_9_p1    -9.664477e-10\n3       0.000063    0.0  0.021494  0_1_9_p1    -9.640207e-10\n4       0.000083    0.0  0.021494  0_1_9_p1    -9.615944e-10\n...          ...    ...       ...       ...              ...\n259874  5.414042    0.0  0.021495  0_1_9_p1     2.985620e-06\n259875  5.414062    0.0  0.021495  0_1_9_p1     2.985668e-06\n259876  5.414083    0.0  0.021495  0_1_9_p1     2.985711e-06\n259877  5.414104    0.0  0.021495  0_1_9_p1     2.985749e-06\n259878  5.414125    0.0  0.000000  0_1_9_p1     2.985781e-06\n\n[259879 rows x 5 columns]\n          time  left_back  right_forward  right_back  left_forward     COPXc  \\\n0     0.000000   1.044404       0.906044    1.458037      1.411883  0.000454   \n1     0.002000   1.044434       0.905999    1.458389      1.411379  0.000426   \n2     0.004000   1.044423       0.905888    1.458649      1.410924  0.000393   \n3     0.006000   1.044376       0.905727    1.458832      1.410515  0.000358   \n4     0.008000   1.044301       0.905529    1.458951      1.410149  0.000322   \n...        ...        ...            ...         ...           ...       ...   \n2702  5.404141   1.049476       0.903592    1.466965      1.410135 -0.000282   \n2703  5.406141   1.049545       0.903531    1.466952      1.410152 -0.000172   \n2704  5.408141   1.049670       0.903522    1.467050      1.410145 -0.000032   \n2705  5.410141   1.049860       0.903577    1.467281      1.410106  0.000140   \n2706  5.412142   1.050129       0.903707    1.467672      1.410031  0.000348   \n\n         COPYc      COPc   TrialID                        FileInfo  \n0    -0.000533  0.000700  0_1_9_p1  p1_langzaam_geluiden_corrected  \n1    -0.000567  0.000709  0_1_9_p1  p1_langzaam_geluiden_corrected  \n2    -0.000577  0.000698  0_1_9_p1  p1_langzaam_geluiden_corrected  \n3    -0.000568  0.000671  0_1_9_p1  p1_langzaam_geluiden_corrected  \n4    -0.000544  0.000632  0_1_9_p1  p1_langzaam_geluiden_corrected  \n...        ...       ...       ...                             ...  \n2702  0.000030  0.000284  0_1_9_p1  p1_langzaam_geluiden_corrected  \n2703 -0.000084  0.000191  0_1_9_p1  p1_langzaam_geluiden_corrected  \n2704 -0.000229  0.000231  0_1_9_p1  p1_langzaam_geluiden_corrected  \n2705 -0.000410  0.000433  0_1_9_p1  p1_langzaam_geluiden_corrected  \n2706 -0.000632  0.000722  0_1_9_p1  p1_langzaam_geluiden_corrected  \n\n[2707 rows x 10 columns]\nc:\\Users\\Sarka Kadava\\Documents\\Github\\FLESH_ContinuousBodilyEffort\\TS_processing/Videos\\0_1_9_p1.avi\n0_1_17_p1\n            time     audio  envelope    trialID  envelope_change\n0       0.000000 -0.000031  0.016463  0_1_17_p1    -4.141022e-09\n1       0.000021  0.000031  0.016463  0_1_17_p1    -4.119415e-09\n2       0.000042 -0.000031  0.016463  0_1_17_p1    -4.097816e-09\n3       0.000063  0.000031  0.016463  0_1_17_p1    -4.076222e-09\n4       0.000083 -0.000031  0.016463  0_1_17_p1    -4.054636e-09\n...          ...       ...       ...        ...              ...\n204637  4.263271 -0.000061  0.016504  0_1_17_p1     8.388603e-11\n204638  4.263292  0.000000  0.016504  0_1_17_p1     8.388602e-11\n204639  4.263312  0.000092  0.016504  0_1_17_p1     8.388601e-11\n204640  4.263333 -0.000153  0.016504  0_1_17_p1     8.388600e-11\n204641  4.263354  0.000153  0.016504  0_1_17_p1     8.388599e-11\n\n[204642 rows x 5 columns]\n          time  left_back  right_forward  right_back  left_forward     COPXc  \\\n0     0.000000   1.046164       0.906371    1.468911      1.414711  0.000311   \n1     0.002000   1.045908       0.906415    1.469220      1.414883  0.000246   \n2     0.004000   1.045729       0.906469    1.469438      1.415019  0.000182   \n3     0.006000   1.045615       0.906529    1.469576      1.415128  0.000118   \n4     0.008000   1.045555       0.906591    1.469648      1.415215  0.000057   \n...        ...        ...            ...         ...           ...       ...   \n2127  4.254111   1.032707       0.895223    1.468664      1.394672 -0.000152   \n2128  4.256111   1.032934       0.894993    1.468801      1.394542 -0.000182   \n2129  4.258111   1.033035       0.894622    1.468782      1.394275 -0.000220   \n2130  4.260111   1.032967       0.894063    1.468557      1.393822 -0.000266   \n2131  4.262111   1.032680       0.893265    1.468070      1.393131 -0.000321   \n\n         COPYc      COPc    TrialID                 FileInfo  \n0     0.000079  0.000321  0_1_17_p1  p1_luidruchtig_geluiden  \n1     0.000100  0.000266  0_1_17_p1  p1_luidruchtig_geluiden  \n2     0.000115  0.000215  0_1_17_p1  p1_luidruchtig_geluiden  \n3     0.000125  0.000172  0_1_17_p1  p1_luidruchtig_geluiden  \n4     0.000131  0.000143  0_1_17_p1  p1_luidruchtig_geluiden  \n...        ...       ...        ...                      ...  \n2127 -0.000725  0.000741  0_1_17_p1  p1_luidruchtig_geluiden  \n2128 -0.000723  0.000745  0_1_17_p1  p1_luidruchtig_geluiden  \n2129 -0.000721  0.000754  0_1_17_p1  p1_luidruchtig_geluiden  \n2130 -0.000719  0.000767  0_1_17_p1  p1_luidruchtig_geluiden  \n2131 -0.000718  0.000787  0_1_17_p1  p1_luidruchtig_geluiden  \n\n[2132 rows x 10 columns]\nc:\\Users\\Sarka Kadava\\Documents\\Github\\FLESH_ContinuousBodilyEffort\\TS_processing/Videos\\0_1_17_p1.avi\n0_2_91_p0\n            time  audio  envelope    trialID  envelope_change\n0       0.000000    0.0  0.008579  0_2_91_p0    -2.723590e-11\n1       0.000021    0.0  0.008579  0_2_91_p0    -2.720828e-11\n2       0.000042    0.0  0.008579  0_2_91_p0    -2.718062e-11\n3       0.000063    0.0  0.008579  0_2_91_p0    -2.715294e-11\n4       0.000083    0.0  0.008579  0_2_91_p0    -2.712522e-11\n...          ...    ...       ...        ...              ...\n151126  3.148458    0.0  0.008579  0_2_91_p0     5.947310e-11\n151127  3.148479    0.0  0.008579  0_2_91_p0     5.947309e-11\n151128  3.148500    0.0  0.008579  0_2_91_p0     5.947308e-11\n151129  3.148521    0.0  0.008579  0_2_91_p0     5.947308e-11\n151130  3.148542    0.0  0.008579  0_2_91_p0     5.947307e-11\n\n[151131 rows x 5 columns]\n          time  left_back  right_forward  right_back  left_forward     COPXc  \\\n0     0.000000   1.167883       0.727232    1.682591      1.209081 -0.000502   \n1     0.002000   1.168072       0.727433    1.682287      1.209658 -0.000486   \n2     0.004000   1.168214       0.727583    1.682034      1.210105 -0.000455   \n3     0.006000   1.168320       0.727693    1.681828      1.210444 -0.000414   \n4     0.008000   1.168398       0.727773    1.681666      1.210694 -0.000365   \n...        ...        ...            ...         ...           ...       ...   \n1569  3.138083   1.105138       0.775676    1.619991      1.256946  0.000462   \n1570  3.140083   1.104845       0.776100    1.619892      1.257085  0.000486   \n1571  3.142083   1.104612       0.776549    1.619854      1.257229  0.000504   \n1572  3.144083   1.104454       0.777027    1.619892      1.257386  0.000516   \n1573  3.146083   1.104384       0.777539    1.620017      1.257565  0.000521   \n\n         COPYc      COPc    TrialID              FileInfo  \n0     0.000585  0.000771  0_2_91_p0  p0_jagen_geluiden_c2  \n1     0.000512  0.000706  0_2_91_p0  p0_jagen_geluiden_c2  \n2     0.000445  0.000636  0_2_91_p0  p0_jagen_geluiden_c2  \n3     0.000382  0.000563  0_2_91_p0  p0_jagen_geluiden_c2  \n4     0.000325  0.000488  0_2_91_p0  p0_jagen_geluiden_c2  \n...        ...       ...        ...                   ...  \n1569  0.001068  0.001164  0_2_91_p0  p0_jagen_geluiden_c2  \n1570  0.000982  0.001095  0_2_91_p0  p0_jagen_geluiden_c2  \n1571  0.000874  0.001009  0_2_91_p0  p0_jagen_geluiden_c2  \n1572  0.000743  0.000904  0_2_91_p0  p0_jagen_geluiden_c2  \n1573  0.000583  0.000782  0_2_91_p0  p0_jagen_geluiden_c2  \n\n[1574 rows x 10 columns]\nc:\\Users\\Sarka Kadava\\Documents\\Github\\FLESH_ContinuousBodilyEffort\\TS_processing/Videos\\0_2_91_p0.avi\n0_2_21_p1\n            time  audio  envelope    trialID  envelope_change\n0       0.000000    0.0  0.026419  0_2_21_p1         0.000021\n1       0.000021    0.0  0.026460  0_2_21_p1         0.000021\n2       0.000042    0.0  0.026501  0_2_21_p1         0.000021\n3       0.000063    0.0  0.026542  0_2_21_p1         0.000021\n4       0.000083    0.0  0.026583  0_2_21_p1         0.000021\n...          ...    ...       ...        ...              ...\n223522  4.656708    0.0  0.032840  0_2_21_p1         0.000036\n223523  4.656729    0.0  0.032840  0_2_21_p1         0.000036\n223524  4.656750    0.0  0.032840  0_2_21_p1         0.000036\n223525  4.656771    0.0  0.032840  0_2_21_p1         0.000036\n223526  4.656792    0.0  0.032840  0_2_21_p1         0.000036\n\n[223527 rows x 5 columns]\n          time  left_back  right_forward  right_back  left_forward     COPXc  \\\n0     0.000000   1.093185       0.849433    1.497622      1.392628  0.000289   \n1     0.002000   1.092974       0.849335    1.497603      1.392317  0.000205   \n2     0.004000   1.092805       0.849206    1.497559      1.392048  0.000131   \n3     0.006000   1.092672       0.849052    1.497492      1.391814  0.000067   \n4     0.008000   1.092568       0.848877    1.497406      1.391609  0.000011   \n...        ...        ...            ...         ...           ...       ...   \n2323  4.646122   0.980580       0.958312    1.431339      1.438344  0.000641   \n2324  4.648123   0.981022       0.958940    1.432179      1.438610  0.000753   \n2325  4.650123   0.981483       0.959619    1.433067      1.438833  0.000882   \n2326  4.652123   0.981969       0.960358    1.434008      1.439004  0.001029   \n2327  4.654123   0.982486       0.961162    1.435009      1.439115  0.001194   \n\n         COPYc      COPc    TrialID                   FileInfo  \n0    -0.000062  0.000295  0_2_21_p1  p1_glimlach_combinatie_c2  \n1    -0.000112  0.000234  0_2_21_p1  p1_glimlach_combinatie_c2  \n2    -0.000150  0.000199  0_2_21_p1  p1_glimlach_combinatie_c2  \n3    -0.000176  0.000188  0_2_21_p1  p1_glimlach_combinatie_c2  \n4    -0.000192  0.000192  0_2_21_p1  p1_glimlach_combinatie_c2  \n...        ...       ...        ...                        ...  \n2323 -0.000337  0.000724  0_2_21_p1  p1_glimlach_combinatie_c2  \n2324 -0.000386  0.000846  0_2_21_p1  p1_glimlach_combinatie_c2  \n2325 -0.000446  0.000988  0_2_21_p1  p1_glimlach_combinatie_c2  \n2326 -0.000518  0.001151  0_2_21_p1  p1_glimlach_combinatie_c2  \n2327 -0.000604  0.001338  0_2_21_p1  p1_glimlach_combinatie_c2  \n\n[2328 rows x 10 columns]\nc:\\Users\\Sarka Kadava\\Documents\\Github\\FLESH_ContinuousBodilyEffort\\TS_processing/Videos\\0_2_21_p1.avi\n\n\n  0%|          | 0/324 [00:00&lt;?, ?it/s]C:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  body['time'] = body['time'] - midpoint\nC:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:27: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  env['time'] = env['time'] - midpoint\nC:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  bb['time'] = bb['time'] - midpoint\n100%|██████████| 324/324 [10:38&lt;00:00,  1.97s/it]\n  0%|          | 0/255 [00:00&lt;?, ?it/s]C:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  body['time'] = body['time'] - midpoint\nC:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:27: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  env['time'] = env['time'] - midpoint\nC:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  bb['time'] = bb['time'] - midpoint\n100%|██████████| 255/255 [08:34&lt;00:00,  2.02s/it]\n  0%|          | 0/188 [00:00&lt;?, ?it/s]C:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  body['time'] = body['time'] - midpoint\nC:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:27: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  env['time'] = env['time'] - midpoint\nC:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  bb['time'] = bb['time'] - midpoint\n 82%|████████▏ | 155/188 [06:45&lt;07:04, 12.86s/it]C:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:44: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  plt.tight_layout()\n100%|██████████| 188/188 [07:51&lt;00:00,  2.51s/it]\n  0%|          | 0/280 [00:00&lt;?, ?it/s]C:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  body['time'] = body['time'] - midpoint\nC:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:27: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  env['time'] = env['time'] - midpoint\nC:\\Users\\Sarka Kadava\\AppData\\Local\\Temp\\ipykernel_25268\\4023784591.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  bb['time'] = bb['time'] - midpoint\n 65%|██████▌   | 183/280 [06:20&lt;03:21,  2.08s/it]\n\n\nMemoryError: Unable to allocate 2.64 MiB for an array with shape (2, 173127) and data type float64\n\n\n\n\n\n\n\n\n\n\nimport ffmpeg\n\n# add the audio\n# load in the video again and add the audio using moviepy\nfor vidf in videofiles:\n    print(vidf)\n    # go two folders back in the directory\n    #v = vidf.split('\\\\')[-3] + '/' + vidf.split('\\\\')[-2] + '/'\n    # connect 0-8 element of path vidf\n    v = vidf.split('\\\\')[:8]\n    v = '/'.join(v[:4]) + '/'\n    # find in v mp4 file with _animated\n    animatedfile = glob.glob(v + '*_animated.mp4')[0]\n    print(animatedfile)\n    audiofilename = [x for x in foi_wav if '0_1_trial_35' in x][0]\n    print(audiofilename)\n    input_audio = ffmpeg.input(audiofilename)\n    print(input_audio)\n    print('Loading the video')\n    input_video = ffmpeg.input(animatedfile)\n    print(input_video)\n\n    print('Combining audio and video')\n    output_path = animatedfile.replace('.mp4', '_animated_audio.mp4')\n    ffmpeg.concat(input_video, input_audio, v=1, a=1).output(output_path).run(overwrite_output=True)\n        \n    #save it\n    print('Saving the video')\n\nC:/Users/kadava/Documents/Github/FLESH_3Dtracking_new/projectdata\\Session_0_1\\P1\\0_1_35_p1\\pose-2d-trackingvideos\\video1.avi\nC:/Users/kadava/Documents/Github/FLESH_3Dtracking_new/projectdata/Session_0_1/P1/0_1_35_p1\\0_1_35_p1_sample_wrist_animated.mp4\nE:\\charade_experiment_WORKSPACE\\xdf_procedure\\data\\Data_processed\\Data_trials/Audio_48\\0_1_trial_35_Mic_nominal_srate48000_p1_langzaam_combinatie.wav\ninput(filename='E:\\\\charade_experiment_WORKSPACE\\\\xdf_procedure\\\\data\\\\Data_processed\\\\Data_trials/Audio_48\\\\0_1_trial_35_Mic_nominal_srate48000_p1_langzaam_combinatie.wav')[None] &lt;1de3190524b5&gt;\nLoading the video\ninput(filename='C:/Users/kadava/Documents/Github/FLESH_3Dtracking_new/projectdata/Session_0_1/P1/0_1_35_p1\\\\0_1_35_p1_sample_wrist_animated.mp4')[None] &lt;c1358c19e5aa&gt;\nCombining audio and video\nSaving the video"
  },
  {
    "objectID": "5_ANIMATIONS/X_VISUALS/MT_animation.html#function-to-animate-forces",
    "href": "5_ANIMATIONS/X_VISUALS/MT_animation.html#function-to-animate-forces",
    "title": "PREPARATION",
    "section": "FUNCTION TO ANIMATE FORCES",
    "text": "FUNCTION TO ANIMATE FORCES\n\nimport tempfile\nimport shutil\nimport matplotlib.pyplot as plt\n\n\n# what is the window size in seconds\nwindow = 4\n\n# a function that generates a plot containing two panel time series of the envelope (panel 1) and the pose time series (panel 2) with a window of 4 seconds\ndef plot_forces(marker1, maxmarker1, minmarker1, marker2, maxmarker2, minmarker2, marker3, maxmarker3, minmarker3, marker4, maxmarker4, minmarker4, midpoint):\n    # make a temporary folder\n    tempfolder = tempfile.mkdtemp()\n    fig, ax = plt.subplots(4, 1, figsize=(14, 10))\n    # selection 4 seconds from midpoint\n    start = midpoint - window/2\n    end = midpoint + window/2\n    # if start is negative, set it to 0\n    if start &lt; 0:\n        start = 0\n    # subset the body time series and amplitude envelope based on the start and end\n    marker1 = marker1[(marker1['Time'] &gt;= start) & (marker1['Time'] &lt;= end)]\n    marker2 = marker2[(marker2['time'] &gt;= start) & (marker2['time'] &lt;= end)]\n    marker3 = marker3[(marker3['time'] &gt;= start) & (marker3['time'] &lt;= end)]\n    marker4 = marker4[(marker4['time'] &gt;= start) & (marker4['time'] &lt;= end)]\n    \n    # do a cross correlation between the envelope and the pose time series\n    # recenter time\n    marker1['Time'] = marker1['Time'] - midpoint\n    marker2['time'] = marker2['time'] - midpoint\n    marker3['time'] = marker3['time'] - midpoint\n    marker4['time'] = marker4['time'] - midpoint\n\n    # Plot SPEED\n    ax[0].plot(marker1['Time'], marker1['RElbow_speed'], label='RElbow_speed', color='magenta', alpha=0.8, linewidth=4)\n    ax[0].plot(marker1['Time'], marker1['LElbow_speed'], label='LElbow_speed', color='darkblue', alpha=0.8, linewidth=4)\n    ax[0].legend() # show labels in legend\n    ax[0].legend(prop={'size': 24})\n    plt.tight_layout()\n\n    # Plot ARM_FLEX MOMENT\n    ax[1].plot(marker2['time'], marker2['elbow_flex_r_moment'], label = 'elbow_flex_r_moment', linewidth=4, color='magenta')\n    ax[1].plot(marker2['time'], marker2['elbow_flex_l_moment'], label = 'elbow_flex_l_moment', linewidth=4, color='darkblue')\n    ax[1].legend() # show labels in legend\n    ax[1].legend(prop={'size': 24})\n\n    \n    # Plot PELVIS\n    ax[2].plot(marker3['time'], marker3['pelvis_rotation_moment'], label='pelvis_rotation_moment', color='green', alpha=0.8, linewidth=4)\n    #ax[2].plot(marker3['time'], marker3['elbow_add_l_moment'], label='elbow_add_l_moment', color='darkblue', alpha=0.8, linewidth=4)\n    ax[2].legend() # show labels in legend\n    ax[2].legend(prop={'size': 24})\n    plt.tight_layout()\n\n    # plot NECK\n    ax[3].plot(marker4['time'], marker4['neck_rotation_moment'], label='neck_rotation_moment', color='black', alpha=0.8, linewidth=4)\n    #ax[3].plot(marker4['time'], marker4['elbow_rot_l_moment'], label='elbow_rot_l_moment', color='darkblue', alpha=0.8, linewidth=4)\n    ax[3].set_xlabel('Time (ms)', fontsize=24)\n    ax[3].legend() # show labels in legend\n    ax[3].legend(prop={'size': 24})\n    plt.tight_layout()\n\n    # set the x axes to centered by 0 minus and plus half the window\n    ax[0].set_xlim(-window/2, window/2)\n    ax[1].set_xlim(-window/2, window/2)\n    ax[2].set_xlim(-window/2, window/2)\n    ax[3].set_xlim(-window/2, window/2)\n\n    # add a vertical line in the plot at 0\n    ax[0].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n    ax[1].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n    ax[2].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n    ax[3].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n\n    # set the y axes\n    ax[0].set_ylim(minmarker1, maxmarker1) #\n    ax[1].set_ylim(minmarker2, maxmarker2)\n    ax[2].set_ylim(minmarker3, maxmarker3)\n    ax[3].set_ylim(minmarker4, maxmarker4)\n    \n    # increase font size\n    for a in ax:\n        a.tick_params(axis='both', which='major', labelsize=24)\n        a.tick_params(axis='both', which='major', width=6)\n    # save the plot\n    tpf = tempfolder + 'tempfig.png'\n    plt.savefig(tpf )\n    plt.close()\n    img = cv2.imread(tpf)\n    shutil.rmtree(tempfolder)\n    return img"
  },
  {
    "objectID": "5_ANIMATIONS/X_VISUALS/MT_animation.html#animation-of-forces",
    "href": "5_ANIMATIONS/X_VISUALS/MT_animation.html#animation-of-forces",
    "title": "PREPARATION",
    "section": "ANIMATION OF FORCES",
    "text": "ANIMATION OF FORCES\n\n\nimport tqdm\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nanifolder = curfolder + '/Animations/'\n\n# from mttotrack, get rid of all files with tpose in it\nmttotrack = [x for x in mttotrack if 'tpose' not in x]\n\nfor file in mttotrack:\n    # get the trialid\n    trialid = file.split('\\\\')[-1].split('.')[0]\n    # get rid of the mt_\n    trialid = trialid[3:]\n    print(trialid)\n\n    # load it\n    mt = pd.read_csv(file)\n\n    # convert time to seconds\n    mt['Time'] = mt['Time']/1000\n\n    # get the max and min values for the speed\n    maxbody = max(mt['RElbow_speed'].max(), mt['LElbow_speed'].max())\n    minbody = 0\n\n    ###### PREPARING FORCES\n\n    # get audio file from envtotracki with the same trialid\n    idfile = [x for x in idtotrack if trialid in x][0]\n\n    id = pd.read_csv(idfile)\n\n    # convert time to seconds\n    id['time'] = id['time']/1000\n\n    # get the max and min values for elbow flex\n    maxelb = max(id['elbow_flex_r_moment'].max(), id['elbow_flex_l_moment'].max())\n    minelb = min(id['elbow_flex_r_moment'].min(), id['elbow_flex_l_moment'].min())\n\n    # get max min for pelvis \n    maxpel = max(id['pelvis_rotation_moment'])\n    minpel = min(id['pelvis_rotation_moment'])\n\n    # get max min for \n    maxneck = max(id['neck_rotation_moment'])\n    minneck = min(id['neck_rotation_moment'])\n\n\n    # PREPARE VIDEO\n    \n    # get the video file with the same trialid\n    videofile = [x for x in vidtotrack if trialid in x][0]\n    print(videofile)\n\n    # load the video in opencv and prepare to loop over it\n    capture = cv2.VideoCapture(videofile)\n    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n    fps = capture.get(cv2.CAP_PROP_FPS)\n    # out\n    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n\n    outputpath = anifolder + '/' + trialid + '_sample_body_forces_animated.mp4'\n    # check if the file exists\n    if os.path.exists(outputpath):\n        print('File exists')\n        continue\n    else:\n        out = cv2.VideoWriter(outputpath, fourcc, fps, (int(frameWidth)*4, int(frameHeight)))\n        # loop over the video\n        frame_number = 0\n        # with progress bar\n        for i in tqdm.tqdm(range(int(capture.get(cv2.CAP_PROP_FRAME_COUNT)))):\n            ret, frame = capture.read()\n            if ret == True:\n                img = plot_forces(mt, maxbody, minbody, id, maxelb, minelb, id, maxpel, minpel, id, maxneck, minneck, frame_number/fps)\n                # resize the image\n                img = cv2.resize(img, (int(frameWidth*3), int(frameHeight)))\n                # lets put the plot on the right side of the frame using concat\n                frame = np.concatenate([frame, img], axis=1)            \n                out.write(frame)\n                frame_number += 1\n                # also show the frame (optional)\n                #cv2.imshow('Frame', frame)\n                #if cv2.waitKey(1) & 0xFF == ord('q'):\n                #   break\n            else:\n                break\n        capture.release()\n        out.release()    \n        cv2.destroyAllWindows()"
  },
  {
    "objectID": "4b_SynchronyAnalyses/processing_and_visualization.html",
    "href": "4b_SynchronyAnalyses/processing_and_visualization.html",
    "title": "MMLAB PLOTS MULTIMODAL PLOT BONANZA",
    "section": "",
    "text": "We lean on several key measurments here to determine relationships between modalities.\n\nCoherence: Spectral coherence is a measure of the correlation between two signals in the frequency domain.\nCross-correlation: Cross-correlation is a measure of the similarity between two signals as a function of the time-lag applied to one of them.\nMutual information: Mutual information quantifies the amount of information obtained about one random variable through another random variable. It is a measure of dependency, linear or non-linear.\nPhase locking value (PLV): PLV is a measure of the consistency of phase differences between two signals over time. It is often used in neuroscience to assess functional connectivity between brain regions.\nPhases: Phase analysis involves examining the phase relationships between signals, which can provide insights into synchronization and timing relationships.\nCorrelation: Expression of linear relationships between two variables (this may be used if we want to look at whether more coupling between p1-p2 for modality X is predictive of more coupling in modality Y).\n\n\nimport pandas as pd\nimport numpy as np\nimport glob\nimport os\nfrom scipy import signal\nfrom scipy import stats\nfrom sklearn.metrics import mutual_info_score\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom matplotlib.patches import FancyArrowPatch\nimport matplotlib.patches as mpatches\n\n# Your existing functions (keeping them as they are)\ndef compute_coherence(x, y, fs):\n    f, Cxy = signal.coherence(x, y, fs)\n    return f, Cxy\n\ndef crosscorr(datax, datay, lag=0):\n    \"\"\" Lag-N cross correlation. Shift datax by N elements. \"\"\"\n    return datax.corr(datay.shift(lag))\n\ndef compute_mutual_information(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute mutual information between two time series using KDE.\n    \"\"\"\n    # Clean data\n    mask = ~np.isnan(x) & ~np.isnan(y)\n    x = x[mask]\n    y = y[mask]\n    \n    if len(x) &lt; 2 or len(y) &lt; 2:\n        return 0.0\n    \n    # Standardize the data\n    x = (x - np.mean(x)) / np.std(x)\n    y = (y - np.mean(y)) / np.std(y)\n    \n    # Create KDE estimators\n    kde_joint = stats.gaussian_kde(np.vstack([x, y]))\n    kde_x = stats.gaussian_kde(x)\n    kde_y = stats.gaussian_kde(y)\n    \n    # Sample points for numerical integration\n    n_samples = 50\n    x_range = np.linspace(min(x) - 1, max(x) + 1, n_samples)\n    y_range = np.linspace(min(y) - 1, max(y) + 1, n_samples)\n    X, Y = np.meshgrid(x_range, y_range)\n    positions = np.vstack([X.ravel(), Y.ravel()])\n    \n    # Evaluate densities\n    joint_density = kde_joint(positions).reshape(X.shape)\n    x_density = kde_x(X[0,:])\n    y_density = kde_y(Y[:,0])\n    X_density, Y_density = np.meshgrid(x_density, y_density)\n    \n    # Compute MI\n    with np.errstate(divide='ignore', invalid='ignore'):\n        mi_density = joint_density * np.log(joint_density / (X_density * Y_density))\n    mi = np.nansum(mi_density) * (x_range[1] - x_range[0]) * (y_range[1] - y_range[0])\n    \n    return max(0, mi)  # Ensure non-negative MI\n\ndef compute_coupling_statistics(name, motion_ts, sound_ts, time):\n    \"\"\"Your existing coupling statistics function\"\"\"\n    # Ensure inputs are numpy arrays\n    motion_ts = np.array(motion_ts)\n    sound_ts = np.array(sound_ts)\n    time = np.array(time)\n\n    # Check if inputs are scalar (single values)\n    if motion_ts.ndim == 0 or sound_ts.ndim == 0 or time.ndim == 0:\n        return pd.DataFrame({\n            'scene': [name],\n            'lags': [np.nan],\n            'crosscorr': [np.nan],\n        }), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n\n    # Check if inputs have the same length\n    if len(motion_ts) != len(sound_ts) or len(motion_ts) != len(time):\n        raise ValueError(\"motion_ts, sound_ts, and time must have the same length\")\n\n    # normalize and center the data\n    motion_ts = (motion_ts - np.min(motion_ts)) / (np.max(motion_ts) - np.min(motion_ts))\n    motion_ts = motion_ts - np.mean(motion_ts)\n    sound_ts = (sound_ts - np.min(sound_ts)) / (np.max(sound_ts) - np.min(sound_ts))\n    sound_ts = sound_ts - np.mean(sound_ts)\n\n    # check if values are finite\n    if not np.all(np.isfinite(motion_ts)) or not np.all(np.isfinite(sound_ts)):\n        return pd.DataFrame({\n            'scene': [name],\n            'lags': [np.nan],\n            'crosscorr': [np.nan],\n        }), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n\n    # Compute sampling frequency\n    fs = 1/np.mean(np.diff(time))\n\n    # compute the average mutual information\n    mi = []\n    lags = [-0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3]\n    fs = 1/np.mean(np.diff(time))\n    lags_samples = [int(lag * fs) for lag in lags]\n    \n    for lag in lags_samples:\n        if lag &lt; 0:\n            motsub = motion_ts[:lag]\n            soundsub = sound_ts[-lag:]\n        elif lag &gt; 0:\n            motsub = motion_ts[lag:]\n            soundsub = sound_ts[:-lag]\n        else:\n            motsub = motion_ts\n            soundsub = sound_ts\n            \n        # Add smoothing to capture temporal dependencies\n        window = int(0.1 * fs)  # 100ms window\n        if window &gt; 1:\n            motsub = np.convolve(motsub, np.ones(window)/window, mode='valid')\n            soundsub = np.convolve(soundsub, np.ones(window)/window, mode='valid')\n        \n        mi_value = compute_mutual_information(motsub, soundsub)\n        mi.append(mi_value)\n    \n    max_mi = np.max(mi)\n    optimal_lag = lags[np.argmax(mi)]\n    \n    #################################################### Coherence\n    # Compute coherence\n    f, Cxy = compute_coherence(motion_ts, sound_ts, fs)\n\n    # keep all values lower than 10 Hz\n    mask = f &lt; 10\n    Cxy = Cxy[mask]\n    f = f[mask]\n\n    # maximum coherence\n    coherence = np.max(Cxy)\n\n    # Frequency of max coherence\n    f_max = f[np.argmax(Cxy)]\n        \n    #################################################### Cross-correlation\n    lag_seconds = 0.3\n    lag_points = int(lag_seconds * fs)\n\n    # Create a range of lags to examine\n    lags = np.arange(-lag_points, lag_points + 1)\n\n    # calc cross-cor\n    cc = [crosscorr(pd.Series(motion_ts), pd.Series(sound_ts), lag) for lag in lags]\n\n    # Calculate cross-correlation\n    crosscorrdf = pd.DataFrame({\n        'scene': name,\n        'lags': lags*(1/fs),\n        'crosscorr': cc,\n    })\n\n    ########## Phase locking value\n    # Compute analytic signal (using Hilbert transform)\n    motion_analytic = signal.hilbert(motion_ts)\n    sound_analytic = signal.hilbert(sound_ts)\n\n    # Extract instantaneous phase\n    motion_phase = np.angle(motion_analytic)\n    sound_phase = np.angle(sound_analytic)\n\n    # Compute phase difference\n    phase_diff = motion_phase - sound_phase\n\n    # compute the plv\n    plv = np.abs(np.mean(np.exp(1j * phase_diff)))\n    \n    # make phase_diff a regular list not a numpy array\n    try:\n        motion_phase = motion_phase.tolist()[0] if hasattr(motion_phase, 'tolist') else motion_phase[0]\n        phase_diff = phase_diff.tolist()[0] if hasattr(phase_diff, 'tolist') else phase_diff[0]\n    except:\n        motion_phase = np.mean(motion_phase)\n        phase_diff = np.mean(phase_diff)\n\n    return crosscorrdf, coherence, f_max, max_mi, phase_diff, plv, optimal_lag\n\ndef calculate_p1_p2_coupling_stats(merged_folder='../merged_filteredtimeseries/', output_file='p1_p2_coupling_statistics.csv'):\n    \"\"\"\n    Loop through merged time series files and calculate coupling statistics between P1 and P2\n    \"\"\"\n    \n    # Find all merged CSV files\n    csv_files = glob.glob(os.path.join(merged_folder, \"*.csv\"))\n    print(f\"Found {len(csv_files)} files to process\")\n    \n    # Define P1-P2 variable pairs to analyze\n    p1_modalities = ['Amplitude_Envelope_P1', 'Filtered_ECG_P1', 'Filtered_Respiration_P1', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1']\n    p2_modalities = ['Amplitude_Envelope_P2', 'Filtered_ECG_P2', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n    variable_pairs = []\n    # Add same-participant, same-modality (if needed for matrix)\n    for i in range(len(p1_modalities)):\n        p1_var = p1_modalities[i]\n        p2_var = p2_modalities[i] # Assumes corresponding P1/P2 vars are at same index\n        if p1_var.replace('_P1', '') == p2_var.replace('_P2', ''): # Only if same modality\n            variable_pairs.append((p1_var, p2_var))\n\n    # Add ALL cross-participant pairs (same and cross-modality)\n    for p1_var in p1_modalities:\n        for p2_var in p2_modalities:\n            if (p1_var.replace('_P1', '') == p2_var.replace('_P2', '')) and (p1_var.replace('_P1', '') == 'Amplitude_Envelope'):\n                # This is the P1-P2 envelope, which is handled separately in plotting\n                # You might want to exclude it here if you only want to compute it once for the scatter plots\n                pass\n            else:\n                variable_pairs.append((p1_var, p2_var))\n    \n    # Initialize results list\n    results = []\n    \n    # Loop through each file\n    for file_path in csv_files:\n        print(f\"\\nProcessing: {os.path.basename(file_path)}\")\n        \n        try:\n            # Load the data\n            df = pd.read_csv(file_path)\n            \n            # Extract metadata from filename and dataframe\n            filename = os.path.basename(file_path)\n            \n            # Try to extract condition information\n                # if filename has NoVision or NoMovement, set to 'NoVision' or 'NoMovement'\n            condition_vision = 'Vision' if 'NoVision' not in filename else 'NoVision'\n            condition_movement = 'Movement' if 'NoMovement' not in filename else 'NoMovement'\n            trial = df['Trial'].iloc[0] if 'Trial' in df.columns else 'Unknown'\n            \n            # Get time vector\n            time = df['Time'].values\n            \n            # Check if we have enough data (at least 3 seconds)\n            if time.max() - time.min() &lt; 5.0:\n                print(f\"  Skipping {filename}: insufficient data duration ({time.max() - time.min():.2f}s)\")\n                continue\n            \n            # Set up sliding window analysis\n            window_duration = 5.0  # seconds\n            step_size = 1.0  # seconds\n            n_windows = int((time.max() - time.min() - window_duration) / step_size) + 1\n            \n            print(f\"  Analyzing {n_windows} windows of {window_duration}s each\")\n            \n            # Loop through each variable pair\n            for var1, var2 in variable_pairs:\n                if var1 not in df.columns or var2 not in df.columns:\n                    continue\n                    \n                print(f\"    Processing {var1} vs {var2}\")\n                \n                # Loop through sliding windows\n                for window_idx in range(n_windows):\n                    window_start = time.min() + window_idx * step_size\n                    window_end = window_start + window_duration\n                    \n                    # Extract window data\n                    window_mask = (time &gt;= window_start) & (time &lt;= window_end)\n                    time_window = time[window_mask]\n                    var1_window = df[var1].values[window_mask]\n                    var2_window = df[var2].values[window_mask]\n                    \n                    # Skip if not enough data in window or too many NaNs\n                    if len(time_window) &lt; 100 or np.sum(~np.isnan(var1_window)) &lt; 50 or np.sum(~np.isnan(var2_window)) &lt; 50:\n                        continue\n                    \n                    try:\n                        # Calculate coupling statistics\n                        crosscorrdf, coherence, f_max, max_mi, phase_diff, plv, optimal_lag = \\\n                            compute_coupling_statistics(filename, var1_window, var2_window, time_window)\n                        \n                        # Extract max cross-correlation\n                        max_crosscorr = crosscorrdf['crosscorr'].abs().max() if not crosscorrdf.empty else np.nan\n                        lag_at_max_crosscorr = crosscorrdf.loc[crosscorrdf['crosscorr'].abs().idxmax(), 'lags'] if not crosscorrdf.empty else np.nan\n                        \n                        # Store results\n                        result = {\n                            'filename': filename,\n                            'condition_vision': condition_vision,\n                            'condition_movement': condition_movement,\n                            'trial': trial,\n                            'window_idx': window_idx,\n                            'window_start': window_start,\n                            'window_end': window_end,\n                            'var1': var1,\n                            'var2': var2,\n                            'variable_pair_type': classify_variable_pair(var1, var2),\n                            'max_crosscorr': max_crosscorr,\n                            'lag_at_max_crosscorr': lag_at_max_crosscorr,\n                            'max_coherence': coherence,\n                            'freq_at_max_coherence': f_max,\n                            'max_mutual_info': max_mi,\n                            'optimal_lag_mi': optimal_lag,\n                            'phase_locking_value': plv,\n                            'mean_phase_diff': phase_diff,\n                            'n_samples': len(time_window),\n                            'sampling_rate': 1/np.mean(np.diff(time_window))\n                        }\n                        \n                        results.append(result)\n                        \n                    except Exception as e:\n                        print(f\"      Error in window {window_idx}: {e}\")\n                        continue\n                        \n        except Exception as e:\n            print(f\"  Error processing {filename}: {e}\")\n            continue\n    \n    # Convert to DataFrame\n    if results:\n        results_df = pd.DataFrame(results)\n        \n        # Save to CSV\n        results_df.to_csv(output_file, index=False)\n        print(f\"\\n✓ Saved {len(results_df)} coupling statistics to {output_file}\")\n        \n        # Print summary statistics\n        print(f\"\\nSummary:\")\n        print(f\"  Files processed: {results_df['filename'].nunique()}\")\n        print(f\"  Variable pairs analyzed: {results_df['variable_pair_type'].nunique()}\")\n        print(f\"  Total windows analyzed: {len(results_df)}\")\n        print(f\"  Conditions: {results_df['condition_vision'].unique()} x {results_df['condition_movement'].unique()}\")\n        \n        return results_df\n    else:\n        print(\"No results generated - check your data files and variable names\")\n        return pd.DataFrame()\n\ndef classify_variable_pair(var1, var2):\n    \"\"\"\n    Classify the type of variable pair for easier analysis\n    \"\"\"\n    # Extract modality information\n    modalities = []\n    participants = []\n    \n    for var in [var1, var2]:\n        if 'Amplitude_Envelope' in var:\n            modalities.append('Audio')\n        elif 'heart_rate' in var:\n            modalities.append('ECG')\n        elif 'Respiration' in var:\n            modalities.append('Respiration')\n        elif 'EMG' in var:\n            modalities.append('EMG')\n        elif 'right_index' in var:\n            modalities.append('Motion')\n        else:\n            modalities.append('Other')\n            \n        if '_P1' in var:\n            participants.append('P1')\n        elif '_P2' in var:\n            participants.append('P2')\n        else:\n            participants.append('Unknown')\n    \n    # Classify pair type\n    if participants[0] == participants[1]:\n        if participants[0] == 'P1':\n            pair_type = 'Within_P1'\n        else:\n            pair_type = 'Within_P2'\n    else:\n        pair_type = 'Between_Participants'\n    \n    if modalities[0] == modalities[1]:\n        modality_type = f\"Same_{modalities[0]}\"\n    else:\n        modality_type = f\"Cross_{modalities[0]}_{modalities[1]}\"\n    \n    return f\"{pair_type}_{modality_type}\"\n\n\ndef create_modality_network_plots(results_df, output_folder='coupling_plots/'):\n    \"\"\"\n    Create network visualizations showing P1-P2 modality coupling\n    Network 1: P1 and P2 modalities as separate nodes with directed edges\n    Network 2: Envelope coupling vs all other P1-P2 coupling pairs\n    \"\"\"\n    \n    os.makedirs(output_folder, exist_ok=True)\n    \n    # Define consistent color scheme for conditions\n    CONDITION_COLORS = {\n        'NoVision x Movement': '#e41a1c',      # Red\n        'NoVision x NoMovement': '#377eb8',    # Blue  \n        'Vision x Movement': '#4daf4a',        # Green\n        'Vision x NoMovement': '#984ea3'       # Purple\n    }\n    \n    def get_modality_from_var(var_name):\n        if 'Amplitude_Envelope' in var_name:\n            return 'Audio'\n        elif 'Filtered_ECG' in var_name:  # Added Filtered_ECG mapping\n            return 'Heart Rate'\n        elif 'Filtered_Respiration' in var_name:\n            return 'Respiration'\n        elif 'Filtered_EMG_Bicep' in var_name:\n            return 'EMG Bicep'\n        elif 'Filtered_EMG_Tricep' in var_name:\n            return 'EMG Tricep'\n        elif 'right_index_x' in var_name:\n            return 'Motion X'\n        elif 'right_index_y' in var_name:\n            return 'Motion Y'\n        elif 'right_index_z' in var_name:\n            return 'Motion Z'\n        else:\n            return 'Unknown'\n    \n    # Get P1-P2 coupling data\n    p1_p2_data = results_df[\n        (results_df['var1'].str.contains('_P1')) &\n        (results_df['var2'].str.contains('_P2'))\n    ].copy()\n    \n    if len(p1_p2_data) == 0:\n        print(\"No P1-P2 coupling data found!\")\n        return\n    \n    # Add modality information\n    p1_p2_data['modality1'] = p1_p2_data['var1'].apply(get_modality_from_var)\n    p1_p2_data['modality2'] = p1_p2_data['var2'].apply(get_modality_from_var)\n    \n    # Get conditions\n    conditions = p1_p2_data.groupby(['condition_vision', 'condition_movement']).size().reset_index()[['condition_vision', 'condition_movement']]\n    \n    # ============================================\n    # CALCULATE GLOBAL MIN/MAX VALUES FOR CONSISTENT SCALING\n    # ============================================\n    \n    print(\"\\n=== Calculating Global Min/Max for Consistent Scaling ===\")\n    \n    # Collect ALL MI values across all conditions for Network 1\n    all_mi_values_network1 = []\n    \n    for _, cond_row in conditions.iterrows():\n        vision = cond_row['condition_vision']\n        movement = cond_row['condition_movement']\n        \n        cond_data = p1_p2_data[\n            (p1_p2_data['condition_vision'] == vision) & \n            (p1_p2_data['condition_movement'] == movement)\n        ]\n        \n        if len(cond_data) &gt; 0:\n            valid_mi = cond_data['max_mutual_info'].dropna().values\n            all_mi_values_network1.extend(valid_mi)\n    \n    global_mi_min = 0  # MI is always &gt;= 0\n    global_mi_max = np.percentile(all_mi_values_network1, 95) if len(all_mi_values_network1) &gt; 0 else 2.0\n    \n    print(f\"Global MI range for edges: {global_mi_min:.3f} - {global_mi_max:.3f}\")\n    \n    # ============================================\n    # NETWORK 1: P1-P2 MODALITY NETWORKS WITH JITTER PLOTS (GLOBAL SCALING)\n    # ============================================\n    \n    print(\"\\n=== Creating P1-P2 Modality Networks with Jitter Plots (Global Scaling) ===\")\n    \n    for _, cond_row in conditions.iterrows():\n        vision = cond_row['condition_vision']\n        movement = cond_row['condition_movement']\n        cond_name = f\"{vision} x {movement}\"\n        \n        print(f\"  Creating network with jitter plot for: {cond_name}\")\n        \n        # Get data for this condition\n        cond_data = p1_p2_data[\n            (p1_p2_data['condition_vision'] == vision) & \n            (p1_p2_data['condition_movement'] == movement)\n        ]\n        \n        if len(cond_data) == 0:\n            continue\n        \n        # Create subplot layout: network on left, jitter plot on right\n        fig, (ax_net, ax_jitter) = plt.subplots(1, 2, figsize=(24, 12))\n        \n        # ============================================\n        # LEFT PANEL: NETWORK PLOT\n        # ============================================\n        \n        # Create directed graph\n        G = nx.DiGraph()\n        \n        # Get unique modalities\n        modalities = sorted(list(set(cond_data['modality1'].unique()) | \n                                set(cond_data['modality2'].unique())))\n        modalities = [m for m in modalities if m != 'Unknown']\n        \n        # Add nodes for P1 and P2 versions of each modality\n        for modality in modalities:\n            G.add_node(f\"{modality}_P1\", participant='P1', modality=modality)\n            G.add_node(f\"{modality}_P2\", participant='P2', modality=modality)\n        \n        # Add edges based on coupling strength\n        edge_weights = []\n        for _, row in cond_data.iterrows():\n            p1_mod = row['modality1']\n            p2_mod = row['modality2']\n            mi_value = row['max_mutual_info']\n            \n            if not np.isnan(mi_value) and p1_mod != 'Unknown' and p2_mod != 'Unknown':\n                G.add_edge(f\"{p1_mod}_P1\", f\"{p2_mod}_P2\", weight=mi_value)\n                edge_weights.append(mi_value)\n        \n        if len(edge_weights) &gt; 0:\n            # Position nodes in two columns (P1 left, P2 right)\n            pos = {}\n            y_positions = np.linspace(1, 0, len(modalities))\n            \n            for i, modality in enumerate(modalities):\n                pos[f\"{modality}_P1\"] = (0, y_positions[i])\n                pos[f\"{modality}_P2\"] = (1, y_positions[i])\n            \n            # Node colors and sizes\n            node_colors = []\n            node_sizes = []\n            for node in G.nodes():\n                if '_P1' in node:\n                    node_colors.append('lightblue')\n                else:\n                    node_colors.append('lightcoral')\n                node_sizes.append(3000)\n            \n            # Edge colors and widths based on MI strength (GLOBAL MIN/MAX)\n            edges = G.edges()\n            edge_weights = [G[u][v]['weight'] for u, v in edges]\n            \n            # Calculate normalized values once for consistent scaling\n            if len(edge_weights) &gt; 0 and global_mi_max &gt; global_mi_min:\n                normalized_values = [(w - global_mi_min) / (global_mi_max - global_mi_min) for w in edge_weights]\n                edge_widths = [1 + 8 * norm_val for norm_val in normalized_values]  # 1-9 width range\n                edge_colors = [plt.cm.viridis(norm_val) for norm_val in normalized_values]\n            else:\n                edge_widths = [5] * len(edge_weights)  # Default width\n                edge_colors = ['gray'] * len(edge_weights)\n            \n            # Draw network\n            nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, \n                                  ax=ax_net, alpha=0.8, edgecolors='black', linewidths=2)\n            \n            # Draw edges with varying thickness and color\n            for i, (u, v) in enumerate(edges):\n                if i &lt; len(edge_widths):\n                    x1, y1 = pos[u]\n                    x2, y2 = pos[v]\n                    \n                    # Use pre-calculated color and width (both scaled identically)\n                    arrow = FancyArrowPatch((x1, y1), (x2, y2),\n                                          connectionstyle=\"arc3,rad=0.1\",\n                                          arrowstyle='-&gt;', mutation_scale=20,\n                                          linewidth=edge_widths[i],\n                                          color=edge_colors[i],\n                                          alpha=0.8)\n                    ax_net.add_patch(arrow)\n            \n            # Add labels for nodes\n            labels = {}\n            for node in G.nodes():\n                modality = node.replace('_P1', '').replace('_P2', '')\n                participant = 'P1' if '_P1' in node else 'P2'\n                labels[node] = f\"{modality}\\n({participant})\"\n            \n            nx.draw_networkx_labels(G, pos, labels, font_size=12, font_weight='bold', ax=ax_net)\n            \n            # Add participant group labels\n            ax_net.text(0, 1.1, 'P1 Modalities', ha='center', va='center', \n                       fontsize=18, fontweight='bold', \n                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n            ax_net.text(1, 1.1, 'P2 Modalities', ha='center', va='center', \n                       fontsize=18, fontweight='bold',\n                       bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n            \n            # Add colorbar with GLOBAL scale for edge weights\n            sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, \n                                      norm=plt.Normalize(vmin=global_mi_min, vmax=global_mi_max))\n            sm.set_array([])\n            cbar = plt.colorbar(sm, ax=ax_net, shrink=0.6, pad=0.1)\n            cbar.set_label('Mutual Information (Global Scale)', fontsize=14, fontweight='bold')\n            cbar.ax.tick_params(labelsize=12)\n            \n            ax_net.set_xlim(-0.3, 1.3)\n            ax_net.set_ylim(-0.2, 1.3)\n            ax_net.set_aspect('equal')\n            ax_net.axis('off')\n            ax_net.set_title(f'P1-P2 Modality Coupling Network\\n{cond_name}', \n                            fontsize=20, fontweight='bold', pad=20)\n        \n        # ============================================\n        # RIGHT PANEL: JITTER PLOT\n        # ============================================\n        \n        # Categorize P1-P2 pairs\n        envelope_data = []\n        within_modality_data = []\n        cross_modality_data = []\n        \n        for _, row in cond_data.iterrows():\n            mod1 = row['modality1']\n            mod2 = row['modality2']\n            mi_val = row['max_mutual_info']\n            \n            if np.isnan(mi_val):\n                continue\n                \n            # Categorize the pair\n            if mod1 == 'Audio' and mod2 == 'Audio':\n                envelope_data.append(mi_val)\n            elif mod1 == mod2:  # Same modality (but not audio)\n                within_modality_data.append(mi_val)\n            else:  # Different modalities\n                cross_modality_data.append(mi_val)\n        \n        # Create jitter plot\n        categories = []\n        values = []\n        colors = []\n        \n        if len(envelope_data) &gt; 0:\n            categories.extend(['Envelope\\nP1-P2'] * len(envelope_data))\n            values.extend(envelope_data)\n            colors.extend(['gold'] * len(envelope_data))\n        \n        if len(within_modality_data) &gt; 0:\n            categories.extend(['Within-Modality\\nP1-P2'] * len(within_modality_data))\n            values.extend(within_modality_data)\n            colors.extend(['lightgreen'] * len(within_modality_data))\n        \n        if len(cross_modality_data) &gt; 0:\n            categories.extend(['Cross-Modality\\nP1-P2'] * len(cross_modality_data))\n            values.extend(cross_modality_data)\n            colors.extend(['lightcoral'] * len(cross_modality_data))\n        \n        if len(values) &gt; 0:\n            # Convert to DataFrame for easier plotting\n            jitter_df = pd.DataFrame({\n                'category': categories,\n                'mi_value': values,\n                'color': colors\n            })\n            \n            # Plot jitter plot\n            category_names = jitter_df['category'].unique()\n            for i, cat in enumerate(category_names):\n                cat_data = jitter_df[jitter_df['category'] == cat]\n                \n                # Add jitter to x-axis\n                x_jitter = np.random.normal(i, 0.1, len(cat_data))\n                \n                ax_jitter.scatter(x_jitter, cat_data['mi_value'], \n                                 c=cat_data['color'].iloc[0], s=60, alpha=0.7, \n                                 edgecolors='black', linewidth=0.5)\n                \n                # Add mean line\n                mean_val = cat_data['mi_value'].mean()\n                ax_jitter.plot([i-0.3, i+0.3], [mean_val, mean_val], \n                              'k-', linewidth=3, alpha=0.8)\n            \n            ax_jitter.set_xticks(range(len(category_names)))\n            ax_jitter.set_xticklabels(category_names, fontsize=14, fontweight='bold')\n            ax_jitter.set_ylabel('Mutual Information', fontsize=16, fontweight='bold')\n            ax_jitter.set_title(f'MI Distribution by Coupling Type\\n{cond_name}', \n                               fontsize=18, fontweight='bold')\n            ax_jitter.grid(True, alpha=0.3)\n            ax_jitter.tick_params(axis='both', which='major', labelsize=12)\n            \n            # Set y-axis to match global scale\n            ax_jitter.set_ylim(global_mi_min, global_mi_max)\n        \n        plt.tight_layout()\n        \n        # Save plot\n        safe_condition = cond_name.replace(' ', '_').replace('x', 'vs')\n        plt.savefig(os.path.join(output_folder, f'modality_network_with_jitter_{safe_condition}.png'), \n                   dpi=300, bbox_inches='tight', facecolor='white')\n        # also svg\n        plt.savefig(os.path.join(output_folder, f'modality_network_with_jitter_{safe_condition}.svg'),\n                   dpi=300, bbox_inches='tight', facecolor='white')\n        plt.close()\n        \n    print(\"\\n✓ Network visualizations created successfully!\")\n    print(f\"  - P1-P2 modality networks with jitter plots showing MI distributions\")\n    print(f\"  - Envelope correlation networks with mini scatter plots\")\n    print(f\"  - Edge thickness and colors scaled using global/local scales as appropriate\")\n    print(f\"  - Global MI range: {global_mi_min:.3f} - {global_mi_max:.3f}\")\n    \n    # Remove the envelope network section from the complete plotting function\n    print(\"\\n✓ All plots saved successfully!\")\n    print(f\"  - Envelope overview with consistent colors and separate phase plots\")\n    print(f\"  - Cross-modality matrices with proper scaling\") \n    print(f\"  - Individual scatter plots for each modality x condition combination\")\n    print(f\"  - P1-P2 modality networks with jitter plots\")\n    print(f\"  - Envelope correlation networks with scatter plots\")\n\n\ndef create_complete_coupling_plots(results_df, output_folder='coupling_plots/'):\n    \"\"\"\n    Create comprehensive coupling analysis plots including:\n    1. Envelope overview with proper cross-correlation lines and consistent colors\n    2. Separate cross-modality P1-P2 matrix for each condition with proper scaling\n    3. Separate scatter plots for audio P1-P2 vs each other modality P1-P2 pair PER CONDITION\n    4. Phase plots separated per condition\n    5. NEW: Network visualizations for modality coupling\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import numpy as np\n    from scipy import stats\n    import pandas as pd\n    import os\n    \n    os.makedirs(output_folder, exist_ok=True)\n    plt.style.use('seaborn-white')\n    \n    # Define consistent color scheme for conditions\n    CONDITION_COLORS = {\n        'NoVision x Movement': '#e41a1c',      # Red\n        'NoVision x NoMovement': '#377eb8',    # Blue  \n        'Vision x Movement': '#4daf4a',        # Green\n        'Vision x NoMovement': '#984ea3'       # Purple\n    }\n    \n    # Print available variables to debug\n    print(\"\\n=== Data Analysis ===\")\n    print(f\"Data shape: {results_df.shape}\")\n    \n    # Get all unique P1 and P2 variables\n    p1_vars = sorted([v for v in results_df['var1'].unique() if '_P1' in v])\n    p2_vars = sorted([v for v in results_df['var2'].unique() if '_P2' in v])\n    \n    print(f\"\\nP1 variables found:\")\n    for v in p1_vars:\n        print(f\"  - {v}\")\n    \n    print(f\"\\nP2 variables found:\")\n    for v in p2_vars:\n        print(f\"  - {v}\")\n    \n    # Updated modality extraction function (ensuring heart rate is properly mapped)\n    def get_modality_from_var(var_name):\n        if 'Amplitude_Envelope' in var_name:\n            return 'Audio'\n        elif 'Filtered_ECG' in var_name:  # Map both heart_rate and Filtered_ECG\n            return 'Heart Rate'\n        elif 'Filtered_Respiration' in var_name:\n            return 'Respiration'\n        elif 'Filtered_EMG_Bicep' in var_name:\n            return 'EMG Bicep'\n        elif 'Filtered_EMG_Tricep' in var_name:\n            return 'EMG Tricep'\n        elif 'right_index_x' in var_name:\n            return 'Motion X'\n        elif 'right_index_y' in var_name:\n            return 'Motion Y'\n        elif 'right_index_z' in var_name:\n            return 'Motion Z'\n        else:\n            return 'Unknown'\n    \n    # ============================================\n    # 1. MODALITY COUPLING OVERVIEWS (FOR EACH P1-P2 PAIR)\n    # ============================================\n    \n    print(\"\\n=== Creating Modality Overview Plots ===\")\n    \n    # Find all P1-P2 modality pairs\n    p1_p2_pairs = results_df[\n        (results_df['var1'].str.contains('_P1')) &\n        (results_df['var2'].str.contains('_P2'))\n    ].copy()\n    \n    # Get unique modality pairs\n    p1_p2_pairs['modality1'] = p1_p2_pairs['var1'].apply(get_modality_from_var)\n    p1_p2_pairs['modality2'] = p1_p2_pairs['var2'].apply(get_modality_from_var)\n    \n    # Find same-modality P1-P2 pairs\n    same_modality_pairs = p1_p2_pairs[p1_p2_pairs['modality1'] == p1_p2_pairs['modality2']]\n    unique_modalities = same_modality_pairs['modality1'].unique()\n    \n    print(f\"Creating overview plots for {len(unique_modalities)} modalities: {list(unique_modalities)}\")\n    \n    for modality in unique_modalities:\n        print(f\"  Creating overview for: {modality}\")\n        \n        # Get data for this modality\n        modality_data = same_modality_pairs[same_modality_pairs['modality1'] == modality].copy()\n        \n        if len(modality_data) == 0:\n            continue\n        \n        modality_data['condition'] = modality_data['condition_vision'].astype(str) + ' x ' + modality_data['condition_movement'].astype(str)\n        conditions = sorted(modality_data['condition'].unique())\n        \n        # Create main figure with updated layout for phase subplots\n        fig = plt.figure(figsize=(24, 20))\n        \n        # Panel 1: Mutual Information\n        ax1 = plt.subplot(3, 3, 1)\n        data_by_condition = [modality_data[modality_data['condition'] == cond]['max_mutual_info'].dropna().values \n                            for cond in conditions]\n        \n        if all(len(d) &gt; 0 for d in data_by_condition):\n            parts = ax1.violinplot(data_by_condition, positions=range(len(conditions)),\n                                  showmeans=False, showmedians=True, showextrema=False)\n            \n            for pc in parts['bodies']:\n                pc.set_facecolor('lightcoral')\n                pc.set_alpha(0.7)\n            \n            for i, cond in enumerate(conditions):\n                cond_data = data_by_condition[i]\n                x = np.random.normal(i, 0.04, size=len(cond_data))\n                color = CONDITION_COLORS.get(cond, 'gray')\n                ax1.scatter(x, cond_data, alpha=0.7, s=40, color=color)\n        \n        ax1.set_xticks(range(len(conditions)))\n        ax1.set_xticklabels(conditions, rotation=45, ha='right', fontsize=16, fontweight='bold')\n        ax1.set_ylabel('Mutual Information', fontsize=18, fontweight='bold')\n        ax1.set_title(f'Mutual Information for {modality} P1-P2', fontsize=20, fontweight='bold')\n        ax1.grid(True, alpha=0.3)\n        ax1.tick_params(axis='both', which='major', labelsize=16)\n        \n        # Panel 2: Cross-correlation AS LINES (consistent colors)\n        ax2 = plt.subplot(3, 3, 2)\n        \n        # Create lag bins for averaging\n        lag_bins = np.linspace(-0.3, 0.3, 13)\n        lag_centers = (lag_bins[:-1] + lag_bins[1:]) / 2\n        \n        for i, cond in enumerate(conditions):\n            cond_data = modality_data[modality_data['condition'] == cond]\n            color = CONDITION_COLORS.get(cond, 'gray')\n            \n            if len(cond_data) &gt; 0:\n                # Bin the data\n                mean_corrs = []\n                std_corrs = []\n                \n                for j in range(len(lag_bins)-1):\n                    mask = (cond_data['lag_at_max_crosscorr'] &gt;= lag_bins[j]) & \\\n                           (cond_data['lag_at_max_crosscorr'] &lt; lag_bins[j+1])\n                    \n                    bin_corrs = cond_data.loc[mask, 'max_crosscorr'].values\n                    if len(bin_corrs) &gt; 0:\n                        mean_corrs.append(np.mean(bin_corrs))\n                        std_corrs.append(np.std(bin_corrs) / np.sqrt(len(bin_corrs)))\n                    else:\n                        mean_corrs.append(np.nan)\n                        std_corrs.append(np.nan)\n                \n                # Plot line\n                mean_corrs = np.array(mean_corrs)\n                std_corrs = np.array(std_corrs)\n                valid = ~np.isnan(mean_corrs)\n                \n                if np.sum(valid) &gt; 1:\n                    ax2.plot(lag_centers[valid], mean_corrs[valid], \n                            'o-', color=color, linewidth=4, markersize=10,\n                            label=cond)\n                    \n                    # Add error bands\n                    ax2.fill_between(lag_centers[valid], \n                                    mean_corrs[valid] - std_corrs[valid],\n                                    mean_corrs[valid] + std_corrs[valid],\n                                    alpha=0.2, color=color)\n        \n        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n        ax2.axvline(x=0, color='black', linestyle='--', linewidth=0.5, alpha=0.5)\n        ax2.set_xlabel('Lag (s)', fontsize=18, fontweight='bold')\n        ax2.set_ylabel('Cross-correlation', fontsize=18, fontweight='bold')\n        ax2.set_title(f'Cross-correlation for {modality} P1-P2', fontsize=20, fontweight='bold')\n        ax2.legend(loc='best', fontsize=14)\n        ax2.grid(True, alpha=0.3)\n        ax2.set_xlim(-0.35, 0.35)\n        ax2.tick_params(axis='both', which='major', labelsize=16)\n        \n        # Panel 3: Phase Locking Value\n        ax3 = plt.subplot(3, 3, 3)\n        plv_data = [modality_data[modality_data['condition'] == cond]['phase_locking_value'].dropna().values \n                    for cond in conditions]\n        \n        if all(len(d) &gt; 0 for d in plv_data):\n            parts = ax3.violinplot(plv_data, positions=range(len(conditions)),\n                                  showmeans=False, showmedians=True, showextrema=False)\n            \n            for pc in parts['bodies']:\n                pc.set_facecolor('lightblue')\n                pc.set_alpha(0.7)\n            \n            for i, cond in enumerate(conditions):\n                cond_data = plv_data[i]\n                x = np.random.normal(i, 0.04, size=len(cond_data))\n                color = CONDITION_COLORS.get(cond, 'gray')\n                ax3.scatter(x, cond_data, alpha=0.7, s=40, color=color)\n        \n        ax3.set_xticks(range(len(conditions)))\n        ax3.set_xticklabels(conditions, rotation=45, ha='right', fontsize=16, fontweight='bold')\n        ax3.set_ylabel('Phase Locking Value', fontsize=18, fontweight='bold')\n        ax3.set_title(f'Phase Locking Value for {modality} P1-P2', fontsize=20, fontweight='bold')\n        ax3.grid(True, alpha=0.3)\n        ax3.tick_params(axis='both', which='major', labelsize=16)\n        \n        # Panels 4-7: Phase Distribution per condition (separate polar plots)\n        phase_positions = [(3, 3, 4), (3, 3, 5), (3, 3, 7), (3, 3, 8)]  # Skip middle position\n        \n        for i, cond in enumerate(conditions):\n            if i &gt;= len(phase_positions):\n                break\n                \n            ax_phase = plt.subplot(*phase_positions[i], projection='polar')\n            color = CONDITION_COLORS.get(cond, 'gray')\n            \n            cond_data = modality_data[modality_data['condition'] == cond]\n            if 'mean_phase_diff' in cond_data.columns:\n                phases = cond_data['mean_phase_diff'].dropna().values\n                \n                if len(phases) &gt; 0:\n                    bins = np.linspace(-np.pi, np.pi, 36)\n                    hist, _ = np.histogram(phases, bins=bins)\n                    hist = hist / (len(phases) * (bins[1] - bins[0]))  # Density\n                    \n                    theta = (bins[:-1] + bins[1:]) / 2\n                    ax_phase.bar(theta, hist, width=bins[1]-bins[0], \n                               alpha=0.7, color=color, edgecolor='black', linewidth=0.5)\n                    \n                    # Add mean direction arrow\n                    mean_angle = np.angle(np.mean(np.exp(1j * phases)))\n                    mean_length = np.abs(np.mean(np.exp(1j * phases)))\n                    ax_phase.arrow(mean_angle, 0, 0, mean_length * max(hist) * 0.8, \n                                 head_width=0.1, head_length=max(hist)*0.1, \n                                 fc='red', ec='red', linewidth=2)\n            \n            ax_phase.set_theta_zero_location('N')\n            ax_phase.set_title(f'{cond}\\nPhase Distribution', pad=20, fontsize=18, fontweight='bold')\n            ax_phase.grid(True, alpha=0.3)\n            ax_phase.tick_params(axis='both', which='major', labelsize=14)\n        \n        plt.tight_layout()\n        \n        # Save with modality-specific filename\n        safe_modality = modality.replace(' ', '_').replace('/', '_')\n        plt.savefig(os.path.join(output_folder, f'{safe_modality}_coupling_overview.png'), \n                   dpi=300, bbox_inches='tight', facecolor='white')\n        # also save as svg\n        plt.savefig(os.path.join(output_folder, f'{safe_modality}_coupling_overview.svg'),\n                   dpi=300, bbox_inches='tight', facecolor='white')\n        plt.close()\n    \n    # ============================================\n    # 2. SEPARATE CROSS-MODALITY P1-P2 MATRIX FOR EACH CONDITION\n    # ============================================\n    \n    print(\"\\n=== Creating Separate Cross-Modality Matrices ===\")\n    \n    # Get all P1-P2 pairs\n    p1_p2_data = results_df[\n        (results_df['var1'].str.contains('_P1')) &\n        (results_df['var2'].str.contains('_P2'))\n    ].copy()\n    \n    if len(p1_p2_data) &gt; 0:\n        # Add modality columns\n        p1_p2_data['modality1'] = p1_p2_data['var1'].apply(get_modality_from_var)\n        p1_p2_data['modality2'] = p1_p2_data['var2'].apply(get_modality_from_var)\n        \n        # Get all unique modalities (excluding Unknown)\n        all_modalities = sorted(list(set(p1_p2_data['modality1'].unique()) | \n                                    set(p1_p2_data['modality2'].unique())))\n        all_modalities = [m for m in all_modalities if m != 'Unknown']\n        \n        print(f\"Modalities found: {all_modalities}\")\n        \n        # Get conditions\n        conditions = p1_p2_data.groupby(['condition_vision', 'condition_movement']).size().reset_index()[['condition_vision', 'condition_movement']]\n        \n        # Calculate global min/max for consistent scaling across conditions\n        all_values = []\n        for _, cond_row in conditions.iterrows():\n            vision = cond_row['condition_vision']\n            movement = cond_row['condition_movement']\n            cond_data = p1_p2_data[\n                (p1_p2_data['condition_vision'] == vision) & \n                (p1_p2_data['condition_movement'] == movement)\n            ]\n            if len(cond_data) &gt; 0:\n                all_values.extend(cond_data['max_mutual_info'].dropna().values)\n        \n        global_vmin = 0\n        global_vmax = np.percentile(all_values, 95) if len(all_values) &gt; 0 else 2.0\n        \n        print(f\"Using global scale: {global_vmin:.3f} to {global_vmax:.3f}\")\n        \n        # Create separate plot for each condition\n        for _, cond_row in conditions.iterrows():\n            vision = cond_row['condition_vision']\n            movement = cond_row['condition_movement']\n            cond_name = f\"{vision} x {movement}\"\n            \n            print(f\"  Creating matrix for: {cond_name}\")\n            \n            # Create individual figure for this condition - MUCH LARGER\n            fig, ax = plt.subplots(1, 1, figsize=(20, 18))\n            \n            # Get data for this condition\n            cond_data = p1_p2_data[\n                (p1_p2_data['condition_vision'] == vision) & \n                (p1_p2_data['condition_movement'] == movement)\n            ]\n            \n            # Create matrix\n            matrix = np.full((len(all_modalities), len(all_modalities)), np.nan)\n            \n            for i, p2_mod in enumerate(all_modalities):\n                for j, p1_mod in enumerate(all_modalities):\n                    pair_data = cond_data[\n                        (cond_data['modality1'] == p1_mod) & \n                        (cond_data['modality2'] == p2_mod)\n                    ]\n                    \n                    if len(pair_data) &gt; 0:\n                        matrix[i, j] = pair_data['max_mutual_info'].mean()\n            \n            # Plot heatmap with proper scaling\n            mask = np.isnan(matrix)\n            \n            # Use a more contrastive colormap\n            cmap = plt.cm.viridis  # High contrast colormap\n            \n            # Calculate annotation font size based on matrix size\n            n_modalities = len(all_modalities)\n            if n_modalities &lt;= 5:\n                annot_fontsize = 24\n            elif n_modalities &lt;= 7:\n                annot_fontsize = 20\n            else:\n                annot_fontsize = 16\n            \n            sns.heatmap(matrix, annot=True, fmt='.3f', cmap=cmap, \n                       mask=mask, \n                       cbar_kws={'label': 'Mutual Information', 'shrink': 0.6},\n                       xticklabels=all_modalities, yticklabels=all_modalities,\n                       ax=ax, square=True, \n                       vmin=global_vmin, vmax=global_vmax,\n                       annot_kws={'fontsize': annot_fontsize, 'fontweight': 'bold'}, \n                       linewidths=2.0, linecolor='white')\n            # size font of matrix labels\n            \n            # MUCH larger fonts for labels and title\n            ax.set_xlabel('P1 Modality', fontsize=32, fontweight='bold', labelpad=25)\n            ax.set_ylabel('P2 Modality', fontsize=32, fontweight='bold', labelpad=25) \n            ax.set_title(f'P1-P2 Cross-Modality Coupling - {cond_name}', \n                        fontsize=36, fontweight='bold', pad=80)\n            \n            # Larger tick labels with proper spacing\n            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', \n                             fontsize=26, fontweight='bold')\n            ax.set_yticklabels(ax.get_yticklabels(), rotation=0, \n                             fontsize=26, fontweight='bold')\n            \n            # Customize colorbar with larger fonts\n            cbar = ax.collections[0].colorbar\n            cbar.ax.tick_params(labelsize=24)\n            cbar.set_label('Mutual Information', fontsize=28, fontweight='bold')\n            \n            # Add extra padding to prevent cutoff\n            fig.subplots_adjust(top=0.88)\n            \n            # Save individual condition plot\n            safe_condition = cond_name.replace(' ', '_').replace('x', 'vs')\n            plt.savefig(os.path.join(output_folder, f'cross_modality_matrix_{safe_condition}.png'), \n                       dpi=300, facecolor='white', pad_inches=1.5 )\n            # also svg\n            plt.savefig(os.path.join(output_folder, f'cross_modality_matrix_{safe_condition}.svg'),\n                       dpi=300, facecolor='white', pad_inches=1.5 )\n            plt.close()\n    \n    # ============================================\n    # 3. SEPARATE AUDIO P1-P2 vs EACH OTHER MODALITY P1-P2 SCATTER PLOTS PER CONDITION\n    # ============================================\n    \n    print(\"\\n=== Creating Individual Audio vs Modality Scatter Plots Per Condition ===\")\n    \n    # Get envelope coupling data\n    envelope_coupling = results_df[\n        (results_df['var1'] == 'Amplitude_Envelope_P1') & \n        (results_df['var2'] == 'Amplitude_Envelope_P2')\n    ].copy()\n    \n    if len(envelope_coupling) == 0:\n        print(\"No envelope coupling data found!\")\n        return\n    \n    # Average per trial/condition/window\n    envelope_avg = envelope_coupling.groupby(\n        ['trial', 'condition_vision', 'condition_movement', 'window_idx']\n    )['max_mutual_info'].mean().reset_index()\n    \n    # Find all P1-P2 pairs (excluding audio)\n    all_p1_p2_pairs = []\n    \n    print(\"\\nChecking for P1-P2 pairs:\")\n    \n    # Method 1: Look for exact P1-P2 matches\n    for p1_var in p1_vars:\n        if 'Amplitude_Envelope' in p1_var:\n            continue\n            \n        # Find matching P2 variable\n        p1_base = p1_var.replace('_P1', '')\n        p2_var = p1_base + '_P2'\n        \n        if p2_var in p2_vars:\n            # Check if data exists\n            pair_exists = len(results_df[\n                (results_df['var1'] == p1_var) & \n                (results_df['var2'] == p2_var)\n            ]) &gt; 0\n            \n            if pair_exists:\n                modality = get_modality_from_var(p1_var)\n                all_p1_p2_pairs.append((p1_var, p2_var, modality))\n                print(f\"  ✓ Found: {p1_var} → {p2_var} ({modality})\")\n    \n    # Get all condition combinations\n    all_conditions = envelope_avg.groupby(['condition_vision', 'condition_movement']).size().reset_index()[['condition_vision', 'condition_movement']]\n    \n    print(f\"\\nCreating separate scatter plots for envelope coupling P1-P2 versus coupling {len(all_p1_p2_pairs)} modalities x {len(all_conditions)} conditions:\")\n    \n    # Create separate plots for P1-P2 MI envelope vs each modality P1-P2 pair per condition\n    for p1_var, p2_var, modality in all_p1_p2_pairs:\n        for _, cond_row in all_conditions.iterrows():\n            vision = cond_row['condition_vision']\n            movement = cond_row['condition_movement']\n            cond_name = f\"{vision} x {movement}\"\n            \n            print(f\"  Creating plot for: {modality} - {cond_name}\")\n            \n            # Create individual figure for this modality and condition\n            fig, ax = plt.subplots(1, 1, figsize=(7, 10))\n            # set the size excplittly to make it square\n            fig.set_size_inches(7, 7)\n            \n            # Get modality data\n            mod_data = results_df[\n                (results_df['var1'] == p1_var) & \n                (results_df['var2'] == p2_var) &\n                (results_df['condition_vision'] == vision) &\n                (results_df['condition_movement'] == movement)\n            ].copy()\n            \n            if len(mod_data) &gt; 0:\n                # Average per trial/window\n                mod_avg = mod_data.groupby(\n                    ['trial', 'window_idx']\n                )['max_mutual_info'].mean().reset_index()\n                \n                # Get corresponding envelope data for this condition\n                env_cond = envelope_avg[\n                    (envelope_avg['condition_vision'] == vision) &\n                    (envelope_avg['condition_movement'] == movement)\n                ]\n                \n                # Merge with envelope\n                merged = pd.merge(\n                    env_cond,\n                    mod_avg,\n                    on=['trial', 'window_idx'],\n                    suffixes=('_env', '_mod')\n                )\n                \n                if len(merged) &gt; 0:\n                    x = merged['max_mutual_info_env'].values\n                    y = merged['max_mutual_info_mod'].values\n                    \n                    # Remove NaN\n                    mask = ~(np.isnan(x) | np.isnan(y))\n                    x = x[mask]\n                    y = y[mask]\n                    \n                    if len(x) &gt; 2:\n                        # Plot with condition color\n                        color = CONDITION_COLORS.get(cond_name, 'gray')\n                        ax.scatter(x, y, alpha=0.7, s=120, color=color, \n                                 edgecolors='black', linewidth=1.0, label=cond_name)\n                        \n                        # Add regression line\n                        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n                        line_x = np.array([x.min(), x.max()])\n                        line_y = slope * line_x + intercept\n                        ax.plot(line_x, line_y, 'k--', linewidth=4, alpha=0.8)\n                        \n                        # Add statistics with larger font\n                        ax.text(0.05, 0.95, f'r = {r_value:.3f}\\np = {p_value:.3f}\\nn = {len(x)}',\n                               transform=ax.transAxes, verticalalignment='top',\n                               fontsize=14, fontweight='bold',\n                               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n                        \n                        ax.legend(fontsize=20, loc='upper right')\n                    else:\n                        ax.text(0.5, 0.5, f'Insufficient data (n={len(x)})', \n                               ha='center', va='center', transform=ax.transAxes, \n                               fontsize=14, fontweight='bold')\n                else:\n                    ax.text(0.5, 0.5, 'No matched data', \n                           ha='center', va='center', transform=ax.transAxes, \n                           fontsize=14, fontweight='bold')\n            else:\n                ax.text(0.5, 0.5, f'No {modality} data for {cond_name}', \n                       ha='center', va='center', transform=ax.transAxes, \n                       fontsize=14, fontweight='bold')\n            \n            ax.set_xlabel('MI (Audio Envelope P1-P2)', fontsize=28, fontweight='bold')\n            ax.set_ylabel(f'MI ({modality} P1-P2)', fontsize=28, fontweight='bold')\n            ax.set_title(f'{cond_name}', \n                        fontsize=32, fontweight='bold')\n            ax.grid(True, alpha=0.3)\n            \n            # Increase tick label sizes\n            ax.tick_params(axis='both', which='major', labelsize=24)\n            \n            # Set equal aspect ratio\n            #ax.set_aspect('equal', adjustable='box')\n            \n            plt.tight_layout(pad=2.0)\n            \n            # Save individual plot\n            safe_modality = modality.replace(' ', '_').replace('/', '_')\n            safe_condition = cond_name.replace(' ', '_').replace('x', 'vs')\n            plt.savefig(os.path.join(output_folder, f'audio_vs_{safe_modality}_{safe_condition}_scatter_MI.png'), \n                       dpi=300, bbox_inches='tight', facecolor='white', pad_inches=1)\n            # also save as svg\n            plt.savefig(os.path.join(output_folder, f'audio_vs_{safe_modality}_{safe_condition}_scatter_MI.svg'), \n                       dpi=300, bbox_inches='tight', facecolor='white', pad_inches=1)\n            plt.close()\n    \n    # ============================================\n    # 4. NEW: CREATE NETWORK VISUALIZATIONS\n    # ============================================\n    \n    print(\"\\n=== Creating Network Visualizations ===\")\n    create_modality_network_plots(results_df, output_folder)\n    \n    print(\"\\n✓ All plots saved successfully!\")\n    print(f\"  - Overview plots for {len(unique_modalities)} modalities: {list(unique_modalities)}\")\n    print(f\"  - Cross-modality matrices with proper scaling\") \n    print(f\"  - Individual scatter plots for each modality x condition combination\")\n    print(f\"  - P1-P2 modality networks with jitter plots showing MI distributions\")\n    print(f\"  - Envelope correlation networks with mini scatter plots\")\n\n\nmerged_folder = '../4a_PROCESSED/merged_filteredtimeseries/'  # Update this path\noutput_file = './p1_p2_coupling_statistics.csv'\n\n# Calculate coupling statistics\noverwrite = True  # Set to True to recalculate, False to skip if file exists\n# check if already exist and overwrite = False\nif overwrite or not os.path.exists(output_file):\n    print(f\"File {output_file} already exists. Set overwrite=True to recalculate.\")\n    results_df = calculate_p1_p2_coupling_stats(merged_folder, output_file)\n\n\n\nFile ./p1_p2_coupling_statistics.csv already exists. Set overwrite=True to recalculate.\nFound 20 files to process\n\nProcessing: NoVisionMovement_Trial0.csv\n  Analyzing 8 windows of 5.0s each\n    Processing Amplitude_Envelope_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_ECG_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_Respiration_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Bicep_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Tricep_P2\n    Processing Amplitude_Envelope_P1 vs right_index_x_P2\n    Processing Amplitude_Envelope_P1 vs right_index_y_P2\n    Processing Amplitude_Envelope_P1 vs right_index_z_P2\n    Processing Filtered_ECG_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_ECG_P1 vs Filtered_Respiration_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_ECG_P1 vs right_index_x_P2\n    Processing Filtered_ECG_P1 vs right_index_y_P2\n    Processing Filtered_ECG_P1 vs right_index_z_P2\n    Processing Filtered_Respiration_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_Respiration_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_Respiration_P1 vs right_index_x_P2\n    Processing Filtered_Respiration_P1 vs right_index_y_P2\n    Processing Filtered_Respiration_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Bicep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Tricep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_z_P2\n    Processing right_index_x_P1 vs Amplitude_Envelope_P2\n    Processing right_index_x_P1 vs Filtered_ECG_P2\n    Processing right_index_x_P1 vs Filtered_Respiration_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_x_P1 vs right_index_y_P2\n    Processing right_index_x_P1 vs right_index_z_P2\n    Processing right_index_y_P1 vs Amplitude_Envelope_P2\n    Processing right_index_y_P1 vs Filtered_ECG_P2\n    Processing right_index_y_P1 vs Filtered_Respiration_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_y_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_y_P1 vs right_index_z_P2\n    Processing right_index_z_P1 vs Amplitude_Envelope_P2\n    Processing right_index_z_P1 vs Filtered_ECG_P2\n    Processing right_index_z_P1 vs Filtered_Respiration_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_z_P1 vs right_index_x_P2\n    Processing right_index_z_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n\nProcessing: NoVisionMovement_Trial1.csv\n  Analyzing 8 windows of 5.0s each\n    Processing Amplitude_Envelope_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_ECG_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_Respiration_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Bicep_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Tricep_P2\n    Processing Amplitude_Envelope_P1 vs right_index_x_P2\n    Processing Amplitude_Envelope_P1 vs right_index_y_P2\n    Processing Amplitude_Envelope_P1 vs right_index_z_P2\n    Processing Filtered_ECG_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_ECG_P1 vs Filtered_Respiration_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_ECG_P1 vs right_index_x_P2\n    Processing Filtered_ECG_P1 vs right_index_y_P2\n    Processing Filtered_ECG_P1 vs right_index_z_P2\n    Processing Filtered_Respiration_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_Respiration_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_Respiration_P1 vs right_index_x_P2\n    Processing Filtered_Respiration_P1 vs right_index_y_P2\n    Processing Filtered_Respiration_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Bicep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Tricep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_z_P2\n    Processing right_index_x_P1 vs Amplitude_Envelope_P2\n    Processing right_index_x_P1 vs Filtered_ECG_P2\n    Processing right_index_x_P1 vs Filtered_Respiration_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_x_P1 vs right_index_y_P2\n    Processing right_index_x_P1 vs right_index_z_P2\n    Processing right_index_y_P1 vs Amplitude_Envelope_P2\n    Processing right_index_y_P1 vs Filtered_ECG_P2\n    Processing right_index_y_P1 vs Filtered_Respiration_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_y_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_y_P1 vs right_index_z_P2\n    Processing right_index_z_P1 vs Amplitude_Envelope_P2\n    Processing right_index_z_P1 vs Filtered_ECG_P2\n    Processing right_index_z_P1 vs Filtered_Respiration_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_z_P1 vs right_index_x_P2\n    Processing right_index_z_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n\nProcessing: NoVisionMovement_Trial2.csv\n  Analyzing 8 windows of 5.0s each\n    Processing Amplitude_Envelope_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_ECG_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_Respiration_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Bicep_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Tricep_P2\n    Processing Amplitude_Envelope_P1 vs right_index_x_P2\n    Processing Amplitude_Envelope_P1 vs right_index_y_P2\n    Processing Amplitude_Envelope_P1 vs right_index_z_P2\n    Processing Filtered_ECG_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_ECG_P1 vs Filtered_Respiration_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_ECG_P1 vs right_index_x_P2\n    Processing Filtered_ECG_P1 vs right_index_y_P2\n    Processing Filtered_ECG_P1 vs right_index_z_P2\n    Processing Filtered_Respiration_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_Respiration_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_Respiration_P1 vs right_index_x_P2\n    Processing Filtered_Respiration_P1 vs right_index_y_P2\n    Processing Filtered_Respiration_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Bicep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Tricep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_z_P2\n    Processing right_index_x_P1 vs Amplitude_Envelope_P2\n    Processing right_index_x_P1 vs Filtered_ECG_P2\n    Processing right_index_x_P1 vs Filtered_Respiration_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_x_P1 vs right_index_y_P2\n    Processing right_index_x_P1 vs right_index_z_P2\n    Processing right_index_y_P1 vs Amplitude_Envelope_P2\n    Processing right_index_y_P1 vs Filtered_ECG_P2\n    Processing right_index_y_P1 vs Filtered_Respiration_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_y_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_y_P1 vs right_index_z_P2\n    Processing right_index_z_P1 vs Amplitude_Envelope_P2\n    Processing right_index_z_P1 vs Filtered_ECG_P2\n    Processing right_index_z_P1 vs Filtered_Respiration_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_z_P1 vs right_index_x_P2\n    Processing right_index_z_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n\nProcessing: NoVisionMovement_Trial3.csv\n  Analyzing 8 windows of 5.0s each\n    Processing Amplitude_Envelope_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_ECG_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_Respiration_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Bicep_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Tricep_P2\n    Processing Amplitude_Envelope_P1 vs right_index_x_P2\n    Processing Amplitude_Envelope_P1 vs right_index_y_P2\n    Processing Amplitude_Envelope_P1 vs right_index_z_P2\n    Processing Filtered_ECG_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_ECG_P1 vs Filtered_Respiration_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_ECG_P1 vs right_index_x_P2\n    Processing Filtered_ECG_P1 vs right_index_y_P2\n    Processing Filtered_ECG_P1 vs right_index_z_P2\n    Processing Filtered_Respiration_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_Respiration_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_Respiration_P1 vs right_index_x_P2\n    Processing Filtered_Respiration_P1 vs right_index_y_P2\n    Processing Filtered_Respiration_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Bicep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Tricep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_z_P2\n    Processing right_index_x_P1 vs Amplitude_Envelope_P2\n    Processing right_index_x_P1 vs Filtered_ECG_P2\n    Processing right_index_x_P1 vs Filtered_Respiration_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_x_P1 vs right_index_y_P2\n    Processing right_index_x_P1 vs right_index_z_P2\n    Processing right_index_y_P1 vs Amplitude_Envelope_P2\n    Processing right_index_y_P1 vs Filtered_ECG_P2\n    Processing right_index_y_P1 vs Filtered_Respiration_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_y_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_y_P1 vs right_index_z_P2\n    Processing right_index_z_P1 vs Amplitude_Envelope_P2\n    Processing right_index_z_P1 vs Filtered_ECG_P2\n    Processing right_index_z_P1 vs Filtered_Respiration_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_z_P1 vs right_index_x_P2\n    Processing right_index_z_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n\nProcessing: NoVisionMovement_Trial4.csv\n  Analyzing 8 windows of 5.0s each\n    Processing Amplitude_Envelope_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_ECG_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_Respiration_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Bicep_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Tricep_P2\n    Processing Amplitude_Envelope_P1 vs right_index_x_P2\n    Processing Amplitude_Envelope_P1 vs right_index_y_P2\n    Processing Amplitude_Envelope_P1 vs right_index_z_P2\n    Processing Filtered_ECG_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_ECG_P1 vs Filtered_Respiration_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_ECG_P1 vs right_index_x_P2\n    Processing Filtered_ECG_P1 vs right_index_y_P2\n    Processing Filtered_ECG_P1 vs right_index_z_P2\n    Processing Filtered_Respiration_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_Respiration_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_Respiration_P1 vs right_index_x_P2\n    Processing Filtered_Respiration_P1 vs right_index_y_P2\n    Processing Filtered_Respiration_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Bicep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Tricep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_z_P2\n    Processing right_index_x_P1 vs Amplitude_Envelope_P2\n    Processing right_index_x_P1 vs Filtered_ECG_P2\n    Processing right_index_x_P1 vs Filtered_Respiration_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_x_P1 vs right_index_y_P2\n    Processing right_index_x_P1 vs right_index_z_P2\n    Processing right_index_y_P1 vs Amplitude_Envelope_P2\n    Processing right_index_y_P1 vs Filtered_ECG_P2\n    Processing right_index_y_P1 vs Filtered_Respiration_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_y_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_y_P1 vs right_index_z_P2\n    Processing right_index_z_P1 vs Amplitude_Envelope_P2\n    Processing right_index_z_P1 vs Filtered_ECG_P2\n    Processing right_index_z_P1 vs Filtered_Respiration_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_z_P1 vs right_index_x_P2\n    Processing right_index_z_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n\nProcessing: NoVisionNoMovement_Trial0.csv\n  Analyzing 8 windows of 5.0s each\n    Processing Amplitude_Envelope_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_ECG_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_Respiration_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Bicep_P2\n    Processing Amplitude_Envelope_P1 vs Filtered_EMG_Tricep_P2\n    Processing Amplitude_Envelope_P1 vs right_index_x_P2\n    Processing Amplitude_Envelope_P1 vs right_index_y_P2\n    Processing Amplitude_Envelope_P1 vs right_index_z_P2\n    Processing Filtered_ECG_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n    Processing Filtered_ECG_P1 vs Filtered_Respiration_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_ECG_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_ECG_P1 vs right_index_x_P2\n    Processing Filtered_ECG_P1 vs right_index_y_P2\n    Processing Filtered_ECG_P1 vs right_index_z_P2\n    Processing Filtered_Respiration_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_Respiration_P1 vs Filtered_ECG_P2\n    Processing Filtered_Respiration_P1 vs Filtered_Respiration_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_Respiration_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_Respiration_P1 vs right_index_x_P2\n    Processing Filtered_Respiration_P1 vs right_index_y_P2\n    Processing Filtered_Respiration_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Bicep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Bicep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Bicep_P1 vs right_index_z_P2\n    Processing Filtered_EMG_Tricep_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_ECG_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_Respiration_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Bicep_P2\n    Processing Filtered_EMG_Tricep_P1 vs Filtered_EMG_Tricep_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_x_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_y_P2\n    Processing Filtered_EMG_Tricep_P1 vs right_index_z_P2\n    Processing right_index_x_P1 vs Amplitude_Envelope_P2\n    Processing right_index_x_P1 vs Filtered_ECG_P2\n    Processing right_index_x_P1 vs Filtered_Respiration_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_x_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_x_P1 vs right_index_x_P2\n    Processing right_index_x_P1 vs right_index_y_P2\n    Processing right_index_x_P1 vs right_index_z_P2\n    Processing right_index_y_P1 vs Amplitude_Envelope_P2\n    Processing right_index_y_P1 vs Filtered_ECG_P2\n    Processing right_index_y_P1 vs Filtered_Respiration_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_y_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_y_P1 vs right_index_x_P2\n    Processing right_index_y_P1 vs right_index_y_P2\n    Processing right_index_y_P1 vs right_index_z_P2\n    Processing right_index_z_P1 vs Amplitude_Envelope_P2\n    Processing right_index_z_P1 vs Filtered_ECG_P2\n    Processing right_index_z_P1 vs Filtered_Respiration_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Bicep_P2\n    Processing right_index_z_P1 vs Filtered_EMG_Tricep_P2\n    Processing right_index_z_P1 vs right_index_x_P2\n    Processing right_index_z_P1 vs right_index_y_P2\n    Processing right_index_z_P1 vs right_index_z_P2\n\nProcessing: NoVisionNoMovement_Trial1.csv\n  Analyzing 8 windows of 5.0s each\n    Processing Amplitude_Envelope_P1 vs Amplitude_Envelope_P2\n    Processing Filtered_ECG_P1 vs Filtered_ECG_P2\n\n\n\nimport matplotlib.pyplot as plt\n\n# Create summary plots if results were generated\noutput_file = './p1_p2_coupling_statistics.csv'\nresults_df = pd.read_csv(output_file)\nif not results_df.empty:\n    #create_plots(results_df, output_folder='coupling_plots/')\n    create_complete_coupling_plots(results_df, output_folder='coupling_plots/')\n   \n    \n    # Display some example results\n    print(\"\\nExample results:\")\n    print(results_df.head())\n    \n    print(\"\\nVariable pair types found:\")\n    print(results_df['variable_pair_type'].value_counts())\n\n\n=== Data Analysis ===\nData shape: (8512, 20)\n\nP1 variables found:\n  - Amplitude_Envelope_P1\n  - Filtered_EMG_Bicep_P1\n  - Filtered_EMG_Tricep_P1\n  - Filtered_Respiration_P1\n  - right_index_x_P1\n  - right_index_y_P1\n  - right_index_z_P1\n\nP2 variables found:\n  - Amplitude_Envelope_P2\n  - Filtered_EMG_Bicep_P2\n  - Filtered_EMG_Tricep_P2\n  - Filtered_Respiration_P2\n  - right_index_x_P2\n  - right_index_y_P2\n  - right_index_z_P2\n\n=== Creating Modality Overview Plots ===\nCreating overview plots for 7 modalities: ['Audio', 'Respiration', 'EMG Bicep', 'EMG Tricep', 'Motion X', 'Motion Y', 'Motion Z']\n  Creating overview for: Audio\n  Creating overview for: Respiration\n  Creating overview for: EMG Bicep\n  Creating overview for: EMG Tricep\n  Creating overview for: Motion X\n  Creating overview for: Motion Y\n  Creating overview for: Motion Z\n\n=== Creating Separate Cross-Modality Matrices ===\nModalities found: ['Audio', 'EMG Bicep', 'EMG Tricep', 'Motion X', 'Motion Y', 'Motion Z', 'Respiration']\nUsing global scale: 0.000 to 0.924\n  Creating matrix for: NoVision x Movement\n  Creating matrix for: NoVision x NoMovement\n  Creating matrix for: Vision x Movement\n  Creating matrix for: Vision x NoMovement\n\n=== Creating Individual Audio vs Modality Scatter Plots Per Condition ===\n\nChecking for P1-P2 pairs:\n  ✓ Found: Filtered_EMG_Bicep_P1 → Filtered_EMG_Bicep_P2 (EMG Bicep)\n  ✓ Found: Filtered_EMG_Tricep_P1 → Filtered_EMG_Tricep_P2 (EMG Tricep)\n  ✓ Found: Filtered_Respiration_P1 → Filtered_Respiration_P2 (Respiration)\n  ✓ Found: right_index_x_P1 → right_index_x_P2 (Motion X)\n  ✓ Found: right_index_y_P1 → right_index_y_P2 (Motion Y)\n  ✓ Found: right_index_z_P1 → right_index_z_P2 (Motion Z)\n\nCreating separate scatter plots for envelope coupling P1-P2 versus coupling 6 modalities x 4 conditions:\n  Creating plot for: EMG Bicep - NoVision x Movement\n  Creating plot for: EMG Bicep - NoVision x NoMovement\n  Creating plot for: EMG Bicep - Vision x Movement\n  Creating plot for: EMG Bicep - Vision x NoMovement\n  Creating plot for: EMG Tricep - NoVision x Movement\n  Creating plot for: EMG Tricep - NoVision x NoMovement\n  Creating plot for: EMG Tricep - Vision x Movement\n  Creating plot for: EMG Tricep - Vision x NoMovement\n  Creating plot for: Respiration - NoVision x Movement\n  Creating plot for: Respiration - NoVision x NoMovement\n  Creating plot for: Respiration - Vision x Movement\n  Creating plot for: Respiration - Vision x NoMovement\n  Creating plot for: Motion X - NoVision x Movement\n  Creating plot for: Motion X - NoVision x NoMovement\n  Creating plot for: Motion X - Vision x Movement\n  Creating plot for: Motion X - Vision x NoMovement\n  Creating plot for: Motion Y - NoVision x Movement\n  Creating plot for: Motion Y - NoVision x NoMovement\n  Creating plot for: Motion Y - Vision x Movement\n  Creating plot for: Motion Y - Vision x NoMovement\n  Creating plot for: Motion Z - NoVision x Movement\n  Creating plot for: Motion Z - NoVision x NoMovement\n  Creating plot for: Motion Z - Vision x Movement\n  Creating plot for: Motion Z - Vision x NoMovement\n\n=== Creating Network Visualizations ===\n\n=== Calculating Global Min/Max for Consistent Scaling ===\nGlobal MI range for edges: 0.000 - 0.924\n\n=== Creating P1-P2 Modality Networks with Jitter Plots (Global Scaling) ===\n  Creating network with jitter plot for: NoVision x Movement\n  Creating network with jitter plot for: NoVision x NoMovement\n  Creating network with jitter plot for: Vision x Movement\n  Creating network with jitter plot for: Vision x NoMovement\n\n=== Creating Envelope vs All Modalities Networks (Global Scaling) ===\n  Creating envelope network for: NoVision x Movement\n\n\nNameError: name 'significant_correlations' is not defined",
    "crumbs": [
      "Analysis",
      "MMLAB PLOTS MULTIMODAL PLOT BONANZA"
    ]
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#define-variables",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_Movement\\trial_4\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#d-plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#define-variables",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_Movement\\trial_2\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#d-plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#define-variables",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_Movement\\trial_0\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#d-plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/Examplescriptforsynchrony/processing_and_visualization.html",
    "href": "4a_PROCESSED/Examplescriptforsynchrony/processing_and_visualization.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport glob\nimport os\nfrom scipy import signal\nfrom sklearn.metrics import mutual_info_score\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Your existing functions\ndef compute_coherence(x, y, fs):\n    f, Cxy = signal.coherence(x, y, fs)\n    return f, Cxy\n\ndef crosscorr(datax, datay, lag=0):\n    \"\"\" Lag-N cross correlation. Shift datax by N elements. \"\"\"\n    return datax.corr(datay.shift(lag))\n\ndef compute_mutual_information(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute mutual information between two time series using KDE.\n    \"\"\"\n    # Clean data\n    mask = ~np.isnan(x) & ~np.isnan(y)\n    x = x[mask]\n    y = y[mask]\n    \n    if len(x) &lt; 2 or len(y) &lt; 2:\n        return 0.0\n    \n    # Standardize the data\n    x = (x - np.mean(x)) / np.std(x)\n    y = (y - np.mean(y)) / np.std(y)\n    \n    # Create KDE estimators\n    kde_joint = stats.gaussian_kde(np.vstack([x, y]))\n    kde_x = stats.gaussian_kde(x)\n    kde_y = stats.gaussian_kde(y)\n    \n    # Sample points for numerical integration\n    n_samples = 50\n    x_range = np.linspace(min(x) - 1, max(x) + 1, n_samples)\n    y_range = np.linspace(min(y) - 1, max(y) + 1, n_samples)\n    X, Y = np.meshgrid(x_range, y_range)\n    positions = np.vstack([X.ravel(), Y.ravel()])\n    \n    # Evaluate densities\n    joint_density = kde_joint(positions).reshape(X.shape)\n    x_density = kde_x(X[0,:])\n    y_density = kde_y(Y[:,0])\n    X_density, Y_density = np.meshgrid(x_density, y_density)\n    \n    # Compute MI\n    with np.errstate(divide='ignore', invalid='ignore'):\n        mi_density = joint_density * np.log(joint_density / (X_density * Y_density))\n    mi = np.nansum(mi_density) * (x_range[1] - x_range[0]) * (y_range[1] - y_range[0])\n    \n    return max(0, mi)  # Ensure non-negative MI\n\ndef compute_coupling_statistics(name, motion_ts, sound_ts, time):\n    \"\"\"Your existing coupling statistics function\"\"\"\n    # Ensure inputs are numpy arrays\n    motion_ts = np.array(motion_ts)\n    sound_ts = np.array(sound_ts)\n    time = np.array(time)\n\n    # Check if inputs are scalar (single values)\n    if motion_ts.ndim == 0 or sound_ts.ndim == 0 or time.ndim == 0:\n        return pd.DataFrame({\n            'scene': [name],\n            'lags': [np.nan],\n            'crosscorr': [np.nan],\n        }), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n\n    # Check if inputs have the same length\n    if len(motion_ts) != len(sound_ts) or len(motion_ts) != len(time):\n        raise ValueError(\"motion_ts, sound_ts, and time must have the same length\")\n\n    # normalize and center the data\n    motion_ts = (motion_ts - np.min(motion_ts)) / (np.max(motion_ts) - np.min(motion_ts))\n    motion_ts = motion_ts - np.mean(motion_ts)\n    sound_ts = (sound_ts - np.min(sound_ts)) / (np.max(sound_ts) - np.min(sound_ts))\n    sound_ts = sound_ts - np.mean(sound_ts)\n\n    # check if values are finite\n    if not np.all(np.isfinite(motion_ts)) or not np.all(np.isfinite(sound_ts)):\n        return pd.DataFrame({\n            'scene': [name],\n            'lags': [np.nan],\n            'crosscorr': [np.nan],\n        }), np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n\n    # Compute sampling frequency\n    fs = 1/np.mean(np.diff(time))\n\n    # compute the average mutual information\n    mi = []\n    lags = [-0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3]\n    fs = 1/np.mean(np.diff(time))\n    lags_samples = [int(lag * fs) for lag in lags]\n    \n    for lag in lags_samples:\n        if lag &lt; 0:\n            motsub = motion_ts[:lag]\n            soundsub = sound_ts[-lag:]\n        elif lag &gt; 0:\n            motsub = motion_ts[lag:]\n            soundsub = sound_ts[:-lag]\n        else:\n            motsub = motion_ts\n            soundsub = sound_ts\n            \n        # Add smoothing to capture temporal dependencies\n        window = int(0.1 * fs)  # 100ms window\n        if window &gt; 1:\n            motsub = np.convolve(motsub, np.ones(window)/window, mode='valid')\n            soundsub = np.convolve(soundsub, np.ones(window)/window, mode='valid')\n        \n        mi_value = compute_mutual_information(motsub, soundsub)\n        mi.append(mi_value)\n    \n    max_mi = np.max(mi)\n    optimal_lag = lags[np.argmax(mi)]\n    \n    #################################################### Coherence\n    # Compute coherence\n    f, Cxy = compute_coherence(motion_ts, sound_ts, fs)\n\n    # keep all values lower than 10 Hz\n    mask = f &lt; 10\n    Cxy = Cxy[mask]\n    f = f[mask]\n\n    # maximum coherence\n    coherence = np.max(Cxy)\n\n    # Frequency of max coherence\n    f_max = f[np.argmax(Cxy)]\n        \n    #################################################### Cross-correlation\n    lag_seconds = 0.3\n    lag_points = int(lag_seconds * fs)\n\n    # Create a range of lags to examine\n    lags = np.arange(-lag_points, lag_points + 1)\n\n    # calc cross-cor\n    cc = [crosscorr(pd.Series(motion_ts), pd.Series(sound_ts), lag) for lag in lags]\n\n    # Calculate cross-correlation\n    crosscorrdf = pd.DataFrame({\n        'scene': name,\n        'lags': lags*(1/fs),\n        'crosscorr': cc,\n    })\n\n    ########## Phase locking value\n    # Compute analytic signal (using Hilbert transform)\n    motion_analytic = signal.hilbert(motion_ts)\n    sound_analytic = signal.hilbert(sound_ts)\n\n    # Extract instantaneous phase\n    motion_phase = np.angle(motion_analytic)\n    sound_phase = np.angle(sound_analytic)\n\n    # Compute phase difference\n    phase_diff = motion_phase - sound_phase\n\n    # compute the plv\n    plv = np.abs(np.mean(np.exp(1j * phase_diff)))\n    \n    # make phase_diff a regular list not a numpy array\n    try:\n        motion_phase = motion_phase.tolist()[0] if hasattr(motion_phase, 'tolist') else motion_phase[0]\n        phase_diff = phase_diff.tolist()[0] if hasattr(phase_diff, 'tolist') else phase_diff[0]\n    except:\n        motion_phase = np.mean(motion_phase)\n        phase_diff = np.mean(phase_diff)\n\n    return crosscorrdf, coherence, f_max, max_mi, phase_diff, plv, optimal_lag\n\ndef calculate_p1_p2_coupling_stats(merged_folder='../merged_filteredtimeseries/', output_file='p1_p2_coupling_statistics.csv'):\n    \"\"\"\n    Loop through merged time series files and calculate coupling statistics between P1 and P2\n    \"\"\"\n    \n    # Find all merged CSV files\n    csv_files = glob.glob(os.path.join(merged_folder, \"*.csv\"))\n    print(f\"Found {len(csv_files)} files to process\")\n    \n    # Define P1-P2 variable pairs to analyze\n    variable_pairs = [\n        # Audio Envelope\n        ('Amplitude_Envelope_P1', 'Amplitude_Envelope_P2'),\n        \n        # Heart Rate\n        ('heart_rate_P1', 'heart_rate_P2'),\n        \n        # Respiration\n        ('Filtered_Respiration_P1', 'Filtered_Respiration_P2'),\n        \n        # EMG - Bicep\n        ('Filtered_EMG_Bicep_P1', 'Filtered_EMG_Bicep_P2'),\n        \n        # EMG - Tricep\n        ('Filtered_EMG_Tricep_P1', 'Filtered_EMG_Tricep_P2'),\n        \n        # Motion Tracking (if available)\n        ('right_index_x_P1', 'right_index_x_P2'),\n        ('right_index_y_P1', 'right_index_y_P2'),\n        ('right_index_z_P1', 'right_index_z_P2'),\n        \n        # Cross-modal pairs (examples)\n        ('heart_rate_P1', 'heart_rate_P2'),\n        ('Amplitude_Envelope_P1', 'heart_rate_P1'),  # Within P1\n        ('Amplitude_Envelope_P2', 'heart_rate_P2'),  # Within P2\n        ('Amplitude_Envelope_P1', 'heart_rate_P2'),  # Cross-participant\n        ('heart_rate_P1', 'Amplitude_Envelope_P2'),  # Cross-participant\n        \n        # Respiration-Heart Rate coupling\n        ('Filtered_Respiration_P1', 'heart_rate_P1'),  # Within P1\n        ('Filtered_Respiration_P2', 'heart_rate_P2'),  # Within P2\n        ('Filtered_Respiration_P1', 'heart_rate_P2'),  # Cross-participant\n        ('heart_rate_P1', 'Filtered_Respiration_P2'),  # Cross-participant\n    ]\n    \n    # Initialize results list\n    results = []\n    \n    # Loop through each file\n    for file_path in csv_files:\n        print(f\"\\nProcessing: {os.path.basename(file_path)}\")\n        \n        try:\n            # Load the data\n            df = pd.read_csv(file_path)\n            \n            # Extract metadata from filename and dataframe\n            filename = os.path.basename(file_path)\n            \n            # Try to extract condition information\n            condition_vision = df['ConditionVision'].iloc[0] if 'ConditionVision' in df.columns else 'Unknown'\n            condition_movement = df['ConditionMovement'].iloc[0] if 'ConditionMovement' in df.columns else 'Unknown'\n            trial = df['Trial'].iloc[0] if 'Trial' in df.columns else 'Unknown'\n            \n            # Get time vector\n            time = df['Time'].values\n            \n            # Check if we have enough data (at least 3 seconds)\n            if time.max() - time.min() &lt; 3.0:\n                print(f\"  Skipping {filename}: insufficient data duration ({time.max() - time.min():.2f}s)\")\n                continue\n            \n            # Set up sliding window analysis\n            window_duration = 3.0  # seconds\n            step_size = 1.0  # seconds\n            n_windows = int((time.max() - time.min() - window_duration) / step_size) + 1\n            \n            print(f\"  Analyzing {n_windows} windows of {window_duration}s each\")\n            \n            # Loop through each variable pair\n            for var1, var2 in variable_pairs:\n                if var1 not in df.columns or var2 not in df.columns:\n                    continue\n                    \n                print(f\"    Processing {var1} vs {var2}\")\n                \n                # Loop through sliding windows\n                for window_idx in range(n_windows):\n                    window_start = time.min() + window_idx * step_size\n                    window_end = window_start + window_duration\n                    \n                    # Extract window data\n                    window_mask = (time &gt;= window_start) & (time &lt;= window_end)\n                    time_window = time[window_mask]\n                    var1_window = df[var1].values[window_mask]\n                    var2_window = df[var2].values[window_mask]\n                    \n                    # Skip if not enough data in window or too many NaNs\n                    if len(time_window) &lt; 100 or np.sum(~np.isnan(var1_window)) &lt; 50 or np.sum(~np.isnan(var2_window)) &lt; 50:\n                        continue\n                    \n                    try:\n                        # Calculate coupling statistics\n                        crosscorrdf, coherence, f_max, max_mi, phase_diff, plv, optimal_lag = \\\n                            compute_coupling_statistics(filename, var1_window, var2_window, time_window)\n                        \n                        # Extract max cross-correlation\n                        max_crosscorr = crosscorrdf['crosscorr'].abs().max() if not crosscorrdf.empty else np.nan\n                        lag_at_max_crosscorr = crosscorrdf.loc[crosscorrdf['crosscorr'].abs().idxmax(), 'lags'] if not crosscorrdf.empty else np.nan\n                        \n                        # Store results\n                        result = {\n                            'filename': filename,\n                            'condition_vision': condition_vision,\n                            'condition_movement': condition_movement,\n                            'trial': trial,\n                            'window_idx': window_idx,\n                            'window_start': window_start,\n                            'window_end': window_end,\n                            'var1': var1,\n                            'var2': var2,\n                            'variable_pair_type': classify_variable_pair(var1, var2),\n                            'max_crosscorr': max_crosscorr,\n                            'lag_at_max_crosscorr': lag_at_max_crosscorr,\n                            'max_coherence': coherence,\n                            'freq_at_max_coherence': f_max,\n                            'max_mutual_info': max_mi,\n                            'optimal_lag_mi': optimal_lag,\n                            'phase_locking_value': plv,\n                            'mean_phase_diff': phase_diff,\n                            'n_samples': len(time_window),\n                            'sampling_rate': 1/np.mean(np.diff(time_window))\n                        }\n                        \n                        results.append(result)\n                        \n                    except Exception as e:\n                        print(f\"      Error in window {window_idx}: {e}\")\n                        continue\n                        \n        except Exception as e:\n            print(f\"  Error processing {filename}: {e}\")\n            continue\n    \n    # Convert to DataFrame\n    if results:\n        results_df = pd.DataFrame(results)\n        \n        # Save to CSV\n        results_df.to_csv(output_file, index=False)\n        print(f\"\\n✓ Saved {len(results_df)} coupling statistics to {output_file}\")\n        \n        # Print summary statistics\n        print(f\"\\nSummary:\")\n        print(f\"  Files processed: {results_df['filename'].nunique()}\")\n        print(f\"  Variable pairs analyzed: {results_df['variable_pair_type'].nunique()}\")\n        print(f\"  Total windows analyzed: {len(results_df)}\")\n        print(f\"  Conditions: {results_df['condition_vision'].unique()} x {results_df['condition_movement'].unique()}\")\n        \n        return results_df\n    else:\n        print(\"No results generated - check your data files and variable names\")\n        return pd.DataFrame()\n\ndef classify_variable_pair(var1, var2):\n    \"\"\"\n    Classify the type of variable pair for easier analysis\n    \"\"\"\n    # Extract modality information\n    modalities = []\n    participants = []\n    \n    for var in [var1, var2]:\n        if 'Amplitude_Envelope' in var:\n            modalities.append('Audio')\n        elif 'heart_rate' in var:\n            modalities.append('ECG')\n        elif 'Respiration' in var:\n            modalities.append('Respiration')\n        elif 'EMG' in var:\n            modalities.append('EMG')\n        elif 'right_index' in var:\n            modalities.append('Motion')\n        else:\n            modalities.append('Other')\n            \n        if '_P1' in var:\n            participants.append('P1')\n        elif '_P2' in var:\n            participants.append('P2')\n        else:\n            participants.append('Unknown')\n    \n    # Classify pair type\n    if participants[0] == participants[1]:\n        if participants[0] == 'P1':\n            pair_type = 'Within_P1'\n        else:\n            pair_type = 'Within_P2'\n    else:\n        pair_type = 'Between_Participants'\n    \n    if modalities[0] == modalities[1]:\n        modality_type = f\"Same_{modalities[0]}\"\n    else:\n        modality_type = f\"Cross_{modalities[0]}_{modalities[1]}\"\n    \n    return f\"{pair_type}_{modality_type}\"\n\ndef create_summary_plots(results_df, output_folder='coupling_plots/'):\n    \"\"\"\n    Create summary plots of the coupling statistics\n    \"\"\"\n    import matplotlib.pyplot as plt\n    \n    os.makedirs(output_folder, exist_ok=True)\n    \n    # Set style\n    plt.style.use('ggplot')\n    \n    # 1. Cross-correlation by condition and variable pair type\n    fig, ax = plt.subplots(figsize=(12, 8))\n    sns.boxplot(data=results_df, x='condition_vision', y='max_crosscorr', \n                hue='variable_pair_type', ax=ax)\n    plt.xticks(rotation=45)\n    plt.title('Maximum Cross-Correlation by Condition and Variable Pair Type')\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_folder, 'crosscorr_by_condition.png'), dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # 2. Phase Locking Value comparison\n    fig, ax = plt.subplots(figsize=(12, 8))\n    sns.boxplot(data=results_df, x='condition_movement', y='phase_locking_value', \n                hue='variable_pair_type', ax=ax)\n    plt.xticks(rotation=45)\n    plt.title('Phase Locking Value by Movement Condition and Variable Pair Type')\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_folder, 'plv_by_condition.png'), dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # 3. Mutual Information heatmap\n    pivot_mi = results_df.groupby(['condition_vision', 'condition_movement', 'variable_pair_type'])['max_mutual_info'].mean().unstack()\n    \n    fig, ax = plt.subplots(figsize=(14, 10))\n    sns.heatmap(pivot_mi, annot=True, fmt='.3f', cmap='viridis', ax=ax)\n    plt.title('Average Mutual Information by Condition and Variable Pair Type')\n    plt.tight_layout()\n    plt.savefig(os.path.join(output_folder, 'mi_heatmap.png'), dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    print(f\"✓ Summary plots saved to {output_folder}\")\n\n\nmerged_folder = '../4a/Processed/merged_filteredtimeseries/'  # Update this path\noutput_file = './p1_p2_coupling_statistics.csv'\n\n# Calculate coupling statistics\nresults_df = calculate_p1_p2_coupling_stats(merged_folder, output_file)\n\n# Create summary plots if results were generated\nif not results_df.empty:\n    create_summary_plots(results_df, output_folder='coupling_plots/')\n    \n    # Display some example results\n    print(\"\\nExample results:\")\n    print(results_df.head())\n    \n    print(\"\\nVariable pair types found:\")\n    print(results_df['variable_pair_type'].value_counts())"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_4/trial_4.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_4/trial_4.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_4/trial_4.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_4/trial_4.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\Vision_NoMovement\\trial_4\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_4/trial_4.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_4/trial_4.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_4/trial_4.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_4/trial_4.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_2/trial_2.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_2/trial_2.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_2/trial_2.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_2/trial_2.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\Vision_NoMovement\\trial_2\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_2/trial_2.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_2/trial_2.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_2/trial_2.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_2/trial_2.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_0/trial_0.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_0/trial_0.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_0/trial_0.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_0/trial_0.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\Vision_NoMovement\\trial_0\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_0/trial_0.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_0/trial_0.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_0/trial_0.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_0/trial_0.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_3/trial_3.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_3/trial_3.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_3/trial_3.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_3/trial_3.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\Vision_Movement\\trial_3\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_3/trial_3.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_3/trial_3.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_3/trial_3.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_3/trial_3.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_0/trial_0.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_0/trial_0.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_0/trial_0.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_0/trial_0.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\Vision_Movement\\trial_0\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_0/trial_0.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_0/trial_0.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_0/trial_0.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_0/trial_0.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_3/trial_.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_3/trial_.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_3/trial_.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_3/trial_.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_NoMovement\\trial_\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_3/trial_.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_3/trial_.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_3/trial_.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_3/trial_.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_1/trial_1.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_1/trial_1.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_1/trial_1.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_1/trial_1.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_NoMovement\\trial_1\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_1/trial_1.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_1/trial_1.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_1/trial_1.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_1/trial_1.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_Movement\\trial_4\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_4/trial_4.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_Movement\\trial_2\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_2/trial_2.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_Movement\\trial_0\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_0/trial_0.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html",
    "href": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html",
    "title": "Donderrs MML: Splitting Multiple Videos",
    "section": "",
    "text": "image.png",
    "crumbs": [
      "Motion Tracking",
      "Donderrs MML: Splitting Multiple Videos"
    ]
  },
  {
    "objectID": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#info-documents",
    "href": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#info-documents",
    "title": "Donderrs MML: Splitting Multiple Videos",
    "section": "Info documents",
    "text": "Info documents\nlocation Repositor :https://github.com/DavAhm/Mobile-Multimodal-Lab/tree/main/2_PREPROCESSING/2_MOTION_TRACKIN\nlocation Jupyter Noteboo\n## Requirements Please install the necessary packages in requirements. ## OVERVIEWE This Python script splits single videos containing multiple cameras views into individual videos of each separate view using OpenCV. It outputs the split video files into a specific folder structure, making them ready for further processing, such as motion tracking with tools like OpenPose or Pose2Sim. Below is an overview of the key steps in the code:\n\nImporting Necessary Packages: see requirements.txt for all necessary packages\nDefining Relevant Directories, Variables, and Functions: Working with relative paths, the scripts defines input and output folders. Video extension (e.g., *.avi) and corresponding codec extension (e.g., XVID) can be ajusted by the user accordingly. The current script assumes 3 cameras for each vidoe, each of them with an euqal height and width. The split_camera_views function is defined to handle the core task of splitting a video file into separate camera views. This function processes each video file, extracting frames and dividing them into distinct sections corresponding to different camera views. The split videos are then saved to specified output files.\nand 2a. In 2 the script automatically scans a specified directory to identify and collect all video files with a specific extension (e.g., .avi). This allows for batch processing of multiple videos without requiring manual file selection. In 2a, the the script provides an option for the user to manually select one or more video files through a graphical file dialog. This feature offers flexibility for the user to choose specific videos from any directory for processing.\nThe scripts loops through the selected or identified video files and splitting each one into separate camera views: For each video file, the script extracts the filename and generates output file paths for each camera view (e.g., _cam1.avi, _cam2.avi, _cam3.avi). The split_camera_views function is called to process the video, splitting it into three separate camera views and saving them in a structured folder, such as a raw-2d subfolder within the output directory.",
    "crumbs": [
      "Motion Tracking",
      "Donderrs MML: Splitting Multiple Videos"
    ]
  },
  {
    "objectID": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#importing-necessary-packages",
    "href": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#importing-necessary-packages",
    "title": "Donderrs MML: Splitting Multiple Videos",
    "section": "0. Importing Necessary Packages",
    "text": "0. Importing Necessary Packages\n\nimport os                                 # Importing the os module which provides functions for interacting with the operating system\nfrom moviepy.editor import (              # Importing functions from the MoviePy library, which is used for video editing.\n                VideoFileClip,                # VideoFileClip allows you to load a video file and manipulate it as a clip object.\n                concatenate_videoclips)       # concatenate_videoclips allows you to concatenate multiple video clips into one video.\nimport cv2                                # Importing cv2 module from OpenCV library, used for computer vision tasks (provides various functions to process images and videos).\nimport subprocess                         # Importing subprocess, commonly used for running external commands or scripts from within a Python script.\nimport tkinter                            # Importning tkinter, whch crates graphical user interfaces (GUIs) and provides a variety of widgets and controls to build desktop applications.\nfrom tkinter import filedialog                # importing filedialog from tkinter, is a module in tkinter, which allows the user to select files or directories through a dialog window.\nfrom tqdm import tqdm                     # Importing tqdm for the progress bar\nimport re\n\nprint(\"Everything was imported succesfully\") #as terminal\n\nEverything was imported succesfully",
    "crumbs": [
      "Motion Tracking",
      "Donderrs MML: Splitting Multiple Videos"
    ]
  },
  {
    "objectID": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#defining-the-relevant-directories-variables-functions",
    "href": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#defining-the-relevant-directories-variables-functions",
    "title": "Donderrs MML: Splitting Multiple Videos",
    "section": "1. Defining the Relevant Directories, Variables & Functions",
    "text": "1. Defining the Relevant Directories, Variables & Functions\n\n# ------------ DIRECTORIES -----------------------------\ninput_folder  = '../2_Video_Calibration/calibration_videos_raw/'    # Relative path to the input folder with the raw video files\noutput_folder = './video_split/'  # output folder where the split videos will be saved (relative path) \n\nprint(\"Input folder =\", os.path.abspath(input_folder))\nprint(\"Output folder =\", os.path.abspath(output_folder))\n\n\n# ----------- VARIABLES --------------------------\nvideo_extension = '.avi'  # Video format extension avi. Change as needed (e.g., .mp4)\ncodec_extension  = 'mp4v'  # Codec for avi. Consider other codecs for different extensions:\n                            # '.mp4': 'libx264',\n                            # '.avi': 'libxvid',\n                            # '.mov': 'libx264',\n                            # '.mkv': 'libx264',\n                            # '.flv': 'flv',\n\nnum_cameras = 3          # Number of cameras for each \"raw video\". Change accordingly. \n\ncondition_pattern_matching = re.compile(r'.(Vision|NoVision)_(Movement|NoMovement)')\n\n\n\n# ----------- FUNCTIONS -------------------------\ndef split_camera_views(input_file, output_files, num_cameras=3, codec_extension='mp4v'):\n    \"\"\"\n    This split_camera_views function takes an input video file and splits it into three separate video files, \n    each corresponding to a different camera view. It assumes that the input video is composed \n    of three side-by-side camera views. The function outputs three video files, each containing \n    one of the camera views.\n\n    Parameters:\n    input_file (str): The path to the input video file.\n    output_files (list of str): A list containing the paths to the output video files for each camera view.\n    num_cameras (int): The number of camera views in the input video. Default is 3.\n    codec_extension (str): The codec extension to be used for the output videos. Default is 'mp4v'.\n    \"\"\"\n    \n    cap = cv2.VideoCapture(input_file)  # Open the video file\n\n    # Extract relevant information from the video\n    width_per_camera = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) // num_cameras  # Calculate width per camera\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Get the height of the video (same for all cameras)\n    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))  # Get the frame rate of the video (same for all cameras)\n\n    # Create VideoWriters for each camera\n    fourcc = cv2.VideoWriter_fourcc(*codec_extension)  # Define the codec for the output videos\n    out_cam1 = cv2.VideoWriter(output_files[0], fourcc, frame_rate, (width_per_camera, height))  # Create VideoWriter for the first camera\n    out_cam2 = cv2.VideoWriter(output_files[1], fourcc, frame_rate, (width_per_camera, height))  # Create VideoWriter for the second camera\n    out_cam3 = cv2.VideoWriter(output_files[2], fourcc, frame_rate, (width_per_camera, height))  # Create VideoWriter for the third camera\n\n    while True:  # Loop to read frames from the video\n        ret, frame = cap.read()  # Read a frame from the video\n\n        # Check if the frame is None (end of video)\n        if frame is None:  # If no frame is returned, break the loop\n            break\n\n        # Break the frame into three parts\n        camera1_frame = frame[:, :width_per_camera, :]                    # Extract the first camera view\n        camera2_frame = frame[:, width_per_camera:2*width_per_camera, :]  # Extract the second camera view\n        camera3_frame = frame[:, 2*width_per_camera:, :]                  # Extract the third camera view\n\n        # Display each camera view separately (optional)\n        cv2.imshow('Camera 1', camera1_frame)  # Display the first camera view\n        # cv2.imshow('Camera 2', camera2_frame)  # Display the second camera view\n        # cv2.imshow('Camera 3', camera3_frame)  # Display the third camera view\n\n        # Write frames to video files\n        out_cam1.write(camera1_frame)  # Write the first camera view to its file\n        out_cam2.write(camera2_frame)  # Write the second camera view to its file\n        out_cam3.write(camera3_frame)  # Write the third camera view to its file\n\n        if cv2.waitKey(1) == 27:  # Break the loop if 'Esc' key is pressed\n            break\n\n    # Release VideoWriters and VideoCapture\n    out_cam1.release()  # Release the VideoWriter for the first camera\n    out_cam2.release()  # Release the VideoWriter for the second camera\n    out_cam3.release()  # Release the VideoWriter for the third camera\n    cap.release()  # Release the VideoCapture\n    cv2.destroyAllWindows()  # Close all OpenCV windows\n\nprint('Function split_camera_views created successfully')\n\nInput folder = f:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\2_Video_Calibration\\calibration_videos_raw\nOutput folder = f:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\1_Video_Segmentation\\video_split\nFunction split_camera_views created successfully",
    "crumbs": [
      "Motion Tracking",
      "Donderrs MML: Splitting Multiple Videos"
    ]
  },
  {
    "objectID": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#identifying-relevant-videos-in-the-input_folder",
    "href": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#identifying-relevant-videos-in-the-input_folder",
    "title": "Donderrs MML: Splitting Multiple Videos",
    "section": "2. Identifying Relevant Videos in the input_folder",
    "text": "2. Identifying Relevant Videos in the input_folder\n\nvideo_files = [ ]    # Initialize an empty list to store paths of video files\n\n# Traverse through the directory and its subdirectories to find XDF files\nfor root, dirs, videos in os.walk(os.path.abspath(input_folder)):  # 1st loop iterating over the results returned by os.walk().\n    print(input_folder)\n    \n    for video in videos:                               # 2nd loop iterating through each file in the current directory\n        \n        if video.endswith(video_extension):            # checking if the video has the specified video_extension (e.g., '.avi') \n            \n             video_files.append(os.path.join(root, video))   # if the video is an .avi file, append its full path to the  list\n            \nprint('We have idenified the following vides: ' + str(video_files))\n\n../2_Video_Calibration/calibration_videos_raw/\nWe have idenified the following vides: ['f:\\\\Mobile-Multimodal-Lab\\\\3_MOTION_TRACKING\\\\2_Video_Calibration\\\\calibration_videos_raw\\\\Calibration_P1_T1_Charuco_2024-04-23_output_compr.avi', 'f:\\\\Mobile-Multimodal-Lab\\\\3_MOTION_TRACKING\\\\2_Video_Calibration\\\\calibration_videos_raw\\\\Calibration_P2_T1_Charuco_2024-04-23_output_compr.avi']\n\n\n\nprint(input_folder)\n\n../2_Video_Calibration/calibration_videos_raw/",
    "crumbs": [
      "Motion Tracking",
      "Donderrs MML: Splitting Multiple Videos"
    ]
  },
  {
    "objectID": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#a.-alternatively-the-user-can-select-their-own-videos-from-any-directory.",
    "href": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#a.-alternatively-the-user-can-select-their-own-videos-from-any-directory.",
    "title": "Donderrs MML: Splitting Multiple Videos",
    "section": "2a. Alternatively, the user can select their own Videos from any directory.",
    "text": "2a. Alternatively, the user can select their own Videos from any directory.\n\nroot = tkinter.Tk()\nroot.attributes('-topmost',True)\nroot.iconify()\n\nvideo_files = filedialog.askopenfilename(title=\"Select one or multiple Videos in the format\" + str(video_extension), filetypes=[(\"Video Files\", str(video_extension))], multiple = 'True')\n\nroot.destroy()\n\n# Convert the tuple returned by askopenfilenames() to a list\nvideo_files = list(video_files)\n\nprint('You have selected the following Videos: ' + str(video_files))\n\nYou have selected the following Videos: ['F:/Mobile-Multimodal-Lab/2_PREPROCESSING/3_MOTION_TRACKING/2_Video Calibration/calibration_videos_raw/Calibration_P1_T1_Charuco_2024-04-23_output_compr.avi', 'F:/Mobile-Multimodal-Lab/2_PREPROCESSING/3_MOTION_TRACKING/2_Video Calibration/calibration_videos_raw/Calibration_P2_T1_Charuco_2024-04-23_output_compr.avi']",
    "crumbs": [
      "Motion Tracking",
      "Donderrs MML: Splitting Multiple Videos"
    ]
  },
  {
    "objectID": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#splitting-videos-loop",
    "href": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#splitting-videos-loop",
    "title": "Donderrs MML: Splitting Multiple Videos",
    "section": "3. Splitting Videos (Loop)",
    "text": "3. Splitting Videos (Loop)\n\n\n## This is temporary for the calibration files (different naming format)\n\n# loop over files in folder and split them\nfor file in tqdm(video_files, desc=\"Splitting the videos: \", unit=\"file\"):\n\n    # Get the name of the file without the extension\n    filename = os.path.splitext(os.path.basename(file))[0]\n    \n    # Get the participant, condition and trial from the filename\n    participant = str(filename.split('_')[1])\n    print(participant)\n\n    \n    subfolder_path = os.path.join(output_folder, participant)\n    # Check if the subfolder exists, if not create it\n    if not os.path.exists(subfolder_path):\n        os.makedirs(subfolder_path)\n        \n    \n    # Check if the subfolder exists, if not create it\n    if not os.path.exists(subfolder_path):\n        os.makedirs(subfolder_path)\n\n    # create output file names for each camera, and save the three videos into ouput_folder with their correspodning camera angle \n    output_files = [\n        os.path.join(subfolder_path, filename + '_cam1.mp4'),\n        os.path.join(subfolder_path, filename + '_cam2.mp4'),\n        os.path.join(subfolder_path, filename + '_cam3.mp4')\n    ]\n\n    # Make sure the output folder exists\n    os.makedirs(output_folder, exist_ok=True)\n\n    print(\"saving Split Videos as: \" + str(output_files))\n\n\n    # Splitting the videos using the function split_camera view\n    split_camera_views(file, output_files)\n\nprint('Done, you can now look into the folder: ' + output_folder)\n\nSplitting the videos:   0%|          | 0/2 [00:00&lt;?, ?file/s]Splitting the videos:  50%|█████     | 1/2 [00:42&lt;00:42, 42.58s/file]Splitting the videos: 100%|██████████| 2/2 [01:30&lt;00:00, 45.00s/file]\n\n\nP1\nsaving Split Videos as: ['./video_split/P1\\\\Calibration_P1_T1_Charuco_2024-04-23_output_compr_cam1.mp4', './video_split/P1\\\\Calibration_P1_T1_Charuco_2024-04-23_output_compr_cam2.mp4', './video_split/P1\\\\Calibration_P1_T1_Charuco_2024-04-23_output_compr_cam3.mp4']\nP2\nsaving Split Videos as: ['./video_split/P2\\\\Calibration_P2_T1_Charuco_2024-04-23_output_compr_cam1.mp4', './video_split/P2\\\\Calibration_P2_T1_Charuco_2024-04-23_output_compr_cam2.mp4', './video_split/P2\\\\Calibration_P2_T1_Charuco_2024-04-23_output_compr_cam3.mp4']\nDone, you can now look into the folder: ./video_split/\n\n\n\n# loop over files in folder and split them\nfor file in tqdm(video_files, desc=\"Splitting the videos: \", unit=\"file\"):\n\n    # Get the name of the file without the extension\n    filename = os.path.splitext(os.path.basename(file))[0]\n    \n    # Get the participant, condition and trial from the filename\n    participant = str(filename.split('_')[3])\n    condition = str(filename.split('_')[4] + '_' + filename.split('_')[5])\n    trial    = 'trial_' + str(filename.split('_')[6])\n    print('Participant: ' + participant)\n    print('Condition: ' + condition)\n    print('Trial: ' + trial)    \n\n    \n    subfolder_path = os.path.join(output_folder, participant, condition, trial)\n    \n    # Check if the subfolder exists, if not create it\n    if not os.path.exists(subfolder_path):\n        os.makedirs(subfolder_path)\n\n    # create output file names for each camera, and save the three videos into ouput_folder with their correspodning camera angle \n    output_files = [\n        os.path.join(subfolder_path, filename + '_cam1.avi'),\n        os.path.join(subfolder_path, filename + '_cam2.avi'),\n        os.path.join(subfolder_path, filename + '_cam3.avi')\n    ]\n\n    # Make sure the output folder exists\n    os.makedirs(output_folder, exist_ok=True)\n\n    print(\"saving Split Videos as: \" + str(output_files))\n\n\n    # Splitting the videos using the function split_camera view\n    split_camera_views(file, output_files)\n\nprint('Done, you can now look into the folder: ' + output_folder)\n\nSplitting the videos:   0%|          | 0/40 [00:00&lt;?, ?file/s]Splitting the videos:   2%|▎         | 1/40 [00:12&lt;07:58, 12.27s/file]Splitting the videos:   5%|▌         | 2/40 [00:24&lt;07:51, 12.42s/file]Splitting the videos:   8%|▊         | 3/40 [00:37&lt;07:41, 12.48s/file]Splitting the videos:  10%|█         | 4/40 [00:49&lt;07:29, 12.50s/file]Splitting the videos:  12%|█▎        | 5/40 [01:02&lt;07:17, 12.51s/file]Splitting the videos:  15%|█▌        | 6/40 [01:14&lt;06:58, 12.30s/file]Splitting the videos:  18%|█▊        | 7/40 [01:26&lt;06:41, 12.17s/file]Splitting the videos:  20%|██        | 8/40 [01:38&lt;06:28, 12.13s/file]Splitting the videos:  22%|██▎       | 9/40 [01:49&lt;06:11, 11.99s/file]Splitting the videos:  25%|██▌       | 10/40 [02:01&lt;05:53, 11.79s/file]Splitting the videos:  28%|██▊       | 11/40 [02:13&lt;05:50, 12.08s/file]Splitting the videos:  30%|███       | 12/40 [02:26&lt;05:42, 12.22s/file]Splitting the videos:  32%|███▎      | 13/40 [02:39&lt;05:32, 12.33s/file]Splitting the videos:  35%|███▌      | 14/40 [02:51&lt;05:19, 12.29s/file]Splitting the videos:  38%|███▊      | 15/40 [03:03&lt;05:05, 12.23s/file]Splitting the videos:  40%|████      | 16/40 [03:15&lt;04:55, 12.31s/file]Splitting the videos:  42%|████▎     | 17/40 [03:27&lt;04:40, 12.20s/file]Splitting the videos:  45%|████▌     | 18/40 [03:40&lt;04:30, 12.29s/file]Splitting the videos:  48%|████▊     | 19/40 [03:52&lt;04:19, 12.37s/file]Splitting the videos:  50%|█████     | 20/40 [04:05&lt;04:07, 12.38s/file]Splitting the videos:  52%|█████▎    | 21/40 [04:18&lt;03:59, 12.59s/file]Splitting the videos:  55%|█████▌    | 22/40 [04:31&lt;03:48, 12.72s/file]Splitting the videos:  57%|█████▊    | 23/40 [04:44&lt;03:38, 12.83s/file]Splitting the videos:  60%|██████    | 24/40 [04:57&lt;03:26, 12.90s/file]Splitting the videos:  62%|██████▎   | 25/40 [05:10&lt;03:14, 12.95s/file]Splitting the videos:  65%|██████▌   | 26/40 [05:23&lt;03:01, 12.93s/file]Splitting the videos:  68%|██████▊   | 27/40 [05:36&lt;02:48, 12.96s/file]Splitting the videos:  70%|███████   | 28/40 [05:49&lt;02:35, 12.99s/file]Splitting the videos:  72%|███████▎  | 29/40 [06:02&lt;02:23, 13.01s/file]Splitting the videos:  75%|███████▌  | 30/40 [06:15&lt;02:10, 13.02s/file]Splitting the videos:  78%|███████▊  | 31/40 [06:28&lt;01:58, 13.11s/file]Splitting the videos:  80%|████████  | 32/40 [06:42&lt;01:45, 13.16s/file]Splitting the videos:  82%|████████▎ | 33/40 [06:55&lt;01:32, 13.17s/file]Splitting the videos:  85%|████████▌ | 34/40 [07:08&lt;01:19, 13.18s/file]Splitting the videos:  88%|████████▊ | 35/40 [07:21&lt;01:05, 13.16s/file]Splitting the videos:  90%|█████████ | 36/40 [07:34&lt;00:52, 13.15s/file]Splitting the videos:  92%|█████████▎| 37/40 [07:48&lt;00:39, 13.14s/file]Splitting the videos:  95%|█████████▌| 38/40 [08:01&lt;00:26, 13.11s/file]Splitting the videos:  98%|█████████▊| 39/40 [08:14&lt;00:13, 13.11s/file]Splitting the videos: 100%|██████████| 40/40 [08:27&lt;00:00, 12.68s/file]\n\n\nParticipant: P1\nCondition: NoVision_Movement\nTrial: trial_0\nsaving Split Videos as: ['./video_split/P1\\\\NoVision_Movement\\\\trial_0\\\\T1_experiment_Video_P1_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\NoVision_Movement\\\\trial_0\\\\T1_experiment_Video_P1_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\NoVision_Movement\\\\trial_0\\\\T1_experiment_Video_P1_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: NoVision_Movement\nTrial: trial_1\nsaving Split Videos as: ['./video_split/P1\\\\NoVision_Movement\\\\trial_1\\\\T1_experiment_Video_P1_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\NoVision_Movement\\\\trial_1\\\\T1_experiment_Video_P1_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\NoVision_Movement\\\\trial_1\\\\T1_experiment_Video_P1_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: NoVision_Movement\nTrial: trial_2\nsaving Split Videos as: ['./video_split/P1\\\\NoVision_Movement\\\\trial_2\\\\T1_experiment_Video_P1_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\NoVision_Movement\\\\trial_2\\\\T1_experiment_Video_P1_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\NoVision_Movement\\\\trial_2\\\\T1_experiment_Video_P1_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: NoVision_Movement\nTrial: trial_3\nsaving Split Videos as: ['./video_split/P1\\\\NoVision_Movement\\\\trial_3\\\\T1_experiment_Video_P1_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\NoVision_Movement\\\\trial_3\\\\T1_experiment_Video_P1_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\NoVision_Movement\\\\trial_3\\\\T1_experiment_Video_P1_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: NoVision_Movement\nTrial: trial_4\nsaving Split Videos as: ['./video_split/P1\\\\NoVision_Movement\\\\trial_4\\\\T1_experiment_Video_P1_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\NoVision_Movement\\\\trial_4\\\\T1_experiment_Video_P1_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\NoVision_Movement\\\\trial_4\\\\T1_experiment_Video_P1_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: NoVision_NoMovement\nTrial: trial_0\nsaving Split Videos as: ['./video_split/P1\\\\NoVision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P1_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\NoVision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P1_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\NoVision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P1_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: NoVision_NoMovement\nTrial: trial_1\nsaving Split Videos as: ['./video_split/P1\\\\NoVision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P1_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\NoVision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P1_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\NoVision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P1_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: NoVision_NoMovement\nTrial: trial_2\nsaving Split Videos as: ['./video_split/P1\\\\NoVision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P1_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\NoVision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P1_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\NoVision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P1_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: NoVision_NoMovement\nTrial: trial_3\nsaving Split Videos as: ['./video_split/P1\\\\NoVision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P1_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\NoVision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P1_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\NoVision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P1_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: NoVision_NoMovement\nTrial: trial_4\nsaving Split Videos as: ['./video_split/P1\\\\NoVision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P1_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\NoVision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P1_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\NoVision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P1_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: Vision_Movement\nTrial: trial_0\nsaving Split Videos as: ['./video_split/P1\\\\Vision_Movement\\\\trial_0\\\\T1_experiment_Video_P1_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\Vision_Movement\\\\trial_0\\\\T1_experiment_Video_P1_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\Vision_Movement\\\\trial_0\\\\T1_experiment_Video_P1_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: Vision_Movement\nTrial: trial_1\nsaving Split Videos as: ['./video_split/P1\\\\Vision_Movement\\\\trial_1\\\\T1_experiment_Video_P1_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\Vision_Movement\\\\trial_1\\\\T1_experiment_Video_P1_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\Vision_Movement\\\\trial_1\\\\T1_experiment_Video_P1_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: Vision_Movement\nTrial: trial_2\nsaving Split Videos as: ['./video_split/P1\\\\Vision_Movement\\\\trial_2\\\\T1_experiment_Video_P1_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\Vision_Movement\\\\trial_2\\\\T1_experiment_Video_P1_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\Vision_Movement\\\\trial_2\\\\T1_experiment_Video_P1_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: Vision_Movement\nTrial: trial_3\nsaving Split Videos as: ['./video_split/P1\\\\Vision_Movement\\\\trial_3\\\\T1_experiment_Video_P1_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\Vision_Movement\\\\trial_3\\\\T1_experiment_Video_P1_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\Vision_Movement\\\\trial_3\\\\T1_experiment_Video_P1_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: Vision_Movement\nTrial: trial_4\nsaving Split Videos as: ['./video_split/P1\\\\Vision_Movement\\\\trial_4\\\\T1_experiment_Video_P1_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\Vision_Movement\\\\trial_4\\\\T1_experiment_Video_P1_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\Vision_Movement\\\\trial_4\\\\T1_experiment_Video_P1_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: Vision_NoMovement\nTrial: trial_0\nsaving Split Videos as: ['./video_split/P1\\\\Vision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P1_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\Vision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P1_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\Vision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P1_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: Vision_NoMovement\nTrial: trial_1\nsaving Split Videos as: ['./video_split/P1\\\\Vision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P1_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\Vision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P1_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\Vision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P1_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: Vision_NoMovement\nTrial: trial_2\nsaving Split Videos as: ['./video_split/P1\\\\Vision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P1_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\Vision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P1_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\Vision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P1_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: Vision_NoMovement\nTrial: trial_3\nsaving Split Videos as: ['./video_split/P1\\\\Vision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P1_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\Vision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P1_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\Vision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P1_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P1\nCondition: Vision_NoMovement\nTrial: trial_4\nsaving Split Videos as: ['./video_split/P1\\\\Vision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P1_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_clipped_cam1.avi', './video_split/P1\\\\Vision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P1_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_clipped_cam2.avi', './video_split/P1\\\\Vision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P1_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: NoVision_Movement\nTrial: trial_0\nsaving Split Videos as: ['./video_split/P2\\\\NoVision_Movement\\\\trial_0\\\\T1_experiment_Video_P2_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\NoVision_Movement\\\\trial_0\\\\T1_experiment_Video_P2_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\NoVision_Movement\\\\trial_0\\\\T1_experiment_Video_P2_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: NoVision_Movement\nTrial: trial_1\nsaving Split Videos as: ['./video_split/P2\\\\NoVision_Movement\\\\trial_1\\\\T1_experiment_Video_P2_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\NoVision_Movement\\\\trial_1\\\\T1_experiment_Video_P2_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\NoVision_Movement\\\\trial_1\\\\T1_experiment_Video_P2_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: NoVision_Movement\nTrial: trial_2\nsaving Split Videos as: ['./video_split/P2\\\\NoVision_Movement\\\\trial_2\\\\T1_experiment_Video_P2_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\NoVision_Movement\\\\trial_2\\\\T1_experiment_Video_P2_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\NoVision_Movement\\\\trial_2\\\\T1_experiment_Video_P2_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: NoVision_Movement\nTrial: trial_3\nsaving Split Videos as: ['./video_split/P2\\\\NoVision_Movement\\\\trial_3\\\\T1_experiment_Video_P2_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\NoVision_Movement\\\\trial_3\\\\T1_experiment_Video_P2_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\NoVision_Movement\\\\trial_3\\\\T1_experiment_Video_P2_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: NoVision_Movement\nTrial: trial_4\nsaving Split Videos as: ['./video_split/P2\\\\NoVision_Movement\\\\trial_4\\\\T1_experiment_Video_P2_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\NoVision_Movement\\\\trial_4\\\\T1_experiment_Video_P2_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\NoVision_Movement\\\\trial_4\\\\T1_experiment_Video_P2_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: NoVision_NoMovement\nTrial: trial_0\nsaving Split Videos as: ['./video_split/P2\\\\NoVision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P2_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\NoVision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P2_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\NoVision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P2_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: NoVision_NoMovement\nTrial: trial_1\nsaving Split Videos as: ['./video_split/P2\\\\NoVision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P2_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\NoVision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P2_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\NoVision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P2_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: NoVision_NoMovement\nTrial: trial_2\nsaving Split Videos as: ['./video_split/P2\\\\NoVision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P2_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\NoVision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P2_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\NoVision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P2_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: NoVision_NoMovement\nTrial: trial_3\nsaving Split Videos as: ['./video_split/P2\\\\NoVision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P2_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\NoVision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P2_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\NoVision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P2_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: NoVision_NoMovement\nTrial: trial_4\nsaving Split Videos as: ['./video_split/P2\\\\NoVision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P2_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\NoVision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P2_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\NoVision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P2_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: Vision_Movement\nTrial: trial_0\nsaving Split Videos as: ['./video_split/P2\\\\Vision_Movement\\\\trial_0\\\\T1_experiment_Video_P2_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\Vision_Movement\\\\trial_0\\\\T1_experiment_Video_P2_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\Vision_Movement\\\\trial_0\\\\T1_experiment_Video_P2_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: Vision_Movement\nTrial: trial_1\nsaving Split Videos as: ['./video_split/P2\\\\Vision_Movement\\\\trial_1\\\\T1_experiment_Video_P2_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\Vision_Movement\\\\trial_1\\\\T1_experiment_Video_P2_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\Vision_Movement\\\\trial_1\\\\T1_experiment_Video_P2_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: Vision_Movement\nTrial: trial_2\nsaving Split Videos as: ['./video_split/P2\\\\Vision_Movement\\\\trial_2\\\\T1_experiment_Video_P2_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\Vision_Movement\\\\trial_2\\\\T1_experiment_Video_P2_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\Vision_Movement\\\\trial_2\\\\T1_experiment_Video_P2_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: Vision_Movement\nTrial: trial_3\nsaving Split Videos as: ['./video_split/P2\\\\Vision_Movement\\\\trial_3\\\\T1_experiment_Video_P2_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\Vision_Movement\\\\trial_3\\\\T1_experiment_Video_P2_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\Vision_Movement\\\\trial_3\\\\T1_experiment_Video_P2_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: Vision_Movement\nTrial: trial_4\nsaving Split Videos as: ['./video_split/P2\\\\Vision_Movement\\\\trial_4\\\\T1_experiment_Video_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\Vision_Movement\\\\trial_4\\\\T1_experiment_Video_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\Vision_Movement\\\\trial_4\\\\T1_experiment_Video_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: Vision_NoMovement\nTrial: trial_0\nsaving Split Videos as: ['./video_split/P2\\\\Vision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P2_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\Vision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P2_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\Vision_NoMovement\\\\trial_0\\\\T1_experiment_Video_P2_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: Vision_NoMovement\nTrial: trial_1\nsaving Split Videos as: ['./video_split/P2\\\\Vision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P2_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\Vision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P2_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\Vision_NoMovement\\\\trial_1\\\\T1_experiment_Video_P2_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: Vision_NoMovement\nTrial: trial_2\nsaving Split Videos as: ['./video_split/P2\\\\Vision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P2_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\Vision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P2_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\Vision_NoMovement\\\\trial_2\\\\T1_experiment_Video_P2_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: Vision_NoMovement\nTrial: trial_3\nsaving Split Videos as: ['./video_split/P2\\\\Vision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P2_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\Vision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P2_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\Vision_NoMovement\\\\trial_3\\\\T1_experiment_Video_P2_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_clipped_cam3.avi']\nParticipant: P2\nCondition: Vision_NoMovement\nTrial: trial_4\nsaving Split Videos as: ['./video_split/P2\\\\Vision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P2_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_clipped_cam1.avi', './video_split/P2\\\\Vision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P2_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_clipped_cam2.avi', './video_split/P2\\\\Vision_NoMovement\\\\trial_4\\\\T1_experiment_Video_P2_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_clipped_cam3.avi']\nDone, you can now look into the folder: ./video_split/",
    "crumbs": [
      "Motion Tracking",
      "Donderrs MML: Splitting Multiple Videos"
    ]
  },
  {
    "objectID": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#mp4-conversion-if-necessary",
    "href": "3_MOTION_TRACKING/1_Video_Segmentation/Donders MML_Video Split.html#mp4-conversion-if-necessary",
    "title": "Donderrs MML: Splitting Multiple Videos",
    "section": "4. MP4 Conversion (if necessary)",
    "text": "4. MP4 Conversion (if necessary)\nNote that you can also use online tools to convert videos like https://cloudconvert.com/\n\ninput_folder  = './videos_split/' \noutput_folder = r \"C:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\2_MOTION_TRACKING\\3_freemocap\\\"\n\n#Should we add this step? \n# Yes, should write some easy code with moviepy that converts whatever format into MP4. \n\nSyntaxError: unterminated string literal (detected at line 2) (1093415785.py, line 2)",
    "crumbs": [
      "Motion Tracking",
      "Donderrs MML: Splitting Multiple Videos"
    ]
  },
  {
    "objectID": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing.html",
    "href": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing.html",
    "title": "Mobile Multimodal Lab (MML): XDF Preprocessing",
    "section": "",
    "text": "Donders MML LOGO.png",
    "crumbs": [
      "Preprocessing",
      "Mobile Multimodal Lab (MML): XDF Preprocessing"
    ]
  },
  {
    "objectID": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing.html#import-all-the-necessary-packages-to-work-with-xdf-audio-and-video-files",
    "href": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing.html#import-all-the-necessary-packages-to-work-with-xdf-audio-and-video-files",
    "title": "Mobile Multimodal Lab (MML): XDF Preprocessing",
    "section": "0. Import all the necessary packages to work with XDF, Audio and Video files",
    "text": "0. Import all the necessary packages to work with XDF, Audio and Video files\n\nimport os             # Importing the os module which provides functions for interacting with the operating system\nimport pyxdf          # Importing pyxdf, a Python library for reading XDF files\nimport glob           # Importing the glob module which helps in finding files/directories with specific patterns\nimport pandas as pd   # Importing pandas library (abbreviated as pd), which is used for data manipulation and analysis\nimport numpy as np    # Importing numpy library (abbreviated as np), which is used for numerical computations\nimport wave           # Importing wave module for reading and writing WAV files (usually audio files) \nimport struct         # Importing struct module which provides functions to convert between Python values and C structs\nimport math           # Importing math module which provides mathematical functions\nimport random         # Importing random module for generating random numbers\nfrom scipy.io import wavfile  # Importing wavfile module from scipy.io (a library built on numpy), for reading and writing WAV files\nimport noisereduce as nr      # Importing noisereduce module for noise reduction in audio signals\nimport json            # Importing json module for working with JSON data\nimport cv2            # Importing OpenCV library for computer vision tasks\nfrom moviepy.editor import (                # Importing various classes and functions from moviepy.editor module\n                            VideoFileClip,  # Class for working with video files\n                            AudioFileClip,  # Class for working with audio files\n                            CompositeAudioClip)  # Class for composing audio clip\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip  # video  clipping fucntion \nfrom moviepy.video.io.VideoFileClip import VideoFileClip          # alternative video clipping function\nimport matplotlib.pyplot as plt                                   # Importing pyplot library to create figures and plot data \nfrom matplotlib.widgets import Slider  \nimport tkinter   as tk                                                 # GUI toolkit \nfrom tkinter import (filedialog, \n                     simpledialog, \n                     Listbox, \n                     StringVar, \n                     messagebox)                         # GUI toolkit \nimport subprocess \n\nprint(\"Everything was imported succesfully\") #as terminal\n\nEverything was imported succesfully",
    "crumbs": [
      "Preprocessing",
      "Mobile Multimodal Lab (MML): XDF Preprocessing"
    ]
  },
  {
    "objectID": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing.html#define-the-relevant-paths-variables-functions",
    "href": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing.html#define-the-relevant-paths-variables-functions",
    "title": "Mobile Multimodal Lab (MML): XDF Preprocessing",
    "section": "1. Define the Relevant Paths, Variables & Functions",
    "text": "1. Define the Relevant Paths, Variables & Functions\n\n# ------------ PATHS -----------------------------------------------------\ninput_folder = './data_raw/'  # input folder with the raw XDF files (relative path) \noutput_folder = './data_processed/'  # output folder where the raw extracted data will be saved (relative path) \n\nprint(\"Input folder =\", os.path.abspath(input_folder))\nprint(\"Output folder =\", os.path.abspath(output_folder))\n\n\n# ------------ VARIABLES ----------------------------------------------\nnoise_reducelevel = 1.5  #This can be changed accordingly \n\n\n# Dictionary to map file extensions to codecs\nextension_to_codec = {\n    '.mp4': 'libx264',\n    '.avi': 'libxvid',\n    '.mov': 'libx264',\n    '.mkv': 'libx264',\n    '.flv': 'flv',\n    # Add more mappings as needed\n                    }\n\n\n# IF NEEDED: Create a dictionary mapping from old stream names to new stream names (# Edit and add more mappings as needed.) \n     # (This dictionary mapping is based both of the stream_names and stream_types because in our case we have 2 streams with the same name (but different types)) \nrename_dict = {\n    ('MyWebcamFrameStream_1', 'frameNR'): 'Video_P1',\n    ('MyWebcamFrameStream_2', 'frameNR'): 'Video_P2',\n    ('Mic', 'voice'): 'Mic_P1',\n    ('Mic_004', 'voice'): 'Mic_P2',\n    ('OpenSignals', '00:07:80:8C:06:6A'): 'PLUX_P2',\n    ('OpenSignals', '00:07:80:D8:A8:81'): 'PLUX_P1'\n}\n\n\n# -------------FUNCTIONS------------------------------------------------------------------------------------\n# AUDIO: Creating a function named \"to_audio\" tht writes audio data (input) and transforms into a WAV file (output). \ndef to_audio(fileloc, timeseries_name, timeseries, samplerate = 16000, channels = 1):   \n    \"\"\"\n    This function - named \"to_audio\" - writes audio data to a WAV file.\n    It accepts the following parameters:\n    - fileloc (str): Location to save the audio file.\n    - timeserie_name: Name of the timeseries stream to be converted to audio.\n    - timeseries (list): List of audio data points to be written to the audio file.\n    - samplerate (int, optional): Sampling rate of the audio data. Defaults to 16000.\n    - channels (int, optional): Number of audio channels (mono or stereo). Defaults to 1 (mono)\n    \"\"\"\n    if 'Mic' in timeseries_name:  #Condition check that the timeseriestype belongs to the microphone.\n            \n        obj = wave.open(fileloc,'w')        # Opens audio file using the wave.open() function write mode ('w'). Assigns data it to the variable obj.\n        obj.setnchannels(channels)          # Sets the number of channels in the audio file using obj.setnchannels(channels). Deafault 1 channel (mono).\n        obj.setsampwidth(2)                 # Sets the sample width in bytes using obj.setsampwidth(2). The value '2' indicates 16-bit audio.\n        obj.setframerate(float(samplerate)) # sets the frame rate of the audio file using obj.setframerate(float(samplerate)), where samplerate is provided as a parameter.\n            \n        for i in timeseries:                      # Loop to iterate over each time-point in the temeseries stream\n            data = struct.pack('&lt;h', int(i[0]))   # Converts the first value of the timeseries to an integer and packs it into a binary string (struck.pack()) according to the '&lt;h' fromat (i.e., short integer (16 bits) in little-endian byte order)   \n            obj.writeframesraw( data )            # Writes the packed binary data into an audio file using the wave function writeframesraw() from the wave library \n        obj.close()                               # Closes the audio file \n\nprint(\"Function \\\"to_audio\\\" created sucesfully\") \n\n\n\n# Renaming XDF Stream (if necessary)\ndef rename_streams(streams, rename_dict):\n    \"\"\"\n    Function rename_stream renames any streams based on the rename dictionary (if name found in remane_dict)\n        Parameters:\n    stream_name (str): The current name of the stream.\n    stream_type (str): The type of the stream.\n    rename_dict (dict): A dictionary mapping old stream names and types to new stream names.\n        Returns:\n    str: The new stream name if found in rename_dict, otherwise the original stream name.\n    \"\"\"\n    for stream in streams:\n        stream_name = stream['info']['name'][0]\n        stream_type = stream['info']['type'][0]\n\n        if (stream_name, stream_type) in rename_dict:\n            new_name = rename_dict[(stream_name, stream_type)]\n            print(f'Renaming stream {stream_name} ({stream_type}) to {new_name}')\n            stream['info']['name'][0] = new_name  # Rename the stream\n    return streams\n\nprint(\"Function \\\"rename_streams\\\" created sucesfully\") \n\n\n\n                       \n# Function to find the START and END times for all streams in XDF file \ndef find_start_end_times(streams):  \n    \"\"\"\n    Function to find START and END (LSL) times based on user input using a GUI window. \n     The function offers two primary methods for selecting start and end times:\n       1. **NOT based on Marker Events**: \n        - Option 1: Automatically selects the latest start time and the earliest end time among all non-marker streams.\n        - Option 2: Allows the user to manually input specific start and end times.\n        \n        2. **Based on Marker Events**:\n        - Option 1: Allows the user to specify a single pair of start and end events, using the markers in the data to determine times.\n        - Option 2: Allows the user to specify multiple pairs of start and end events, using markers in the data to determine times.\n\n    Input: \n        XDF streams: list of streams to be clipped based on user-selected start and end events or time bounds. Each stream is a dictionary \n                        containing information about the stream, including type, name, time stamps, and time series.\n    Output: \n       -  `time_pairs`: A dictionary where the keys are tuples of the form `(start_event, end_event)` (or `(start_marker, end_marker)`), \n      and the values are tuples of the form `(start_time, end_time)`, representing the selected LSL times for extraction.\n       -  `method`: A string indicating the method used. It can be one of the following:\n                - 'lateststart_earliestend'\n                - 'manual_startend'\n                - 'marker_ONEpair'\n                - 'marker_MULTIPLEpairs'\n       \n        Notes:\n        - Ensure that the streams list contains at least one Marker stream if using the marker-based method.\n        - The function relies on `tkinter.simpledialog` for user interaction, so it must be run in an environment that \n          supports GUI windows.\n          \n        Example Usage:\n        start_end_times, method = find_start_end_times(streams)\n    \"\"\"\n    \n    # Initialize the main application window\n    root = tk.Tk()\n    root.withdraw()  # Hide the main window\n    \n    # Ask user for START and END times method with a GUI prompt\n    choice = simpledialog.askstring(\"Clipping Method\", \n                                    \"Choose the START & END of your streams:\\n1. NOT based on Marker Events\\n2. Based on Marker Events\")\n    \n    # Initialize the time_pairs dictionary to store the selected start and end times\n    time_pairs = {}\n    \n    # If the user chooses to select the start and end time NOT based on Marker Events\n    if choice == '1':\n        choice_1 = simpledialog.askstring(\"Method NOT based on Marker Event \", \n                                          \"Choose one option:\\n1. Latest START time (for all streams except Marker) & Earliest END time (for all streams except Marker) \\n2. Manual selection of START and END times\")\n        \n        if choice_1 == '1':  # Find the latest start time across all streams except Marker streams\n            \n            method = 'lateststart_earliestend'\n            \n            print('Method: ' + method)\n            \n            # Filter out marker streams for calculating the latest start and earliest end times\n            non_marker_streams = [stream for stream in streams if stream['info']['type'][0] != 'Markers']\n\n            for i in non_marker_streams:\n                name_i = i['info']['name'][0]\n                print(str(name_i))\n                \n            # Find the latest start time across all non-marker streams\n            begintimes = [stream['time_stamps'][0] for stream in non_marker_streams]\n            latest_start_time = max(begintimes)  # Get the first timestamp of each non-marker stream and find the maximum (latest start time)\n            \n            # Find the earliest end time across all non-marker streams\n            endtimes = [stream['time_stamps'][-1] for stream in non_marker_streams]\n            earliest_end_time = min(endtimes)  # Get the last timestamp of each non-marker stream and find the minimum (earliest end time)\n            \n            # Create a dictionary with the start and end times stored as tuples\n            time_pairs = {('latest_start', 'earliest_end'): (latest_start_time, earliest_end_time)}\n            print(f\"Selected START & END Time Pairs : {time_pairs}\")\n\n        \n        elif choice_1 == '2':  # Manual selection of the start and end times\n            \n            method = 'manual_startend'\n            print('Method: ' + method)\n            \n            # Show the user the Start and end times of all streams \n            for stream in streams:  \n                print(f\"Stream: {stream['info']['name'][0]}\")\n                print(f\"Start time: {stream['time_stamps'][0]}\")\n                print(f\"End time: {stream['time_stamps'][-1]}\")\n                \n            start_time = simpledialog.askfloat(\"Manual Selection\", \n                                               \"Enter the START time in seconds (consider printed start times for all streams for boundaries)\")\n            end_time = simpledialog.askfloat(\"Manual Selection\", \n                                             \"Enter the END time in seconds (consider printed end times for all streams for boundaries)\")\n            \n            # creating a dictionary with the start and end times stored as tuples\n            if start_time is not None and end_time is not None:\n                time_pairs = {('manual_start', 'manual_end'): (start_time, end_time)}\n                print(f\"Selected START & END Time Pairs : {time_pairs}\")\n            \n    elif choice == '2': \n        # Identify a Marker stream \n        marker_stream = [stream for stream in streams if stream['info']['type'][0] == 'Markers'][0]\n        \n        print(\"Marker stream: \")\n        for i in range(len(marker_stream['time_series'])):\n            print(str(marker_stream['time_series'][i]).replace('[', '').replace(']', '').replace(\"'\", \"\"))   ## Print all the events in the marker stream in an organized way\n       \n        \n        choice_2 = simpledialog.askstring(\"Method Based on Marker Event \", \n                                          \"Choose one option:\\n1. Only 1 START & 1 END Events \\n2. MULTIPLE START & END Events\")\n \n        if choice_2 == '1': \n            \n            method = 'marker_ONEpair'\n            \n            print('Method: ' + method)\n            \n            start_event = simpledialog.askstring(\"SINGLE PAIR Marker Event\", \n                                                 \"Enter the START event name (you can copy and paste the event name from the terminal)\")\n            end_event = simpledialog.askstring(\"SINGLE PAIR Marker Event\", \n                                               \"Enter the END event name (you can copy and paste the event name from the terminal)\")\n            \n            print(f\"Selected START Event: {start_event}\")\n            print(f\"Selected END Event: {end_event}\")\n            \n            # Find the corresponding LSL times in the Marker stream based on start_event and end_event\n            start_time = None\n            end_time = None\n            for idx, event in enumerate(marker_stream['time_series']):  # enumerate the events (i.e., 'time_series in XDF stream') in the marker stream\n                if event[0] == start_event.strip():\n                    start_time = marker_stream['time_stamps'][idx]  # Find the corresponding LSL time in the Marker stream based on start_event\n                elif event[0] == end_event.strip():\n                    end_time = marker_stream['time_stamps'][idx]  # Find the corresponding LSL time in the Marker stream based on end_event\n            \n            if start_time is None or end_time is None:\n                print(f\"Could not find both events: {start_event} or {end_event}\")\n                \n            # create a dictionary with the start and end events and the corresponding start and end times stored as tuples\n            time_pairs = {(start_event.strip(), end_event.strip()): (start_time, end_time)}   \n            print(f\"Selected START & END Time Pairs : {time_pairs}\")\n            \n        elif choice_2 == '2':\n            \n            method = 'marker_MULTIPLEpairs'\n            \n            print('Method: ' + method)\n            \n            event_list = simpledialog.askstring(\"MULTIPLE PAIRS Marker Event\",\n                                                \"Enter each PAIR of START and END events SEQUENTIALLY (you can copy and paste the event name from the terminal). \"\n                                                \"Follow the format exactly: START1:END1, START2:END2, START3:END3, etc.\") \n            \n            # Convert the event_list into a list of tuples\n            if event_list:\n                try:\n                    # First, split by commas to get each pair, and ensure no extra spaces\n                    event_pairs = [pair.split(':') for pair in event_list.replace(\" \", \"\").split(',')]\n                    \n                    # Ensure that each pair contains exactly two elements (start and end)\n                    for pair in event_pairs:\n                        if len(pair) != 2:\n                            raise ValueError(f\"Invalid format for pair: {pair}\")\n                except Exception as e:\n                    print(f\"Error in the provided input: {e}\")\n                    event_pairs = []\n\n                print(f\"Selected START & END Event Pairs: {event_pairs}\")\n\n            # Find the corresponding LSL times in the Marker stream based on the event_pairs and store them in a dictionary\n            time_pairs = {}  # Initialize an empty dictionary to store the time pairs\n            for start_event, end_event in event_pairs:  # Iterate over each pair to extract corresponding start and end times\n                start_time = None\n                end_time = None\n                for idx, event in enumerate(marker_stream['time_series']):\n                    if event[0] == start_event.strip():\n                        start_time = marker_stream['time_stamps'][idx]\n                    elif event[0] == end_event.strip():\n                        end_time = marker_stream['time_stamps'][idx]\n\n                # If both times are found, add them to the dictionary\n                if start_time is not None and end_time is not None:\n                   time_pairs[(start_event.strip(), end_event.strip())] = (start_time, end_time)    # Store the events and correspdonding times as a tuple in the dictionary\n                else:\n                    print(f\"Could not find both events: {start_event} or {end_event}\")\n\n            print(f\"Selected START & END Time Pairs : {time_pairs}\")  \n                        \n\n    return time_pairs, method\n\n\nprint(\"Function \\\"find_start_end_times\\\" created sucesfully\")\n\n\n\n\n# Function to extract specified events (with correspodning LSL times) from XDF stream (useful for plotting)\ndef get_events(stream, event_names):\n    \"\"\"\n    Extracts events and corresponding LSL times from the given stream that match any of the event_names.\n\n    Parameters:\n    stream (dict): The stream containing time stamps and event data.\n    event_names (list of str): List of event name substrings to look for in the events.\n\n    Returns:\n    np.array: An array where each row contains a timestamp and the full event name.\n    \"\"\"\n    events = []  # Initialize an empty list to store matching events\n\n    # Check if the stream type is \"Markers\"\n    if stream['info']['type'][0] != \"Markers\":\n        raise ValueError(f\"ERROR: The stream provided ({stream['info']['name'][0]}) is not a Marker stream\")\n\n    # Iterate over the time stamps and corresponding events in the stream\n    for timestamp, event in zip(stream['time_stamps'], stream['time_series']):\n        # Check if any of the specified event names are in the current event\n        for name in event_names:\n            if name in event[0]:\n                # If a match is found, append the timestamp and full event name to the list\n                events.append([timestamp, event[0]])\n\n    # Convert the list of events to a NumPy array and return it\n    return np.array(events)\n\nprint(\"Function \\\"get_events\\\" created sucesfully\") \n\nInput folder = f:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\data_raw\nOutput folder = f:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\data_processed\nFunction \"to_audio\" created sucesfully\nFunction \"rename_streams\" created sucesfully\nFunction \"find_start_end_times\" created sucesfully\nFunction \"get_events\" created sucesfully",
    "crumbs": [
      "Preprocessing",
      "Mobile Multimodal Lab (MML): XDF Preprocessing"
    ]
  },
  {
    "objectID": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing.html#identifying-xdf-files-in-input-folder-or-any-subfolder",
    "href": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing.html#identifying-xdf-files-in-input-folder-or-any-subfolder",
    "title": "Mobile Multimodal Lab (MML): XDF Preprocessing",
    "section": "2. Identifying XDF files in Input Folder or any Subfolder",
    "text": "2. Identifying XDF files in Input Folder or any Subfolder\n\nxdf_files = []  # Initialize an empty list to store paths of XDF files\n\n# Traverse through the directory and its subdirectories to find XDF files\nfor root, dirs, files in os.walk(input_folder):  # 1st loop iterating over the results returned by os.walk().\n    \n    for file in files:                                    # 2nd loop iterating through each file in the current directory\n        \n        if file.endswith(\".xdf\"):                         # checking if the file has and XDF extension \n            \n             xdf_files.append(os.path.join(root, file))   # if the file is an XDF file, append its full path to the xdf_files list\n            \nprint('We have idenified the following XDF files: ' + str(xdf_files))\n\nWe have idenified the following XDF files: ['./data_raw/pilot_ming_02.xdf', './data_raw/T1_experiment.xdf']",
    "crumbs": [
      "Preprocessing",
      "Mobile Multimodal Lab (MML): XDF Preprocessing"
    ]
  },
  {
    "objectID": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing.html#a.-alternatively-the-user-can-select-their-own-xdf-file",
    "href": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing.html#a.-alternatively-the-user-can-select-their-own-xdf-file",
    "title": "Mobile Multimodal Lab (MML): XDF Preprocessing",
    "section": "2a. Alternatively, the user can select their own XDF file",
    "text": "2a. Alternatively, the user can select their own XDF file\n\nimport tkinter # GUI toolkit to open and save files\nfrom tkinter import filedialog # GUI toolkit to open and save files\n\nroot = tkinter.Tk()\nroot.attributes('-topmost',True)\nroot.iconify()\n\nxdf_files = filedialog.askopenfilename(title=\"Select an XDF file\", filetypes=[(\"XDF Files\", \"*.xdf\")], multiple = 'True')\n\nroot.destroy()\n\n# Convert the tuple returned by askopenfilenames() to a list\nxdf_files = list(xdf_files)\n\nprint('You have selected the following XDF files: ' + str(xdf_files))\n\nYou have selected the following XDF files: ['F:/Mobile-Multimodal-Lab/2_PREPROCESSING/XDF_PROCESSING/data_raw/T1_experiment.xdf']",
    "crumbs": [
      "Preprocessing",
      "Mobile Multimodal Lab (MML): XDF Preprocessing"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing - Copy.html",
    "href": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing - Copy.html",
    "title": "Mobile Multimodal Lab (MML): XDF Preprocessing",
    "section": "",
    "text": "Donders MML LOGO.png"
  },
  {
    "objectID": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing - Copy.html#import-all-the-necessary-packages-to-work-with-xdf-audio-and-video-files",
    "href": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing - Copy.html#import-all-the-necessary-packages-to-work-with-xdf-audio-and-video-files",
    "title": "Mobile Multimodal Lab (MML): XDF Preprocessing",
    "section": "0. Import all the necessary packages to work with XDF, Audio and Video files",
    "text": "0. Import all the necessary packages to work with XDF, Audio and Video files\n\nimport os             # Importing the os module which provides functions for interacting with the operating system\nimport pyxdf          # Importing pyxdf, a Python library for reading XDF files\nimport glob           # Importing the glob module which helps in finding files/directories with specific patterns\nimport pandas as pd   # Importing pandas library (abbreviated as pd), which is used for data manipulation and analysis\nimport numpy as np    # Importing numpy library (abbreviated as np), which is used for numerical computations\nimport wave           # Importing wave module for reading and writing WAV files (usually audio files) \nimport struct         # Importing struct module which provides functions to convert between Python values and C structs\nimport math           # Importing math module which provides mathematical functions\nimport random         # Importing random module for generating random numbers\nfrom scipy.io import wavfile  # Importing wavfile module from scipy.io (a library built on numpy), for reading and writing WAV files\nimport noisereduce as nr      # Importing noisereduce module for noise reduction in audio signals\nimport json            # Importing json module for working with JSON data\nimport cv2            # Importing OpenCV library for computer vision tasks\nfrom moviepy.editor import (                # Importing various classes and functions from moviepy.editor module\n                            VideoFileClip,  # Class for working with video files\n                            AudioFileClip,  # Class for working with audio files\n                            CompositeAudioClip)  # Class for composing audio clip\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip  # video  clipping fucntion \nfrom moviepy.video.io.VideoFileClip import VideoFileClip          # alternative video clipping function\nimport matplotlib.pyplot as plt                                   # Importing pyplot library to create figures and plot data \nfrom matplotlib.widgets import Slider  \nimport tkinter   as tk                                                 # GUI toolkit \nfrom tkinter import (filedialog, \n                     simpledialog, \n                     Listbox, \n                     StringVar, \n                     messagebox)                         # GUI toolkit \nimport subprocess \n\nprint(\"Everything was imported succesfully\") #as terminal\n\nEverything was imported succesfully"
  },
  {
    "objectID": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing - Copy.html#define-the-relevant-paths-variables-functions",
    "href": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing - Copy.html#define-the-relevant-paths-variables-functions",
    "title": "Mobile Multimodal Lab (MML): XDF Preprocessing",
    "section": "1. Define the Relevant Paths, Variables & Functions",
    "text": "1. Define the Relevant Paths, Variables & Functions\n\n# ------------ PATHS -----------------------------------------------------\ninput_folder = './data_raw/'  # input folder with the raw XDF files (relative path) \noutput_folder = './data_processed/'  # output folder where the raw extracted data will be saved (relative path) \n\nprint(\"Input folder =\", os.path.abspath(input_folder))\nprint(\"Output folder =\", os.path.abspath(output_folder))\n\n\n# ------------ VARIABLES ----------------------------------------------\nnoise_reducelevel = 1.5  #This can be changed accordingly \n\n\n# Dictionary to map file extensions to codecs\nextension_to_codec = {\n    '.mp4': 'libx264',\n    '.avi': 'libxvid',\n    '.mov': 'libx264',\n    '.mkv': 'libx264',\n    '.flv': 'flv',\n    # Add more mappings as needed\n                    }\n\n\n# IF NEEDED: Create a dictionary mapping from old stream names to new stream names (# Edit and add more mappings as needed.) \n     # (This dictionary mapping is based both of the stream_names and stream_types because in our case we have 2 streams with the same name (but different types)) \nrename_dict = {\n    ('MyWebcamFrameStream_1', 'frameNR'): 'Video_P1',\n    ('MyWebcamFrameStream_2', 'frameNR'): 'Video_P2',\n    ('Mic', 'voice'): 'Mic_P1',\n    ('Mic_004', 'voice'): 'Mic_P2',\n    ('OpenSignals', '00:07:80:8C:06:6A'): 'PLUX_P2',\n    ('OpenSignals', '00:07:80:D8:A8:81'): 'PLUX_P1'\n}\n\n\n# -------------FUNCTIONS------------------------------------------------------------------------------------\n# AUDIO: Creating a function named \"to_audio\" tht writes audio data (input) and transforms into a WAV file (output). \ndef to_audio(fileloc, timeseries_name, timeseries, samplerate = 16000, channels = 1):   \n    \"\"\"\n    This function - named \"to_audio\" - writes audio data to a WAV file.\n    It accepts the following parameters:\n    - fileloc (str): Location to save the audio file.\n    - timeserie_name: Name of the timeseries stream to be converted to audio.\n    - timeseries (list): List of audio data points to be written to the audio file.\n    - samplerate (int, optional): Sampling rate of the audio data. Defaults to 16000.\n    - channels (int, optional): Number of audio channels (mono or stereo). Defaults to 1 (mono)\n    \"\"\"\n    if 'Mic' in timeseries_name:  #Condition check that the timeseriestype belongs to the microphone.\n            \n        obj = wave.open(fileloc,'w')        # Opens audio file using the wave.open() function write mode ('w'). Assigns data it to the variable obj.\n        obj.setnchannels(channels)          # Sets the number of channels in the audio file using obj.setnchannels(channels). Deafault 1 channel (mono).\n        obj.setsampwidth(2)                 # Sets the sample width in bytes using obj.setsampwidth(2). The value '2' indicates 16-bit audio.\n        obj.setframerate(float(samplerate)) # sets the frame rate of the audio file using obj.setframerate(float(samplerate)), where samplerate is provided as a parameter.\n            \n        for i in timeseries:                      # Loop to iterate over each time-point in the temeseries stream\n            data = struct.pack('&lt;h', int(i[0]))   # Converts the first value of the timeseries to an integer and packs it into a binary string (struck.pack()) according to the '&lt;h' fromat (i.e., short integer (16 bits) in little-endian byte order)   \n            obj.writeframesraw( data )            # Writes the packed binary data into an audio file using the wave function writeframesraw() from the wave library \n        obj.close()                               # Closes the audio file \n\nprint(\"Function \\\"to_audio\\\" created sucesfully\") \n\n\n\n# Renaming XDF Stream (if necessary)\ndef rename_streams(streams, rename_dict):\n    \"\"\"\n    Function rename_stream renames any streams based on the rename dictionary (if name found in remane_dict)\n        Parameters:\n    stream_name (str): The current name of the stream.\n    stream_type (str): The type of the stream.\n    rename_dict (dict): A dictionary mapping old stream names and types to new stream names.\n        Returns:\n    str: The new stream name if found in rename_dict, otherwise the original stream name.\n    \"\"\"\n    for stream in streams:\n        stream_name = stream['info']['name'][0]\n        stream_type = stream['info']['type'][0]\n\n        if (stream_name, stream_type) in rename_dict:\n            new_name = rename_dict[(stream_name, stream_type)]\n            print(f'Renaming stream {stream_name} ({stream_type}) to {new_name}')\n            stream['info']['name'][0] = new_name  # Rename the stream\n    return streams\n\nprint(\"Function \\\"rename_streams\\\" created sucesfully\") \n\n\n\n                       \n# Function to find the START and END times for all streams in XDF file \ndef find_start_end_times(streams):  \n    \"\"\"\n    Function to find START and END (LSL) times based on user input using a GUI window. \n     The function offers two primary methods for selecting start and end times:\n       1. **NOT based on Marker Events**: \n        - Option 1: Automatically selects the latest start time and the earliest end time among all non-marker streams.\n        - Option 2: Allows the user to manually input specific start and end times.\n        \n        2. **Based on Marker Events**:\n        - Option 1: Allows the user to specify a single pair of start and end events, using the markers in the data to determine times.\n        - Option 2: Allows the user to specify multiple pairs of start and end events, using markers in the data to determine times.\n\n    Input: \n        XDF streams: list of streams to be clipped based on user-selected start and end events or time bounds. Each stream is a dictionary \n                        containing information about the stream, including type, name, time stamps, and time series.\n    Output: \n       -  `time_pairs`: A dictionary where the keys are tuples of the form `(start_event, end_event)` (or `(start_marker, end_marker)`), \n      and the values are tuples of the form `(start_time, end_time)`, representing the selected LSL times for extraction.\n       -  `method`: A string indicating the method used. It can be one of the following:\n                - 'lateststart_earliestend'\n                - 'manual_startend'\n                - 'marker_ONEpair'\n                - 'marker_MULTIPLEpairs'\n       \n        Notes:\n        - Ensure that the streams list contains at least one Marker stream if using the marker-based method.\n        - The function relies on `tkinter.simpledialog` for user interaction, so it must be run in an environment that \n          supports GUI windows.\n          \n        Example Usage:\n        start_end_times, method = find_start_end_times(streams)\n    \"\"\"\n    \n    # Initialize the main application window\n    root = tk.Tk()\n    root.withdraw()  # Hide the main window\n    \n    # Ask user for START and END times method with a GUI prompt\n    choice = simpledialog.askstring(\"Clipping Method\", \n                                    \"Choose the START & END of your streams:\\n1. NOT based on Marker Events\\n2. Based on Marker Events\")\n    \n    # Initialize the time_pairs dictionary to store the selected start and end times\n    time_pairs = {}\n    \n    # If the user chooses to select the start and end time NOT based on Marker Events\n    if choice == '1':\n        choice_1 = simpledialog.askstring(\"Method NOT based on Marker Event \", \n                                          \"Choose one option:\\n1. Latest START time (for all streams except Marker) & Earliest END time (for all streams except Marker) \\n2. Manual selection of START and END times\")\n        \n        if choice_1 == '1':  # Find the latest start time across all streams except Marker streams\n            \n            method = 'lateststart_earliestend'\n            \n            print('Method: ' + method)\n            \n            # Filter out marker streams for calculating the latest start and earliest end times\n            non_marker_streams = [stream for stream in streams if stream['info']['type'][0] != 'Markers']\n\n            for i in non_marker_streams:\n                name_i = i['info']['name'][0]\n                print(str(name_i))\n                \n            # Find the latest start time across all non-marker streams\n            begintimes = [stream['time_stamps'][0] for stream in non_marker_streams]\n            latest_start_time = max(begintimes)  # Get the first timestamp of each non-marker stream and find the maximum (latest start time)\n            \n            # Find the earliest end time across all non-marker streams\n            endtimes = [stream['time_stamps'][-1] for stream in non_marker_streams]\n            earliest_end_time = min(endtimes)  # Get the last timestamp of each non-marker stream and find the minimum (earliest end time)\n            \n            # Create a dictionary with the start and end times stored as tuples\n            time_pairs = {('latest_start', 'earliest_end'): (latest_start_time, earliest_end_time)}\n            print(f\"Selected START & END Time Pairs : {time_pairs}\")\n\n        \n        elif choice_1 == '2':  # Manual selection of the start and end times\n            \n            method = 'manual_startend'\n            print('Method: ' + method)\n            \n            # Show the user the Start and end times of all streams \n            for stream in streams:  \n                print(f\"Stream: {stream['info']['name'][0]}\")\n                print(f\"Start time: {stream['time_stamps'][0]}\")\n                print(f\"End time: {stream['time_stamps'][-1]}\")\n                \n            start_time = simpledialog.askfloat(\"Manual Selection\", \n                                               \"Enter the START time in seconds (consider printed start times for all streams for boundaries)\")\n            end_time = simpledialog.askfloat(\"Manual Selection\", \n                                             \"Enter the END time in seconds (consider printed end times for all streams for boundaries)\")\n            \n            # creating a dictionary with the start and end times stored as tuples\n            if start_time is not None and end_time is not None:\n                time_pairs = {('manual_start', 'manual_end'): (start_time, end_time)}\n                print(f\"Selected START & END Time Pairs : {time_pairs}\")\n            \n    elif choice == '2': \n        # Identify a Marker stream \n        marker_stream = [stream for stream in streams if stream['info']['type'][0] == 'Markers'][0]\n        \n        print(\"Marker stream: \")\n        for i in range(len(marker_stream['time_series'])):\n            print(str(marker_stream['time_series'][i]).replace('[', '').replace(']', '').replace(\"'\", \"\"))   ## Print all the events in the marker stream in an organized way\n       \n        \n        choice_2 = simpledialog.askstring(\"Method Based on Marker Event \", \n                                          \"Choose one option:\\n1. Only 1 START & 1 END Events \\n2. MULTIPLE START & END Events\")\n \n        if choice_2 == '1': \n            \n            method = 'marker_ONEpair'\n            \n            print('Method: ' + method)\n            \n            start_event = simpledialog.askstring(\"SINGLE PAIR Marker Event\", \n                                                 \"Enter the START event name (you can copy and paste the event name from the terminal)\")\n            end_event = simpledialog.askstring(\"SINGLE PAIR Marker Event\", \n                                               \"Enter the END event name (you can copy and paste the event name from the terminal)\")\n            \n            print(f\"Selected START Event: {start_event}\")\n            print(f\"Selected END Event: {end_event}\")\n            \n            # Find the corresponding LSL times in the Marker stream based on start_event and end_event\n            start_time = None\n            end_time = None\n            for idx, event in enumerate(marker_stream['time_series']):  # enumerate the events (i.e., 'time_series in XDF stream') in the marker stream\n                if event[0] == start_event.strip():\n                    start_time = marker_stream['time_stamps'][idx]  # Find the corresponding LSL time in the Marker stream based on start_event\n                elif event[0] == end_event.strip():\n                    end_time = marker_stream['time_stamps'][idx]  # Find the corresponding LSL time in the Marker stream based on end_event\n            \n            if start_time is None or end_time is None:\n                print(f\"Could not find both events: {start_event} or {end_event}\")\n                \n            # create a dictionary with the start and end events and the corresponding start and end times stored as tuples\n            time_pairs = {(start_event.strip(), end_event.strip()): (start_time, end_time)}   \n            print(f\"Selected START & END Time Pairs : {time_pairs}\")\n            \n        elif choice_2 == '2':\n            \n            method = 'marker_MULTIPLEpairs'\n            \n            print('Method: ' + method)\n            \n            event_list = simpledialog.askstring(\"MULTIPLE PAIRS Marker Event\",\n                                                \"Enter each PAIR of START and END events SEQUENTIALLY (you can copy and paste the event name from the terminal). \"\n                                                \"Follow the format exactly: START1:END1, START2:END2, START3:END3, etc.\") \n            \n            # Convert the event_list into a list of tuples\n            if event_list:\n                try:\n                    # First, split by commas to get each pair, and ensure no extra spaces\n                    event_pairs = [pair.split(':') for pair in event_list.replace(\" \", \"\").split(',')]\n                    \n                    # Ensure that each pair contains exactly two elements (start and end)\n                    for pair in event_pairs:\n                        if len(pair) != 2:\n                            raise ValueError(f\"Invalid format for pair: {pair}\")\n                except Exception as e:\n                    print(f\"Error in the provided input: {e}\")\n                    event_pairs = []\n\n                print(f\"Selected START & END Event Pairs: {event_pairs}\")\n\n            # Find the corresponding LSL times in the Marker stream based on the event_pairs and store them in a dictionary\n            time_pairs = {}  # Initialize an empty dictionary to store the time pairs\n            for start_event, end_event in event_pairs:  # Iterate over each pair to extract corresponding start and end times\n                start_time = None\n                end_time = None\n                for idx, event in enumerate(marker_stream['time_series']):\n                    if event[0] == start_event.strip():\n                        start_time = marker_stream['time_stamps'][idx]\n                    elif event[0] == end_event.strip():\n                        end_time = marker_stream['time_stamps'][idx]\n\n                # If both times are found, add them to the dictionary\n                if start_time is not None and end_time is not None:\n                   time_pairs[(start_event.strip(), end_event.strip())] = (start_time, end_time)    # Store the events and correspdonding times as a tuple in the dictionary\n                else:\n                    print(f\"Could not find both events: {start_event} or {end_event}\")\n\n            print(f\"Selected START & END Time Pairs : {time_pairs}\")  \n                        \n\n    return time_pairs, method\n\n\nprint(\"Function \\\"find_start_end_times\\\" created sucesfully\")\n\n\n\n\n# Function to extract specified events (with correspodning LSL times) from XDF stream (useful for plotting)\ndef get_events(stream, event_names):\n    \"\"\"\n    Extracts events and corresponding LSL times from the given stream that match any of the event_names.\n\n    Parameters:\n    stream (dict): The stream containing time stamps and event data.\n    event_names (list of str): List of event name substrings to look for in the events.\n\n    Returns:\n    np.array: An array where each row contains a timestamp and the full event name.\n    \"\"\"\n    events = []  # Initialize an empty list to store matching events\n\n    # Check if the stream type is \"Markers\"\n    if stream['info']['type'][0] != \"Markers\":\n        raise ValueError(f\"ERROR: The stream provided ({stream['info']['name'][0]}) is not a Marker stream\")\n\n    # Iterate over the time stamps and corresponding events in the stream\n    for timestamp, event in zip(stream['time_stamps'], stream['time_series']):\n        # Check if any of the specified event names are in the current event\n        for name in event_names:\n            if name in event[0]:\n                # If a match is found, append the timestamp and full event name to the list\n                events.append([timestamp, event[0]])\n\n    # Convert the list of events to a NumPy array and return it\n    return np.array(events)\n\nprint(\"Function \\\"get_events\\\" created sucesfully\") \n\nInput folder = f:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\data_raw\nOutput folder = f:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\data_processed\nFunction \"to_audio\" created sucesfully\nFunction \"rename_streams\" created sucesfully\nFunction \"find_start_end_times\" created sucesfully\nFunction \"get_events\" created sucesfully"
  },
  {
    "objectID": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing - Copy.html#identifying-xdf-files-in-input-folder-or-any-subfolder",
    "href": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing - Copy.html#identifying-xdf-files-in-input-folder-or-any-subfolder",
    "title": "Mobile Multimodal Lab (MML): XDF Preprocessing",
    "section": "2. Identifying XDF files in Input Folder or any Subfolder",
    "text": "2. Identifying XDF files in Input Folder or any Subfolder\n\nxdf_files = []  # Initialize an empty list to store paths of XDF files\n\n# Traverse through the directory and its subdirectories to find XDF files\nfor root, dirs, files in os.walk(input_folder):  # 1st loop iterating over the results returned by os.walk().\n    \n    for file in files:                                    # 2nd loop iterating through each file in the current directory\n        \n        if file.endswith(\".xdf\"):                         # checking if the file has and XDF extension \n            \n             xdf_files.append(os.path.join(root, file))   # if the file is an XDF file, append its full path to the xdf_files list\n            \nprint('We have idenified the following XDF files: ' + str(xdf_files))\n\nWe have idenified the following XDF files: ['./data_raw/pilot_ming_02.xdf', './data_raw/T1_experiment.xdf']"
  },
  {
    "objectID": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing - Copy.html#a.-alternatively-the-user-can-select-their-own-xdf-file",
    "href": "2_PREPROCESSING/1_XDF_PROCESSING/MML_XDF_Preprocessing - Copy.html#a.-alternatively-the-user-can-select-their-own-xdf-file",
    "title": "Mobile Multimodal Lab (MML): XDF Preprocessing",
    "section": "2a. Alternatively, the user can select their own XDF file",
    "text": "2a. Alternatively, the user can select their own XDF file\n\nimport tkinter # GUI toolkit to open and save files\nfrom tkinter import filedialog # GUI toolkit to open and save files\n\nroot = tkinter.Tk()\nroot.attributes('-topmost',True)\nroot.iconify()\n\nxdf_files = filedialog.askopenfilename(title=\"Select an XDF file\", filetypes=[(\"XDF Files\", \"*.xdf\")], multiple = 'True')\n\nroot.destroy()\n\n# Convert the tuple returned by askopenfilenames() to a list\nxdf_files = list(xdf_files)\n\nprint('You have selected the following XDF files: ' + str(xdf_files))\n\nYou have selected the following XDF files: ['F:/Mobile-Multimodal-Lab/2_PREPROCESSING/XDF_PROCESSING/data_raw/T1_experiment.xdf']"
  },
  {
    "objectID": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html",
    "href": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html",
    "title": "MOBILE MULTIMODAL LAB (MML): Video Clipping and Audio-Video Alignment",
    "section": "",
    "text": "Donders MML LOGO.png"
  },
  {
    "objectID": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html#importing-relevant-packages",
    "href": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html#importing-relevant-packages",
    "title": "MOBILE MULTIMODAL LAB (MML): Video Clipping and Audio-Video Alignment",
    "section": "0. Importing Relevant Packages",
    "text": "0. Importing Relevant Packages\n\nimport os             # Importing the os module which provides functions for interacting with the operating system\nimport pyxdf          # Importing pyxdf, a Python library for reading XDF files\nimport glob           # Importing the glob module which helps in finding files/directories with specific patterns\nimport pandas as pd   # Importing pandas library (abbreviated as pd), which is used for data manipulation and analysis\nimport numpy as np    # Importing numpy library (abbreviated as np), which is used for numerical computations\nimport wave           # Importing wave module for reading and writing WAV files (usually audio files) \nimport struct         # Importing struct module which provides functions to convert between Python values and C structs\nimport math           # Importing math module which provides mathematical functions\nimport random         # Importing random module for generating random numbers\nfrom scipy.io import wavfile  # Importing wavfile module from scipy.io (a library built on numpy), for reading and writing WAV files\nimport noisereduce as nr      # Importing noisereduce module for noise reduction in audio signals\nimport json            # Importing json module for working with JSON data\nimport cv2            # Importing OpenCV library for computer vision tasks\nfrom moviepy.editor import (                # Importing various classes and functions from moviepy.editor module\n                            VideoFileClip,  # Class for working with video files\n                            AudioFileClip,  # Class for working with audio files\n                            CompositeAudioClip)  # Class for composing audio clip\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip  # video  clipping fucntion \nfrom moviepy.video.io.VideoFileClip import VideoFileClip          # alternative video clipping function\nimport matplotlib.pyplot as plt                                   # Importing pyplot library to create figures and plot data \nfrom matplotlib.widgets import Slider  \nimport tkinter                                                    # GUI toolkit to open and save files\nfrom tkinter import filedialog                                    # GUI toolkit to open and save files\nimport subprocess \nfrom tqdm.notebook import tqdm\nimport re                                                         # Importing re module for working with regular expressions\n\n\nprint(\"Everything was imported succesfully\") #as terminal\n\nEverything was imported succesfully"
  },
  {
    "objectID": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html#establish-relevant-paths-variabls-functions",
    "href": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html#establish-relevant-paths-variabls-functions",
    "title": "MOBILE MULTIMODAL LAB (MML): Video Clipping and Audio-Video Alignment",
    "section": "1. Establish Relevant Paths, Variabls & Functions",
    "text": "1. Establish Relevant Paths, Variabls & Functions\n\n# ------------ PATHS -----------------------------------------------------\ninput_video_folder = r'\\\\fileserver.dccn.nl\\project\\3025011.01\\pilots\\pilot_data_davide'\n#input_file_folder   = '../1_XDF_PROCESSING/data_processed/lateststart_earliestend'    # this folder contains the csv files extracted from the XDF files (data_processed)\n#input_file_folder =   '../1_XDF_PROCESSING/data_processed_temp/manual_startend'\ninput_file_folder   = '../1_XDF_PROCESSING/data_processed_final/marker_3pairs'    # this folder contains the csv files extracted from the XDF files (data_processed)\noutput_video_folder = './video_clipped/marker_3pairs/'     # this folder will contain the clipped videos\noutput_audiovideo_folder = './audiovideo_merged/marker_3pairs'     # this folder will contain the audio files extracted from the videos\n#output_overlay_folder = './audiovideo_overlay/lateststart_earliestend' \n\nprint(\"Input video folder =\", os.path.abspath(input_video_folder))\nprint(\"Input file folder =\", os.path.abspath(input_file_folder))\nprint(\"Output_video folder =\", os.path.abspath(output_video_folder))\nprint(\"Output_audiovideo folder =\", os.path.abspath(output_audiovideo_folder))\n\n\n# ------------ VARIABLES -----------------------------------------------------\n# Dictionary to map file extensions to codecs\nextension_to_codec = {\n    '.mp4': 'libx264',\n    '.avi': 'XVID',\n    '.mov': 'libx264',\n    '.mkv': 'libx264',\n    '.flv': 'flv',\n    # Add more mappings as needed\n                    }\n\n\n# AUDIO-VIDEO MATCHING: Regular expression pattern to match the participant's video file name. Change as needed. \naudio_video_pattern_matching = re.compile(r'pilot_(\\d+)_d(\\d+)')\n# pilot_01_d2_speech_lsl_Mic_latest_start_earliest_end_denoised.wav\n# pilot_05_d2_speech_output_comp\n# pilot_01_d2_speech_lsl_Video_latest_start_earliest_end_denoised.wav\n\n\"\"\"In our case the files are named as # pilot_xx_dx_speech_lsl_Mic_latest_start_earliest_end_denoised.wav and # pilot_xx_dx_speech_output_comp\n        where x are changing expression inside this regular pattern. \n            pilot_: Matches the static string \"pilot_\".\n            (\\d+): Captures one or more digits (xx in your case).\n            _d(\\d+): Captures the dynamic d part followed by digits (dx).\n            _speech: Matches the static string _speech.\n    The regular expression is used to match the file names between audios and videos audiio audio-video alignment\"\"\"\n\n\n\n# Dictionary to map file extensions to FourCC codecs\nextension_to_codec = {\n    '.mp4': 'H264',  # 'H264' is a valid FourCC code for H.264\n    '.avi': 'XVID',  # 'XVID' is a common FourCC for AVI\n    '.mov': 'H264',  # Again, use 'H264' or 'MP4V'\n    '.mkv': 'H264',  # Use 'H264' for MKV files too\n    '.flv': 'FLV1',  # 'FLV1' is a valid codec for FLV\n    # Add more mappings as needed\n}\n\n\n# ------------ FUNCTIONS -----------------------------------------------------\n# VIDEO: Creating a function named frame_to_time to convert frame number to time format \ndef frame_to_time(frame, fps):\n    \"\"\"\n    frame_to_time converts a given frame number to a time format (HH:MM:SS.SS) based on the frames per second (fps).\n    Arguments:\n        frame (int): The frame number to be converted.\n        fps (float): The frames per second of the video.\n    Returns:\n        str: The time format as a string in the format \"HH:MM:SS.SS\".\n    \"\"\"\n    seconds = frame / fps\n    hours = int(seconds // 3600)\n    minutes = int((seconds % 3600) // 60)\n    seconds = seconds % 60\n    return f\"{hours:02}:{minutes:02}:{seconds:.2f}\"\n\nprint(\"Function \\\"frame_to_time\\\" created sucesfully\") \n\n\n\n# Function to extract specified events (with correspodning LSL times) from XDF stream (useful for plotting)\ndef get_events(stream, event_names):\n    \"\"\"\n    Extracts events and corresponding LSL times from the given stream that match any of the event_names.\n\n    Parameters:\n    stream (dict): The stream containing time stamps and event data.\n    event_names (list of str): List of event name substrings to look for in the events.\n\n    Returns:\n    np.array: An array where each row contains a timestamp and the full event name.\n    \"\"\"\n    events = []  # Initialize an empty list to store matching events\n\n    # Check if the stream type is \"Markers\"\n    if stream['info']['type'][0] != \"Markers\":\n        raise ValueError(f\"ERROR: The stream provided ({stream['info']['name'][0]}) is not a Marker stream\")\n\n    # Iterate over the time stamps and corresponding events in the stream\n    for timestamp, event in zip(stream['time_stamps'], stream['time_series']):\n        # Check if any of the specified event names are in the current event\n        for name in event_names:\n            if name in event[0]:\n                # If a match is found, append the timestamp and full event name to the list\n                events.append([timestamp, event[0]])\n\n    # Convert the list of events to a NumPy array and return it\n    return np.array(events)\n\nprint(\"Function \\\"get_events\\\" created sucesfully\") \n\nInput video folder = \\\\fileserver.dccn.nl\\project\\3025011.01\\pilots\\pilot_data_davide\nInput file folder = f:\\SpeakUp-2.0\\2_PREPROCESSING\\1_XDF_PROCESSING\\data_processed_final\\marker_3pairs\nOutput_video folder = f:\\SpeakUp-2.0\\2_PREPROCESSING\\2_Audio_Video_Sync\\video_clipped\\marker_3pairs\nOutput_audiovideo folder = f:\\SpeakUp-2.0\\2_PREPROCESSING\\2_Audio_Video_Sync\\audiovideo_merged\\marker_3pairs\nFunction \"frame_to_time\" created sucesfully\nFunction \"get_events\" created sucesfully\n\n\n\n2. Identify Relevant CVS files, Audio Files and Videos by navigating through Input Folders\n\n## 1. FIND LSL VIDEO FILES (csv files) IN THE INPUT FOLDER\nLSL_video_list = []  # Initialize an empty list to store paths of video files (csv LSL data)\ntry:\n    if os.path.exists(input_file_folder):  # Check if the input folder exists\n       \n        # Traverse through the directory and its subdirectories to find XDF files\n        for root, dirs, files in os.walk(os.path.abspath(input_file_folder)):  # Walk through the directory tree\n          \n            for file in files:\n                \n                if \"speech\" in file and 'Video' in file and file.endswith(\".csv\"):  # Check that the words \"speech\" and \"Video\" are in the file name and that it is a CSV file\n                    \n                    LSL_video_list.append(os.path.join(root, file))  # Append the full path to the xdf_files list\n                    \n        if LSL_video_list:  # Check if any files were found\n            print(f'We have identified the following LSL video files: {LSL_video_list}')\n        \n        else:  # If no XDF files were found\n            print('No LSL video files found.')\n            \n    else: # If the input folder does not exist or is not accessible\n        print(f'The folder {input_file_folder} does not exist or is not accessible.')\n\nexcept Exception as e:\n    print(f\"An error occurred: {str(e)}\")\n    \n    \n    \n    \n## 2. FIND AUDIO FILES (.wav files) IN THE INPUT FILE FOLDER\naudio_list = []  # Initialize an empty list to store paths of video files (csv LSL data)\ntry:\n    if os.path.exists(input_video_folder):  # Check if the input folder exists\n       \n        # Traverse through the directory and its subdirectories to find XDF files\n        for root, dirs, files in os.walk(os.path.abspath(input_file_folder)):  # Walk through the directory tree\n          \n            for file in files:\n                \n                if \"speech\" in file and 'Mic' in file and 'denoised' in file and file.endswith(\".wav\"):  # Check that the words \"speech\" and \"Mic\" and \"denoised\" are in the file name and that it is a wav file\n                    \n                    audio_list.append(os.path.join(root, file))  # Append the full path to the xdf_files list\n                    \n        if audio_list:  # Check if any files were found\n            print(f'We have identified the following Audio files: {audio_list}')\n        \n        else:  # If no  files were found\n            print('No audio files found.')\n            \n    else: # If the input folder does not exist or is not accessible\n        print(f'The folder {input_file_folder} does not exist or is not accessible.')\n\nexcept Exception as e:\n    print(f\"An error occurred: {str(e)}\")\n    \n    \n    \n## 2. FIND RAW VIDEO FILES IN THE INPUT VIDEO FOLDER\nraw_video_list = []  # Initialize an empty list to store paths of video files (csv LSL data)\ntry:\n    if os.path.exists(input_video_folder):  # Check if the input folder exists\n       \n        # Traverse through the directory and its subdirectories to find XDF files\n        for root, dirs, files in os.walk(os.path.abspath(input_video_folder)):  # Walk through the directory tree\n          \n            for file in files:\n                \n                if \"speech\" in file and 'output_compr' in file and file.endswith(\".avi\"):  # Check that the words \"speech\" and \"output_compr\" are in the file name and that it is a avi file\n                    \n                    raw_video_list.append(os.path.join(root, file))  # Append the full path to the xdf_files list\n                    \n        if raw_video_list:  # Check if any files were found\n            print(f'We have identified the following Video files: {raw_video_list}')\n        \n        else:  # If no files were found\n            print('No video files found.')\n            \n    else: # If the input folder does not exist or is not accessible\n        print(f'The folder {input_video_folder} does not exist or is not accessible.')\n\nexcept Exception as e:\n    print(f\"An error occurred: {str(e)}\")\n\nWe have identified the following LSL video files: ['f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day2\\\\pilot_01_d2_speech_lsl_Video_Speech_Start_SUD_1_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day2\\\\pilot_01_d2_speech_lsl_Video_SUD_1_End_SUD_2_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day2\\\\pilot_01_d2_speech_lsl_Video_SUD_2_End_SUD_3_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day3\\\\pilot_01_d3_speech_lsl_Video_Speech_Start_SUD_1_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day3\\\\pilot_01_d3_speech_lsl_Video_SUD_1_End_SUD_2_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day3\\\\pilot_01_d3_speech_lsl_Video_SUD_2_End_SUD_3_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day2\\\\pilot_02_d2_speech_lsl_Video_Speech_Start_SUD_1_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day2\\\\pilot_02_d2_speech_lsl_Video_SUD_1_End_SUD_2_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day2\\\\pilot_02_d2_speech_lsl_Video_SUD_2_End_SUD_3_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day3\\\\pilot_02_d3_speech_lsl_Video_Speech_Start_SUD_1_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day3\\\\pilot_02_d3_speech_lsl_Video_SUD_1_End_SUD_2_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day3\\\\pilot_02_d3_speech_lsl_Video_SUD_2_End_SUD_3_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_04\\\\day2\\\\pilot_04_d2_speech_lsl_Video_Speech_Start_SUD_1_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_04\\\\day2\\\\pilot_04_d2_speech_lsl_Video_SUD_1_End_SUD_2_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_04\\\\day2\\\\pilot_04_d2_speech_lsl_Video_SUD_2_End_SUD_3_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_05\\\\day2\\\\pilot_05_d2_speech_lsl_Video_Speech_Start_SUD_1_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_05\\\\day2\\\\pilot_05_d2_speech_lsl_Video_SUD_1_End_SUD_2_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_05\\\\day2\\\\pilot_05_d2_speech_lsl_Video_SUD_2_End_SUD_3_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_06\\\\day2\\\\pilot_06_d2_speech_lsl_Video_Speech_Start_SUD_1_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_06\\\\day2\\\\pilot_06_d2_speech_lsl_Video_SUD_1_End_SUD_2_Start.csv', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_06\\\\day2\\\\pilot_06_d2_speech_lsl_Video_SUD_2_End_SUD_3_Start.csv']\nWe have identified the following Audio files: ['f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day2\\\\pilot_01_d2_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day2\\\\pilot_01_d2_speech_lsl_Mic_SUD_1_End_SUD_2_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day2\\\\pilot_01_d2_speech_lsl_Mic_SUD_2_End_SUD_3_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day3\\\\pilot_01_d3_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day3\\\\pilot_01_d3_speech_lsl_Mic_SUD_1_End_SUD_2_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day3\\\\pilot_01_d3_speech_lsl_Mic_SUD_2_End_SUD_3_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day2\\\\pilot_02_d2_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day2\\\\pilot_02_d2_speech_lsl_Mic_SUD_1_End_SUD_2_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day2\\\\pilot_02_d2_speech_lsl_Mic_SUD_2_End_SUD_3_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day3\\\\pilot_02_d3_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day3\\\\pilot_02_d3_speech_lsl_Mic_SUD_1_End_SUD_2_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day3\\\\pilot_02_d3_speech_lsl_Mic_SUD_2_End_SUD_3_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_04\\\\day2\\\\pilot_04_d2_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_04\\\\day2\\\\pilot_04_d2_speech_lsl_Mic_SUD_1_End_SUD_2_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_04\\\\day2\\\\pilot_04_d2_speech_lsl_Mic_SUD_2_End_SUD_3_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_05\\\\day2\\\\pilot_05_d2_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_05\\\\day2\\\\pilot_05_d2_speech_lsl_Mic_SUD_1_End_SUD_2_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_05\\\\day2\\\\pilot_05_d2_speech_lsl_Mic_SUD_2_End_SUD_3_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_06\\\\day2\\\\pilot_06_d2_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_06\\\\day2\\\\pilot_06_d2_speech_lsl_Mic_SUD_1_End_SUD_2_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_06\\\\day2\\\\pilot_06_d2_speech_lsl_Mic_SUD_2_End_SUD_3_Start_denoised.wav']\nWe have identified the following Video files: ['\\\\\\\\fileserver.dccn.nl\\\\project\\\\3025011.01\\\\pilots\\\\pilot_data_davide\\\\pilot_01\\\\day2\\\\pilot_01_d2_speech_output_compr.avi', '\\\\\\\\fileserver.dccn.nl\\\\project\\\\3025011.01\\\\pilots\\\\pilot_data_davide\\\\pilot_01\\\\day3\\\\pilot_01_d3_speech_output_compr.avi', '\\\\\\\\fileserver.dccn.nl\\\\project\\\\3025011.01\\\\pilots\\\\pilot_data_davide\\\\pilot_02\\\\day2\\\\pilot_02_d2_speech_output_compr.avi', '\\\\\\\\fileserver.dccn.nl\\\\project\\\\3025011.01\\\\pilots\\\\pilot_data_davide\\\\pilot_02\\\\day3\\\\pilot_02_d3_speech_output_compr.avi', '\\\\\\\\fileserver.dccn.nl\\\\project\\\\3025011.01\\\\pilots\\\\pilot_data_davide\\\\pilot_03\\\\day2\\\\pilot_03_d2_speech_output_compr.avi', '\\\\\\\\fileserver.dccn.nl\\\\project\\\\3025011.01\\\\pilots\\\\pilot_data_davide\\\\pilot_04\\\\day2\\\\pilot_04_d2_speech_output_compr.avi', '\\\\\\\\fileserver.dccn.nl\\\\project\\\\3025011.01\\\\pilots\\\\pilot_data_davide\\\\pilot_05\\\\day2\\\\pilot_05_d2_speech_output_compr.avi', '\\\\\\\\fileserver.dccn.nl\\\\project\\\\3025011.01\\\\pilots\\\\pilot_data_davide\\\\pilot_06\\\\day2\\\\pilot_06_d2_speech_output_compr.avi']"
  },
  {
    "objectID": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html#clipping-videos-based-on-.csv-frames-and-lsl-times",
    "href": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html#clipping-videos-based-on-.csv-frames-and-lsl-times",
    "title": "MOBILE MULTIMODAL LAB (MML): Video Clipping and Audio-Video Alignment",
    "section": "3. Clipping Videos Based on .CSV frames and LSL times",
    "text": "3. Clipping Videos Based on .CSV frames and LSL times\n\n##1. LOAD LSL VIDEO FILES  \n# Loop through the files \nfor file in LSL_video_list: \n    \n    print(f'Processing {file}')\n    \n    file_path = os.path.join(os.path.abspath(input_file_folder), file)  # Create the file path for the CSV file\n    \n    file_data = pd.read_csv(file_path)\n        \n    # Find the start and end frame numbers in the CSV file \n    start_frame = file_data.iloc[0, 1]    # Get the first frame (first row, index 0) in the second coluimn (index 1)\n    end_frame   = file_data.iloc[-1, 1]   # Get the last frame (last row, index -1) in the second coluimn (index 1)\n    \n    fnam = os.path.basename(file)[:-4]     # Extract the file name from the path and assings it to fnam, whilst removing the '.csv' extension (i.e., the last 4 characters in the string)\n    \n    participant_num = '_'.join(fnam.split('_')[0:2])  # Extract the participant number from the file name by splitting the string at the underscores and selecting the first two elements\n    \n    day_num = fnam.split('_')[2]  # Extract the day number from the file name by splitting the string at the underscores and selecting the third element\n\n    condition = '_'.join(fnam.split('_')[6:])  # Extract the condition from the file name by splitting the string at the underscores and selecting the elements from the 7th to the last element\n    \n    video_extension = '.avi'\n    \n    # Create the output subfolder for the clipped videos\n    subfolder_path = os.path.join(os.path.abspath(output_video_folder), participant_num, day_num, f'{participant_num}_{day_num}_{condition}{video_extension}')  # Create the output subfolder for the clipped video with the correct extension\n    print(f'Subfolder path: {subfolder_path}')\n    # Make sure the output folder exists, if not create it\n    if not os.path.exists(os.path.dirname(subfolder_path)):\n        os.makedirs(os.path.dirname(subfolder_path), exist_ok=True)\n        \n    \n    # Extract the identifier from the audio file name that will be used to match the video file\n    LSL_video_match = audio_video_pattern_matching.search(fnam)\n    if not LSL_video_match :\n        # print(f'No matching for video: {fnam}')\n        continue  # Skip current iteration if the pattern is not found\n    \n    LSL_video_identifier = LSL_video_match.group(0)  # Extract the matched portion of the string from the regex result.\n    #print(f'Video identifier: {LSL_video_identifier}')  # Print the matched identifier for audio\n\n        \n    # Find the frame rate of the LSL frames (i.e., the number of frames per second)\n    LSL_frame_rate = (end_frame - start_frame) / (file_data.iloc[-1, 0] - file_data.iloc[0, 0])  # Calculate the frame rate by dividing the number of frames by the time difference between the first and last frame\n        \n        \n    # 2. LOAD CORRESPDONDING RAW VIDELoad\n    for video in raw_video_list :  # Loop through the raw video files to find the corresponding video file\n        \n        print(f'Now Loading the Video :  {os.path.basename(video)}')\n        \n        fnam = os.path.basename(video)[:-4]     # Extract the file name from the path and assings it to fnam, whilst removing the '.csv' extension (i.e., the last 4 characters in the string)\n        \n        # Extract the identifier from the audio file name that will be used to match the video file\n        raw_video_match = audio_video_pattern_matching.search(fnam)\n        if not raw_video_match :\n            # print(f'No matching for video: {fnam}')\n            continue  # Skip current iteration if the pattern is not found\n        \n        raw_video_identifier = raw_video_match.group(0)  # Extract the matched portion of the string from the regex result.\n        #print(f'Video identifier: {LSL_video_identifier}')  # Print the matched identifier for audio\n        \n        \n        # Check that the LSL and raw video identifiers match\n        if LSL_video_identifier == raw_video_identifier: \n            \n            print(f'Found matching raw video: {os.path.basename(video)} for LSL video: {os.path.basename(file)}')\n                \n            video_path = os.path.join(os.path.abspath(input_video_folder), video)\n             \n            capture = cv2.VideoCapture(video_path) # Load the video using OpenCV\n                \n            # Extract the relevant metadata from the video\n            video_frame_width  = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))  \n            video_frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))  \n            video_frame_rate   = capture.get(cv2.CAP_PROP_FPS)\n            video_tot_frames   = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) \n            \n            # Check video extension format and select the appropriate codec in the dictionary\n            video_extension = os.path.splitext(video_path)[1].lower()  # Extract the file extension from the video file path, ensuring it's in lowercase (e.g., '.mp4'), to enable case-insensitive matching in the dictionary\n            if video_extension in extension_to_codec:\n                codec = extension_to_codec[video_extension]\n            else:\n                raise ValueError(f\"ERROR: The video extension {video_extension} is not supported\")\n            \n            \n           # -------------------------------------------------------------------------------------------- \n            \n            # ## 3. CLIP THE VIDEO USING FFMPEG (FASTER, BASED ON TIMES (converted from frame numbers))\n           \n            # # Converting the LSL frames to the video time format to find start_cut and end_cut for the video\n            # start_cut_time = frame_to_time(start_frame, LSL_frame_rate)\n            # end_cut_time = frame_to_time(end_frame, LSL_frame_rate)\n\n            # print(\"start_cut_time\" + str(start_cut_time))\n            # print(\"end_cut_time\" + str(end_cut_time))\n\n            # print('Now cutting the video...')\n            \n            # # Construct output file path with the same extension\n            # subfolder_path = os.path.join(os.path.abspath(output_video_folder), participant_num, day_num, f'{participant_num}_{day_num}_speech_video_clipped{video_extension}')  # Create the output subfolder for the clipped video with the correct extension\n            # # Make sure the output folder exists, if not create it\n            # if not os.path.exists(os.path.dirname(subfolder_path)):\n            #     os.makedirs(os.path.dirname(subfolder_path), exist_ok=True)\n            \n            \n            # # Use ffmpeg to cut the video\n            # ffmpeg_command = [\n            #     'ffmpeg',\n            #     '-y',                  # Add -y flag to overwrite any existing files with the same name\n            #     '-i', video_path,\n            #     '-ss', start_cut_time,  # start time\n            #     '-to', end_cut_time,    # end time\n            #     '-c', 'copy',           # copy codec (no re-encoding)\n            #     subfolder_path]\n            \n\n            # # Execute the command\n            # subprocess.run(ffmpeg_command, check=True)\n\n            # print(f'Video saved as {subfolder_path}')\n\n            # # --------------------------------------------------------------------------------------------\n            \n            ## 3a CLIP THE VIDEO USING MOVIEPY (SLOWER, BASED ON TIME)  \n            # Assign video extension to Four Character Code (fourcc) for the codec\n            fourcc = cv2.VideoWriter_fourcc(*codec)\n            \n            # output_video_folder ='./video_clipped_temp'  \n        \n                \n            # find the codec based on the video extension \n            if video_extension in extension_to_codec:\n                codec = extension_to_codec[video_extension]\n            else:\n                raise ValueError(f\"ERROR: The video extension {video_extension} is not supported\")\n            \n            # assign code to Four Character Code (fourcc) for the codec \n            fourcc = cv2.VideoWriter_fourcc(*codec)\n                \n            # Initialize the VideoWriter object to write frames to a new video file based on the fourcc codec, LSL frame rate, video frame size, and subfolder path\n            out = cv2.VideoWriter(subfolder_path, fourcc, LSL_frame_rate, (video_frame_width, video_frame_height))  \n            \n            # Write the selected frames to the a new clipped video \n            capture.set(cv2.CAP_PROP_POS_FRAMES, start_frame)  # Set the video capture object to the start frame\n            with tqdm(total= end_frame - start_frame + 1,   desc=\"Rewriting Video Progress\", leave=False, ncols=100) as pbar:    # Create a progress bar for the video frames\n\n                frame_count = start_frame  # Initialize the frame count to the start frame\n            \n                while capture.isOpened() and frame_count &lt;= end_frame:  # Loop through the video frames from the start frame until the end frame is reached\n\n                    try: \n                        ret, frame = capture.read()   # Read the next frame from the video\n                        if ret:   # If the frame is read correctly\n                            frame_count += 1\n                            pbar.update(1)\n                            out.write(frame)  # Write the frame to the new video\n                        else:\n                            break\n                        \n                    except Exception as e:  # Catch any errors that occur during the frame processing\n                        print(f\"An error occurred at frame {frame_count}: {e}\")\n                        break  # Stop processing if an error occurs\n                    \n                \n            # Release the video capture and video writer objects\n            capture.release()\n            out.release()\n        \n        print(f'Clipped video saved to: {subfolder_path}')\n            \n        print(\"\\n\")\n        \n    else: # If no matching raw video is found\n        print(f'No matching raw video found for LSL video: {os.path.basename(file)}')\n                \n                \nprint(\"All videos have been clipped successfully. Look into your folder \" + output_video_folder) #as terminal            # Create the output subfolder for the clipped videos"
  },
  {
    "objectID": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html#merging-clipped-vidoes-with-clipped-audios",
    "href": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html#merging-clipped-vidoes-with-clipped-audios",
    "title": "MOBILE MULTIMODAL LAB (MML): Video Clipping and Audio-Video Alignment",
    "section": "3. Merging (Clipped) Vidoes with (Clipped) Audios",
    "text": "3. Merging (Clipped) Vidoes with (Clipped) Audios\n\n## FIRST RE-CREATE A LIST OF THE CLIPPED VIDEO FILES \ninput_video_folder = './video_clipped/marker_3pairs'     # this folder contains the clipped videos\n\ncondition_1 = 'Speech_Start_SUD_1_Start'  # The condition to search for in the file name\n\ncondition_2 = 'SUD_1_End_SUD_2_Start'  # The condition to search for in the file name\n\ncondition_3 = 'SUD_2_End_SUD_3_Start'  # The condition to search for in the file name\n\nclipped_video_list = []  # Initialize an empty list to store paths of video files (csv LSL data)\ntry:\n    if os.path.exists(input_video_folder):  # Check if the input folder exists\n       \n        # Traverse through the directory and its subdirectories to find video files\n        for root, dirs, files in os.walk(os.path.abspath(input_video_folder)):  # Walk through the directory tree\n          \n            for file in files:\n                \n                if condition_1 in file and file.endswith(\".avi\"):  # Check that the words \"speech\" and \"output_compr\" are in the file name and that it is a avi file\n                    \n                    clipped_video_list.append(os.path.join(root, file))  # Append the full path to the xdf_files list\n                    \n        if clipped_video_list :  # Check if any files were found\n            print(f'We have identified the following Video files: {clipped_video_list}')\n        \n        else:  # If no files were found\n            print('No video files found.')\n            \n    else: # If the input folder does not exist or is not accessible\n        print(f'The folder {input_video_folder} does not exist or is not accessible.')\n\nexcept Exception as e:\n    print(f\"An error occurred: {str(e)}\")\n\n\n\n## Also RE-CREATE A LIST OF THE AUDIO FILES TO match only 1 condition at at time \n## 2. FIND AUDIO FILES (.wav files) IN THE INPUT FILE FOLDER\naudio_list = []  # Initialize an empty list to store paths of video files (csv LSL data)\ntry:\n    if os.path.exists(input_video_folder):  # Check if the input folder exists\n       \n        # Traverse through the directory and its subdirectories to find XDF files\n        for root, dirs, files in os.walk(os.path.abspath(input_file_folder)):  # Walk through the directory tree\n          \n            for file in files:\n                \n                if condition_1 in file and 'Mic' in file and 'denoised' in file and file.endswith(\".wav\"):  # Check that the words \"speech\" and \"Mic\" and \"denoised\" are in the file name and that it is a wav file\n                    \n                    audio_list.append(os.path.join(root, file))  # Append the full path to the xdf_files list\n                    \n        if audio_list:  # Check if any files were found\n            print(f'We have identified the following Audio files: {audio_list}')\n        \n        else:  # If no  files were found\n            print('No audio files found.')\n            \n    else: # If the input folder does not exist or is not accessible\n        print(f'The folder {input_file_folder} does not exist or is not accessible.')\n\nexcept Exception as e:\n    print(f\"An error occurred: {str(e)}\")\n\nWe have identified the following Video files: ['f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\video_clipped\\\\marker_3pairs\\\\pilot_01\\\\d2\\\\pilot_01_d2_Speech_Start_SUD_1_Start.avi', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\video_clipped\\\\marker_3pairs\\\\pilot_01\\\\d3\\\\pilot_01_d3_Speech_Start_SUD_1_Start.avi', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\video_clipped\\\\marker_3pairs\\\\pilot_02\\\\d2\\\\pilot_02_d2_Speech_Start_SUD_1_Start.avi', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\video_clipped\\\\marker_3pairs\\\\pilot_02\\\\d3\\\\pilot_02_d3_Speech_Start_SUD_1_Start.avi', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\video_clipped\\\\marker_3pairs\\\\pilot_04\\\\d2\\\\pilot_04_d2_Speech_Start_SUD_1_Start.avi', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\video_clipped\\\\marker_3pairs\\\\pilot_05\\\\d2\\\\pilot_05_d2_Speech_Start_SUD_1_Start.avi', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\video_clipped\\\\marker_3pairs\\\\pilot_06\\\\d2\\\\pilot_06_d2_Speech_Start_SUD_1_Start.avi']\nWe have identified the following Audio files: ['f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day2\\\\pilot_01_d2_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_01\\\\day3\\\\pilot_01_d3_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day2\\\\pilot_02_d2_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_02\\\\day3\\\\pilot_02_d3_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_04\\\\day2\\\\pilot_04_d2_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_05\\\\day2\\\\pilot_05_d2_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav', 'f:\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed_final\\\\marker_3pairs\\\\pilot_06\\\\day2\\\\pilot_06_d2_speech_lsl_Mic_Speech_Start_SUD_1_Start_denoised.wav']\n\n\n\n# THEN WE USE THESE CLIPPED VIDEOS AND ALIGNM THEM TO THE CORREPSONDING AUDIO \n# LOADING AUDIOS: Loop over files in the input_file_folder to extract the relevant audio files\n\n\nfor audio in audio_list:    \n        \n    print(f'Processing the audio file:  {audio}')\n    \n    audio_path = os.path.join(os.path.abspath(input_file_folder), audio)\n    \n    fnam = os.path.basename(audio)[:-4]  \n    \n    participant_num = '_'.join(fnam.split('_')[0:2])  # Extract the participant number from the file name by splitting the string at the underscores and selecting the first two elements\n    print(f'Participant number: {participant_num}')  # Print the participant number\n    \n    day_num = fnam.split('_')[2]  # Extract the day number from the file name by splitting the string at the underscores and selecting the third element\n    print(f'Day number: {day_num}')  # Print the day number\n\n    condition = '_'.join(fnam.split('_')[6:])  # Extract the condition from the file name by splitting the string at the underscores and selecting the elements from the 7th to the last element\n    \n    # Extract the identifier from the audio file name that will be used to match the video file\n    audio_match = audio_video_pattern_matching.search(fnam)\n    if not audio_match:\n        print(f'No matching for audio: {audio}')\n        continue  # Skip current iteration if the pattern is not found\n    \n    audio_identifier = audio_match.group(0)  # Extract the matched portion of the string from the regex result.\n    #print(f'Audio identifier: {audio_identifier}')  # Print the matched identifier for audio\n\n\n    # LOADING VIDEOS: Loop over video files to select the corresponding video file\n    for video in clipped_video_list:\n        \n        fnam = os.path.basename(video)[:-4]  \n        \n        # Extract the identifier from the video file name that will be used to match the audio file\n        video_match = audio_video_pattern_matching.search(fnam)\n        if not video_match:\n            print(f'No matching for video: {video}')\n            continue  # Skip current iteration if the pattern is not found\n        \n        video_identifier = video_match.group(0)  # Extract the matched portion of the string from the regex result.\n        #print(f'Video identifier: {video_identifier}')  # Print the matched identifier for video\n\n    \n        # Check if the identifiers from the audio and video files match\n        if audio_identifier == video_identifier: \n            \n            video_path = os.path.join(os.path.abspath(input_video_folder), video)\n            video_extension = os.path.splitext(video_path)[1].lower()  # Ensure correct video extension\n            \n            print(f'Found matching video: {video} for audio: {audio}')\n            \n            # Create output folder if it doesn't exist\n            os.makedirs(output_audiovideo_folder, exist_ok=True)\n            \n            # ALIGNMENT: Combining Audio and Video using ffmpeg \n            subfolder_path = os.path.join(os.path.abspath(output_audiovideo_folder), participant_num, day_num, f'{video_identifier}_{condition}_audiovideo_merged.{video_extension[1:]}')\n            \n            #make sure the output folder exists, if not create it\n            if not os.path.exists(os.path.dirname(subfolder_path)):\n                os.makedirs(os.path.dirname(subfolder_path), exist_ok=True)\n                \n            \n            # Construct the ffmpeg command to combine the audio and video files\n            ffmpeg_command = [\n                'ffmpeg',              # Call the ffmpeg tool\n                '-y',                  # Overwrite the output file without asking\n                '-i', video_path,      # Specify the input video file\n                '-i', audio_path,      # Specify the input audio file\n                '-c:v', 'copy',        # Copy the video stream without re-encoding\n                '-c:a', 'aac',         # Re-encode the audio stream to AAC format (optimized for playback devices)\n                '-strict', 'experimental',  # Enable experimental features (needed for AAC encoding in some versions)\n                subfolder_path         # Specify the output file path (video with audio)\n            ]\n            \n            # Run the command \n            print('Now combining this Audio and Video')\n            try:\n                result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n                #print(result.stdout)\n                \n                print(f'Video with audio saved as {subfolder_path}')\n                \n            except subprocess.CalledProcessError as e:\n                print(f\"Error combining audio and video {video_path} and {audio_path}: {e.stderr}\")\n                    \n            print('\\n')\n            \nprint(f'Done, you can now look into the folder: {output_audiovideo_folder}')"
  },
  {
    "objectID": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html#optional-overlay-lsl-time-on-the-audiovideos",
    "href": "2_PREPROCESSING/2_AudioVideo_Sync/MML_VideoAudio_Sync.html#optional-overlay-lsl-time-on-the-audiovideos",
    "title": "MOBILE MULTIMODAL LAB (MML): Video Clipping and Audio-Video Alignment",
    "section": "4. (Optional) Overlay LSL Time on the AudioVideos",
    "text": "4. (Optional) Overlay LSL Time on the AudioVideos\n\nThis step is useful to double check whether the Marker & Event informaiton was coded correctly during the recording.\n\n## 1.  AGAIN, FIND ALL THE AUDIO-VIDEO SYNCED FILES IN THE INPUT FOLDER \ninput_audiovideo_folder = './audiovideo_sync/lateststart_earliestend'     # this folder contains the audio-video synced files\n\naudiovideo_list = []  # Initialize an empty list to store paths of video files (csv LSL data)\ntry:\n    if os.path.exists(input_audiovideo_folder):  # Check if the input folder exists\n       \n        # Traverse through the directory and its subdirectories to find XDF files\n        for root, dirs, files in os.walk(os.path.abspath(input_audiovideo_folder)):  # Walk through the directory tree\n          \n            for file in files:\n                \n                if \"synced\" in file and file.endswith(\".avi\"):  # Check that the words \"speech\" and \"output_compr\" are in the file name and that it is a avi file\n                    \n                    audiovideo_list.append(os.path.join(root, file))  # Append the full path to the xdf_files list\n                    \n        if audiovideo_list :  # Check if any files were found\n            print(f'We have identified the following audiovideo files: {audiovideo_list}')\n        \n        else:  # If no files were found\n            print('No video files found.')\n            \n    else: # If the input folder does not exist or is not accessible\n        print(f'The folder {input_audiovideo_folder} does not exist or is not accessible.')\n\nexcept Exception as e:\n    print(f\"An error occurred: {str(e)}\")\n\n\n## 2. FIND THE CORRESPONDING LSL VIDEO FILES IN THE INPUT FOLDER\n# We use the LSL_video_list that was created before\nprint(f'We will use the following LSL Video Files to overlay LSL Times: {LSL_video_list}')\n\nWe have identified the following audiovideo files: ['c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\audiovideo_sync\\\\lateststart_earliestend\\\\pilot_01\\\\d2\\\\pilot_01_d2_speech_audiovideo_synced.avi', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\audiovideo_sync\\\\lateststart_earliestend\\\\pilot_01\\\\d3\\\\pilot_01_d3_speech_audiovideo_synced.avi', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\audiovideo_sync\\\\lateststart_earliestend\\\\pilot_02\\\\d2\\\\pilot_02_d2_speech_audiovideo_synced.avi', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\audiovideo_sync\\\\lateststart_earliestend\\\\pilot_02\\\\d3\\\\pilot_02_d3_speech_audiovideo_synced.avi', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\audiovideo_sync\\\\lateststart_earliestend\\\\pilot_03\\\\d2\\\\pilot_03_d2_speech_audiovideo_synced.avi', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\audiovideo_sync\\\\lateststart_earliestend\\\\pilot_04\\\\d2\\\\pilot_04_d2_speech_audiovideo_synced.avi', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\audiovideo_sync\\\\lateststart_earliestend\\\\pilot_05\\\\d2\\\\pilot_05_d2_speech_audiovideo_synced.avi', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\2_Audio_Video_Sync\\\\audiovideo_sync\\\\lateststart_earliestend\\\\pilot_06\\\\d2\\\\pilot_06_d2_speech_audiovideo_synced.avi']\nWe will use the following LSL Video Files to overlay LSL Times: ['c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed\\\\lateststart_earliestend\\\\pilot_01\\\\day2\\\\pilot_01_d2_speech_lsl_Video_latest_start_earliest_end.csv', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed\\\\lateststart_earliestend\\\\pilot_01\\\\day3\\\\pilot_01_d3_speech_lsl_Video_latest_start_earliest_end.csv', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed\\\\lateststart_earliestend\\\\pilot_02\\\\day2\\\\pilot_02_d2_speech_lsl_Video_latest_start_earliest_end.csv', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed\\\\lateststart_earliestend\\\\pilot_02\\\\day3\\\\pilot_02_d3_speech_lsl_Video_latest_start_earliest_end.csv', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed\\\\lateststart_earliestend\\\\pilot_03\\\\day2\\\\pilot_03_d2_speech_lsl_Video_latest_start_earliest_end.csv', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed\\\\lateststart_earliestend\\\\pilot_04\\\\day2\\\\pilot_04_d2_speech_lsl_Video_latest_start_earliest_end.csv', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed\\\\lateststart_earliestend\\\\pilot_05\\\\day2\\\\pilot_05_d2_speech_lsl_Video_latest_start_earliest_end.csv', 'c:\\\\Users\\\\ahmar\\\\OneDrive\\\\Documents\\\\GitHub\\\\SpeakUp-2.0\\\\2_PREPROCESSING\\\\1_XDF_PROCESSING\\\\data_processed\\\\lateststart_earliestend\\\\pilot_06\\\\day2\\\\pilot_06_d2_speech_lsl_Video_latest_start_earliest_end.csv']\n\n\n\n## 3 NOW WE OVERLAY LSL TIMES ON THE AUDIO-VIDEO SYNCED FILES\n\n\n# 1. Loading the relevant CSV files for each participant\nfor file in LSL_video_list: \n    \n    fnam = os.path.basename(file)[:-4]  # Extract the file name from the path and remove the '.csv' extension (i.e., the last 4 characters in the string)\n    \n    print(f'Processing the LSL video file {file}')\n    \n    file_path = os.path.join(os.path.abspath(input_file_folder), file)\n    \n    participant_num = '_'.join(fnam.split('_')[0:2])  # Extract the participant number from the file name by splitting the string at the underscores and selecting the first two elements\n    \n    day_num = fnam.split('_')[2]  # Extract the day number from the file name by splitting the string at the underscores and selecting the third element\n    \n    # Reading the CSV file \n    file_data = pd.read_csv(file) # Reads the CSV file at the constructed path into a DataFrame called file_data \n\n    # Extract LSL time and frame number from CSV\n    lsl_times = file_data.iloc[:, 0]  \n    lsl_frames = file_data.iloc[:, 1]  \n    \n    # Extract LSL video identifier \n    LSL_video_match = audio_video_pattern_matching.search(fnam)\n    if not LSL_video_match: \n        continue \n    LSL_video_identifier = LSL_video_match.group(0) \n    \n        \n    \n    # 2. Loading the corresponding audiovideo files \n    \n    for audiovideo in audiovideo_list: \n        \n        fnam = os.path.basename(audiovideo)[:-4]  # Extract the file name from the path and remove the '.avi' extension (i.e., the last 4 characters in the string)\n        \n        # Extract the audiovideo identifier based on the pattern: \n        audiovideo_match = audio_video_pattern_matching.search(fnam)\n       #print(f'Video identifier: {video_identifier}')  # Print the matched identifier for video\n\n        \n        # Check tha that the LSL and the audio video identifier match \n        \n        if audiovideo_identifier == LSL_video_identifier: \n            \n            audiovideo_path = os.path.join(os.path.abspath(input_audiovideo_folder), audiovideo)\n            \n            print(\"Found matching audiovideo file: \" + os.path.basename(audiovideo) + \"for LSL video \" + os.path.basename(file)) \n            \n            \n            # Now Loading the video and extracting relevant metadata \n            print('Now loading video: ' + audiovideo)\n            capture = cv2.VideoCapture(audiovideo_path)  # Load the video using OpenCV\n            \n            video_frame_width  = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))  \n            video_frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))  \n            video_frame_rate   = capture.get(cv2.CAP_PROP_FPS)\n            video_tot_frames   = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n               \n            \n            # Find the codec based on the video extension\n            video_extension = os.path.splitext(audiovideo_path)[1].lower()  # Extract the file extension from the video file path, ensuring it's in lowercase (e.g., '.mp4'), to enable case-insensitive matching in the dictionary\n            if video_extension in extension_to_codec: \n                codec = extension_to_codec[video_extension]\n            else:\n                raise ValueError(f\"ERROR: The video extension {video_extension} is not supported\")\n            \n            # Create the Output Folder for the overlayed videos\n            subfolder_path = os.path.join(os.path.abspath(output_overlay_folder), participant_num, day_num, f'{participant_num}_{day_num}_Speech_Video_overlayed{video_extension}')  # Create the output subfolder for the clipped video with the correct extension\n            # Make sure the output folder exists, if not create it\n            if not os.path.exists(os.path.dirname(subfolder_path)):\n                os.makedirs(os.path.dirname(subfolder_path), exist_ok=True)\n            \n            # Create the output video writer\n            fourcc = cv2.VideoWriter_fourcc(*codec)  \n            \n            video_writer = cv2.VideoWriter(subfolder_path, fourcc, video_frame_rate, (video_frame_width, video_frame_height))\n            \n            capture = cv2.VideoCapture(audiovideo_path)  \n            \n            print('Now overlaying LSL time onto video frames for video ' + str(os.path.basename(audiovideo)) + \" with csv file  \" + str(os.path.basename(file)))\n            \n\n            # Process the video frame by frame\n            for i in tqdm(range(video_tot_frames), desc=\"Processing Video\", unit=\"frame\"):\n                ret, frame = capture.read()  # Read a frame\n                if not ret:\n                    break  # Exit loop if no more frames are available\n                \n                # Get the actual frame number from the video capture object\n                current_frame_number = capture.get(cv2.CAP_PROP_POS_FRAMES)\n                print(\"Current frame number: \" + str(current_frame_number))\n\n                # Check if the frame counter is within the range of LSL frames\n                if lsl_frames.iloc[0] &lt;= frame_counter &lt;= lsl_frames.iloc[-1]:\n                    # Find the corresponding LSL time and frame number based on the frame counter\n                    match = file_data[file_data.iloc[:, 1] == int(current_frame_number)]\n                    if not match.empty:\n                        lsl_time = match.iloc[0, 0]  # LSL time\n                        lsl_frame = match.iloc[0, 1]  # Frame number\n\n                        # Overlay the LSL information onto the video frame\n                        overlay_text = f'LSL Time: {lsl_time:.3f}, Frame: {int(lsl_frame)}'\n                        # Calculate the center position for the text\n                        text_size = cv2.getTextSize(overlay_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]\n                        text_x = (video_frame_width - text_size[0]) // 2  # X-coordinate\n                        text_y = (video_frame_height + text_size[1]) // 2  # Y-coordinate\n                        cv2.putText(frame, overlay_text, \n                                    (text_x, text_y),  # Position in the center\n                                    cv2.FONT_HERSHEY_SIMPLEX, 1, \n                                    (255, 255, 255), 2)  # White text with thickness of 2\n\n                # Write the frame to the output video, regardless of overlay\n                video_writer.write(frame)\n\n            # Release resources after processing all frames\n            capture.release()\n            video_writer.release()\n\n            print(f'Video saved as {subfolder_path}')\n            \n            print(\"\\n\")\n                        \n            \n\nprint(\"Done with processing all videos! You can now look into your folder: \" + output_overlay_folder)"
  },
  {
    "objectID": "3_MOTION_TRACKING/2_Video_Calibration/Calibration_anipose.html",
    "href": "3_MOTION_TRACKING/2_Video_Calibration/Calibration_anipose.html",
    "title": "Donders MML: Calibration with Anipose",
    "section": "",
    "text": "Donders MML LOGO.png"
  },
  {
    "objectID": "3_MOTION_TRACKING/2_Video_Calibration/Calibration_anipose.html#importing-necessary-packages",
    "href": "3_MOTION_TRACKING/2_Video_Calibration/Calibration_anipose.html#importing-necessary-packages",
    "title": "Donders MML: Calibration with Anipose",
    "section": "0. Importing Necessary Packages",
    "text": "0. Importing Necessary Packages\n\nimport os                                # Importing the os module which provides functions for interacting with the operating system\nimport glob                              # Importing glob module for finding files and directories matching a specified pattern\nimport cv2                               # Importing OpenCV library for computer vision tasks like image and video processing\nimport aniposelib                        # Importing aniposelib for multi-camera calibration and 3D reconstruction tasks\nfrom aniposelib.boards import CharucoBoard, Checkerboard  # Importing specific board types for camera calibration from aniposelib\nfrom aniposelib.cameras import Camera, CameraGroup        # Importing Camera and CameraGroup classes for handling multiple camera setups in aniposelib\nfrom aniposelib.utils import load_pose2d_fnames           # Importing utility function for loading 2D pose filenames from aniposelib\n# import table                             # Importing the table module, likely used for handling data in tabular format (usage context-dependent)\n\n\nprint(\"Everything was imported succesfully\") #as terminal\n\n\n## Check whether python 3.9 is necessary or the most recent version is also fine\n\nEverything was imported succesfully\n\n\nWe use the following charucoboard, which when printed on a A1, gives the below dimensions you need to set for your anipose settings. \n\n## Here is an example of the videos used for calibration\nfrom IPython.display import Video\nvideo_path = r\"C:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\2_MOTION_TRACKING\\2_Video Calibration\\calibration_videos\\charuco_calibration_oneover250s_high_quality_2024-04-23_output_compr - Trim_cam1.avi\"\n# Display the video\nVideo(video_path)\n\n\n      Your browser does not support the video element."
  },
  {
    "objectID": "3_MOTION_TRACKING/2_Video_Calibration/Calibration_anipose.html#defyining-relevant-directories-variables-functions",
    "href": "3_MOTION_TRACKING/2_Video_Calibration/Calibration_anipose.html#defyining-relevant-directories-variables-functions",
    "title": "Donders MML: Calibration with Anipose",
    "section": "1. Defyining relevant Directories, Variables & Functions",
    "text": "1. Defyining relevant Directories, Variables & Functions\n\n# ---------- DIRECTORIES --------------------------\ninput_folder  = './calibration_videos_split/test/'   # input folder with the raw video files to calibrate (relative path) \noutput_folder = './calibration_results/P2/'  # output folder to store the TOML results (relative path) \n\nprint(\"Input folder =\", os.path.abspath(input_folder))\nprint(\"Output folder =\", os.path.abspath(output_folder))\n\n\n# ---------- VARIABLES -------------------------\n# Videos Requirements\nvideo_extension = '.avi'      # Video format extension avi. Change as needed (e.g., .mp4)\nnum_videos      = 3           # We are working with 3 complementary videos (1 original video split in 3 from the 3 camera angles) \nboard_type      = \"charuco\"   # Change accordingly (e.g., to Checker)\n\n#Cameras\ncamera_names   = ['cam1', 'cam2', 'cam3']   # name of each camera view, consistent with video file names!!\n\n#Board. IMPORTANT TO HAVE THE RIGHT SETTINGS - OTHERWISE THE CALIBRATION WILL NOT WORK\nboard = CharucoBoard(7, 5,                   #  because we had 7 squares in the vertical direction and 5 squares in the borizontal direction.\n                     square_length=108,      #  dimensions of each square on the board is 108 mm\n                     marker_length=85,       #  dimensions of each marker within each square is 85 mm\n                     marker_bits=4,          #  each marker consists of 4x4 bits (default)   !!! WRITE IN INSTRUCTIONS. \n                     dict_size=50)           #  50 types of markers in the marker dictionary (default).\n\nInput folder = e:\\research_flesh\\GtSTcollab\\calibration_videos_split\\test\nOutput folder = e:\\research_flesh\\GtSTcollab\\calibration_results\\P2\n\n\n\nprint(video_files)\n\n[]"
  },
  {
    "objectID": "3_MOTION_TRACKING/2_Video_Calibration/Calibration_anipose.html#calibrating-videos-using-anipose-in-loop",
    "href": "3_MOTION_TRACKING/2_Video_Calibration/Calibration_anipose.html#calibrating-videos-using-anipose-in-loop",
    "title": "Donders MML: Calibration with Anipose",
    "section": "2. Calibrating Videos using Anipose (in Loop)",
    "text": "2. Calibrating Videos using Anipose (in Loop)\n\nvideo_files = [ ]                                     # Initialize an empty list to store paths of video files\nfor root, dirs, videos in os.walk(os.path.abspath(input_folder)):      # 1st loop iterating over the results returned by os.walk(). Traverse through the directory and its subdirectories to find video files\n    \n    for video in videos:                              # 2nd loop iterating through each file in the current directory  \n        \n        if len(videos)== num_videos and video.endswith(video_extension) and board_type in video: # checking that (1) there are exactly 3 videos in the folder, (2) each video has the specified video_extension (e.g., '.avi') and (3) they video files names contain right word of the board (e.g., charuco)\n            video_files.append([os.path.join(root, video).replace(\"/\", \"\\\\\")])                   # Append the file path using double backslashes and wrap each path in a list\n            \n        else: \n            print(\"ERROR: You subfolder \" + str(root) + \" does not have the specified requirements of num_videos: \" +str(num_videos) + \" video_extension: \" + str(video_extension) + \" and board_type: \" + str(board_type))\n            break\n            \n    print('Anipose will calibrate the following videos:  ' + str(video_files))\n    print() #empty line for space\n\n    #-------- Calibration --------\n    cgroup = CameraGroup.from_names(camera_names)    # cgroup is an object used to refer to all of the cameras as a single unit. \n    \n    cgroup.calibrate_videos(video_files, board)      # Calibration of videos in video_files, using the board speicification\n\n    # Saving\n    output_name = str(num_videos) + 'vid_' + video.split('output_compr')[0] + '_calibration_anipose.toml' #Creating the name of output_file with numvid_name video before word \"output_compr\"_calibration_anipose\n    print() \n    print(\"Saving \" + output_name)\n    cgroup.dump(output_folder + output_name)  #Saving calibration results \n\nprint(\"Calibration Finished. You can now look into your folder: \" + str(output_folder ))\n\n\n\n## SAVE THE OUTPUT OF THE CALIBRATION AS A TEXT FILE (SO WE SAVE THE ERROR)\n\nAnipose will calibrate the following videos:  [['f:\\\\Mobile-Multimodal-Lab\\\\2_PREPROCESSING\\\\3_MOTION_TRACKING\\\\2_Video_Calibration\\\\calibration_videos_split\\\\test\\\\charuco_calibration_oneover250s_high_quality_2024-04-23_output_compr - Trim_cam1.avi'], ['f:\\\\Mobile-Multimodal-Lab\\\\2_PREPROCESSING\\\\3_MOTION_TRACKING\\\\2_Video_Calibration\\\\calibration_videos_split\\\\test\\\\charuco_calibration_oneover250s_high_quality_2024-04-23_output_compr - Trim_cam2.avi'], ['f:\\\\Mobile-Multimodal-Lab\\\\2_PREPROCESSING\\\\3_MOTION_TRACKING\\\\2_Video_Calibration\\\\calibration_videos_split\\\\test\\\\charuco_calibration_oneover250s_high_quality_2024-04-23_output_compr - Trim_cam3.avi']]\n\nf:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\3_MOTION_TRACKING\\2_Video_Calibration\\calibration_videos_split\\test\\charuco_calibration_oneover250s_high_quality_2024-04-23_output_compr - Trim_cam1.avi\n0 boards detected\nf:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\3_MOTION_TRACKING\\2_Video_Calibration\\calibration_videos_split\\test\\charuco_calibration_oneover250s_high_quality_2024-04-23_output_compr - Trim_cam2.avi\n0 boards detected\nf:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\3_MOTION_TRACKING\\2_Video_Calibration\\calibration_videos_split\\test\\charuco_calibration_oneover250s_high_quality_2024-04-23_output_compr - Trim_cam3.avi\n0 boards detected\n\n\n100%|████████████████████████████| 1001/1001 [00:02&lt;00:00, 335.54it/s]\n100%|████████████████████████████| 1001/1001 [00:02&lt;00:00, 344.61it/s]\n100%|████████████████████████████| 1001/1001 [00:02&lt;00:00, 357.94it/s]\n\n\nValueError: not enough values to unpack (expected 2, got 0)\n\n\n\nprint(len(videos))\n\n3"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_Movement\\trial_1\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_Movement\\trial_3\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_NoMovement\\trial_4\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_2/trial_2.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_2/trial_2.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_2/trial_2.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_2/trial_2.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_NoMovement\\trial_2\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_2/trial_2.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_2/trial_2.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_2/trial_2.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_2/trial_2.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_4/trial_actually4.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_4/trial_actually4.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_4/trial_actually4.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_4/trial_actually4.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_NoMovement\\trial_actually4\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_4/trial_actually4.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_4/trial_actually4.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_4/trial_actually4.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_4/trial_actually4.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_1/trial_1.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_1/trial_1.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_1/trial_1.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_1/trial_1.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\Vision_Movement\\trial_1\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_1/trial_1.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_1/trial_1.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_1/trial_1.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_1/trial_1.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_4/trial_4.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_4/trial_4.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_4/trial_4.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_4/trial_4.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\Vision_Movement\\trial_4\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_4/trial_4.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_4/trial_4.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_4/trial_4.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_Movement/trial_4/trial_4.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_1/trial_1.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_1/trial_1.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_1/trial_1.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_1/trial_1.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\Vision_NoMovement\\trial_1\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_1/trial_1.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_1/trial_1.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_1/trial_1.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_1/trial_1.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_3/trial_3.html",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_3/trial_3.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_3/trial_3.html#define-variables",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_3/trial_3.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\Vision_NoMovement\\trial_3\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_3/trial_3.html#plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_3/trial_3.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_3/trial_3.html#d-plotting",
    "href": "3_MOTION_TRACKING/3_freemocap/marker_MULTIPLEpairs/P1/Vision_NoMovement/trial_3/trial_3.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "3_MOTION_TRACKING/4_ShutterSpeedCheckAnalysis/shutterspeedcheck.html",
    "href": "3_MOTION_TRACKING/4_ShutterSpeedCheckAnalysis/shutterspeedcheck.html",
    "title": "",
    "section": "",
    "text": "import os\nimport mediapipe as mp\nimport cv2\nimport glob as glob\n\n# files \ndata_local = './Data_Local/'\nslowSP = glob.glob(data_local + '*oneover60*')\nslowSP.sort()\nfastSP = glob.glob(data_local + '*oneover250*')\nfastSP.sort()\n# print\nprint(slowSP)\nprint(fastSP)\n\n['./Data_Local\\\\oneover60_cam1.mp4', './Data_Local\\\\oneover60_cam2.mp4', './Data_Local\\\\oneover60_cam3.mp4']\n['./Data_Local\\\\oneover250_cam1.mp4', './Data_Local\\\\oneover250_cam2.mp4', './Data_Local\\\\oneover250_cam3.mp4']\n\n\n\nimport os\nimport mediapipe as mp\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom pathlib import Path\n\n# Initialize MediaPipe\nmp_holistic = mp.solutions.holistic\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\n\nclass ShutterSpeedDiagnostics:\n    def __init__(self, data_dir):\n        self.data_dir = data_dir\n        self.slow_files = sorted(glob(os.path.join(data_dir, '*oneover60*')))\n        self.fast_files = sorted(glob(os.path.join(data_dir, '*oneover250*')))\n        self.results = {}\n        \n    def calculate_jitter(self, positions, window=5):\n        \"\"\"Calculate jitter using second derivative (acceleration)\"\"\"\n        if len(positions) &lt; window:\n            return np.nan\n        \n        # Calculate velocity (first derivative)\n        velocity = np.diff(positions, axis=0)\n        # Calculate acceleration (second derivative)\n        acceleration = np.diff(velocity, axis=0)\n        # Calculate jitter (magnitude of acceleration)\n        jitter = np.linalg.norm(acceleration, axis=1)\n        return np.mean(jitter)\n    \n    def analyze_video(self, video_path, shutter_speed):\n        \"\"\"Analyze a single video file\"\"\"\n        cap = cv2.VideoCapture(video_path)\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        camera = Path(video_path).stem.split('_')[-1]\n        \n        metrics = {\n            'shutter_speed': shutter_speed,\n            'camera': camera,\n            'fps': fps,\n            'total_frames': total_frames,\n            'hand_visibility': {'left': [], 'right': []},\n            'hand_confidence': {'left': [], 'right': []},\n            'pose_confidence': [],\n            'face_confidence': [],\n            'hand_positions': {'left': [], 'right': []},\n            'jitter': {'left': [], 'right': []}\n        }\n        \n        with mp_holistic.Holistic(\n            static_image_mode=False,\n            model_complexity=2,\n            enable_segmentation=False,\n            refine_face_landmarks=True,\n            min_detection_confidence=0.5,\n            min_tracking_confidence=0.5\n        ) as holistic:\n            \n            frame_idx = 0\n            while cap.isOpened():\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                \n                # Convert to RGB\n                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                \n                # Process frame\n                results = holistic.process(image)\n                \n                # Analyze hand visibility and confidence\n                for hand_side in ['left', 'right']:\n                    hand_landmarks = getattr(results, f'{hand_side}_hand_landmarks')\n                    \n                    if hand_landmarks:\n                        metrics['hand_visibility'][hand_side].append(1)\n                        \n                        # Calculate average confidence for hand landmarks\n                        confidences = []\n                        for landmark in hand_landmarks.landmark:\n                            if hasattr(landmark, 'visibility'):\n                                confidences.append(landmark.visibility)\n                        \n                        if confidences:\n                            metrics['hand_confidence'][hand_side].append(np.mean(confidences))\n                        else:\n                            metrics['hand_confidence'][hand_side].append(1.0)\n                        \n                        # Store hand position (using wrist as reference)\n                        wrist_landmark = hand_landmarks.landmark[0]\n                        metrics['hand_positions'][hand_side].append([\n                            wrist_landmark.x,\n                            wrist_landmark.y,\n                            wrist_landmark.z\n                        ])\n                    else:\n                        metrics['hand_visibility'][hand_side].append(0)\n                        metrics['hand_confidence'][hand_side].append(0)\n                        metrics['hand_positions'][hand_side].append([np.nan, np.nan, np.nan])\n                \n                # Analyze pose confidence\n                if results.pose_landmarks:\n                    pose_confidences = []\n                    for landmark in results.pose_landmarks.landmark:\n                        if hasattr(landmark, 'visibility'):\n                            pose_confidences.append(landmark.visibility)\n                    \n                    if pose_confidences:\n                        metrics['pose_confidence'].append(np.mean(pose_confidences))\n                    else:\n                        metrics['pose_confidence'].append(1.0)\n                else:\n                    metrics['pose_confidence'].append(0)\n                \n                # Analyze face confidence\n                if results.face_landmarks:\n                    # Calculate center face confidence (eyes, nose, mouth area)\n                    center_landmarks = [33, 133, 362, 263, 1, 61, 291]\n                    face_confidences = []\n                    for idx in center_landmarks:\n                        if idx &lt; len(results.face_landmarks.landmark):\n                            landmark = results.face_landmarks.landmark[idx]\n                            if hasattr(landmark, 'visibility'):\n                                face_confidences.append(landmark.visibility)\n                    \n                    if face_confidences:\n                        metrics['face_confidence'].append(np.mean(face_confidences))\n                    else:\n                        metrics['face_confidence'].append(1.0)\n                else:\n                    metrics['face_confidence'].append(0)\n                \n                frame_idx += 1\n                \n                # Print progress\n                if frame_idx % 30 == 0:\n                    print(f\"Processed {frame_idx}/{total_frames} frames for {video_path}\")\n        \n        cap.release()\n        \n        # Calculate final metrics\n        metrics['hand_visibility_percentage'] = {\n            'left': (np.mean(metrics['hand_visibility']['left']) * 100),\n            'right': (np.mean(metrics['hand_visibility']['right']) * 100)\n        }\n        \n        # FIXED: Handle empty lists and NaN values properly\n        metrics['average_hand_confidence'] = {\n            'left': np.mean([c for c in metrics['hand_confidence']['left'] if c &gt; 0]) if any(c &gt; 0 for c in metrics['hand_confidence']['left']) else np.nan,\n            'right': np.mean([c for c in metrics['hand_confidence']['right'] if c &gt; 0]) if any(c &gt; 0 for c in metrics['hand_confidence']['right']) else np.nan\n        }\n        \n        # Calculate jitter for each hand\n        for hand_side in ['left', 'right']:\n            positions = np.array(metrics['hand_positions'][hand_side])\n            # Remove NaN values\n            valid_positions = positions[~np.isnan(positions).any(axis=1)]\n            if len(valid_positions) &gt; 10:\n                jitter = self.calculate_jitter(valid_positions)\n                metrics['jitter'][hand_side] = jitter\n            else:\n                metrics['jitter'][hand_side] = np.nan\n        \n        # FIXED: Handle empty lists properly\n        metrics['average_pose_confidence'] = np.mean([c for c in metrics['pose_confidence'] if c &gt; 0]) if any(c &gt; 0 for c in metrics['pose_confidence']) else np.nan\n        metrics['average_face_confidence'] = np.mean([c for c in metrics['face_confidence'] if c &gt; 0]) if any(c &gt; 0 for c in metrics['face_confidence']) else np.nan\n        \n        return metrics\n    \n    def run_analysis(self):\n        \"\"\"Run analysis on all videos\"\"\"\n        print(\"Analyzing slow shutter speed videos (1/60)...\")\n        for video_path in self.slow_files:\n            self.results[Path(video_path).stem] = self.analyze_video(video_path, \"1/60\")\n        \n        print(\"\\nAnalyzing fast shutter speed videos (1/250)...\")\n        for video_path in self.fast_files:\n            self.results[Path(video_path).stem] = self.analyze_video(video_path, \"1/250\")\n    \n    def generate_visualizations(self, output_dir=\"analysis_output\"):\n        \"\"\"Generate the comprehensive visualization and save individual components separately\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Prepare data for visualization\n        data = []\n        for video_name, metrics in self.results.items():\n            for hand_side in ['left', 'right']:\n                data.append({\n                    'shutter_speed': metrics['shutter_speed'],\n                    'camera': metrics['camera'],\n                    'hand': hand_side,\n                    'visibility_percentage': metrics['hand_visibility_percentage'][hand_side],\n                    'average_confidence': metrics['average_hand_confidence'][hand_side],\n                    'jitter': metrics['jitter'][hand_side],\n                    'video': video_name\n                })\n        \n        df = pd.DataFrame(data)\n        \n        # 1. Create Comprehensive Visualization (exactly as shown in your image)\n        fig = plt.figure(figsize=(20, 16))\n        gs = fig.add_gridspec(3, 3, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1],\n                             hspace=0.3, wspace=0.3)\n        \n        fig.suptitle('Shutter Speed Performance Analysis', fontsize=24, fontweight='bold', y=0.98)\n        \n        # Top row - Hand Visibility Comparison\n        ax1 = fig.add_subplot(gs[0, :])\n        visibility_pivot = df.pivot_table(index=['camera', 'hand'], \n                                         columns='shutter_speed', \n                                         values='visibility_percentage', \n                                         aggfunc='mean')\n        visibility_pivot.plot(kind='bar', ax=ax1, width=0.8, alpha=0.9)\n        ax1.set_title('Hand Visibility by Camera and Shutter Speed', fontsize=16, pad=20)\n        ax1.set_ylabel('Visibility Percentage (%)', fontsize=12)\n        ax1.set_xlabel('')\n        ax1.legend(title='Shutter Speed', fontsize=11, title_fontsize=12)\n        ax1.grid(axis='y', alpha=0.3, linestyle='--')\n        ax1.set_ylim(85, 102)\n        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n        \n        # Middle row visualizations\n        ax2 = fig.add_subplot(gs[1, 0])\n        df_jitter = df.dropna(subset=['jitter'])\n        if not df_jitter.empty:\n            camera_jitter = df_jitter.groupby(['camera', 'shutter_speed'])['jitter'].mean().unstack()\n            camera_jitter.plot(kind='bar', ax=ax2, width=0.8)\n            ax2.set_title('Average Jitter by Camera', fontsize=14)\n            ax2.set_ylabel('Jitter Value', fontsize=12)\n            ax2.set_xlabel('')\n            ax2.legend(title='Shutter Speed', fontsize=10)\n            ax2.grid(axis='y', alpha=0.3, linestyle='--')\n            ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n        \n        ax3 = fig.add_subplot(gs[1, 1])\n        if not df_jitter.empty:\n            sns.boxplot(data=df_jitter, x='shutter_speed', y='jitter', \n                       hue='hand', ax=ax3, palette=['#3498db', '#e74c3c'])\n            ax3.set_title('Jitter Distribution by Hand', fontsize=14)\n            ax3.set_ylabel('Jitter Value', fontsize=12)\n            ax3.set_xlabel('')\n            ax3.legend(title='Hand', fontsize=10)\n            ax3.grid(axis='y', alpha=0.3, linestyle='--')\n        \n        ax4 = fig.add_subplot(gs[1, 2])\n        improvements = []\n        for camera in df['camera'].unique():\n            for hand in ['left', 'right']:\n                slow = df[(df['camera'] == camera) & (df['shutter_speed'] == '1/60') & (df['hand'] == hand)]\n                fast = df[(df['camera'] == camera) & (df['shutter_speed'] == '1/250') & (df['hand'] == hand)]\n                \n                if not slow.empty and not fast.empty:\n                    vis_improvement = fast['visibility_percentage'].iloc[0] - slow['visibility_percentage'].iloc[0]\n                    improvements.append({\n                        'camera': camera,\n                        'hand': hand,\n                        'vis_improvement': vis_improvement\n                    })\n        \n        if improvements:\n            imp_df = pd.DataFrame(improvements)\n            imp_pivot = imp_df.pivot(index='camera', columns='hand', values='vis_improvement')\n            im = ax4.imshow(imp_pivot, cmap='RdYlGn', aspect='auto', vmin=-2, vmax=15)\n            ax4.set_title('Visibility Improvement (%)\\n(1/250 vs 1/60)', fontsize=14)\n            ax4.set_yticks(range(len(imp_pivot.index)))\n            ax4.set_yticklabels(imp_pivot.index)\n            ax4.set_xticks(range(len(imp_pivot.columns)))\n            ax4.set_xticklabels(imp_pivot.columns)\n            \n            for i in range(len(imp_pivot.index)):\n                for j in range(len(imp_pivot.columns)):\n                    text = ax4.text(j, i, f'{imp_pivot.iloc[i, j]:.1f}%',\n                                   ha='center', va='center', color='black', fontweight='bold')\n            \n            plt.colorbar(im, ax=ax4, label='Improvement (%)')\n        \n        # Bottom row - Summary Table\n        ax5 = fig.add_subplot(gs[2, :])\n        ax5.axis('tight')\n        ax5.axis('off')\n        \n        summary_data = []\n        for camera in sorted(df['camera'].unique()):\n            for shutter in sorted(df['shutter_speed'].unique()):\n                camera_data = df[(df['camera'] == camera) & (df['shutter_speed'] == shutter)]\n                \n                if not camera_data.empty:\n                    left_data = camera_data[camera_data['hand'] == 'left'].iloc[0]\n                    right_data = camera_data[camera_data['hand'] == 'right'].iloc[0]\n                    \n                    summary_data.append([\n                        camera,\n                        shutter,\n                        f\"{left_data['visibility_percentage']:.1f}%\",\n                        f\"{right_data['visibility_percentage']:.1f}%\",\n                        f\"{left_data['jitter']:.4f}\" if not pd.isna(left_data['jitter']) else \"N/A\",\n                        f\"{right_data['jitter']:.4f}\" if not pd.isna(right_data['jitter']) else \"N/A\"\n                    ])\n        \n        table = ax5.table(cellText=summary_data,\n                         colLabels=['Camera', 'Shutter Speed', 'Left Visibility', 'Right Visibility', \n                                   'Left Jitter', 'Right Jitter'],\n                         cellLoc='center',\n                         loc='center')\n        \n        table.auto_set_font_size(False)\n        table.set_fontsize(11)\n        table.auto_set_column_width(col=list(range(len(summary_data[0]))))\n        \n        for i in range(len(summary_data[0])):\n            table[(0, i)].set_facecolor('#4a86e8')\n            table[(0, i)].set_text_props(weight='bold', color='white')\n            table[(0, i)].set_height(0.06)\n        \n        for i in range(1, len(summary_data) + 1):\n            for j in range(len(summary_data[0])):\n                if i % 2 == 0:\n                    table[(i, j)].set_facecolor('#f0f0f0')\n                table[(i, j)].set_height(0.05)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'comprehensive_performance_analysis.png'), \n                    dpi=300, bbox_inches='tight', facecolor='white')\n        plt.close()\n        \n        # 2. Save Individual Components\n        \n        # 2.1 Hand Visibility by Camera and Shutter Speed\n        fig, ax = plt.subplots(figsize=(20, 8))\n        visibility_pivot.plot(kind='bar', ax=ax, width=0.8, alpha=0.9)\n        ax.set_title('Hand Visibility by Camera and Shutter Speed', fontsize=16, pad=20)\n        ax.set_ylabel('Visibility Percentage (%)', fontsize=12)\n        ax.set_xlabel('')\n        ax.legend(title='Shutter Speed', fontsize=11, title_fontsize=12)\n        ax.grid(axis='y', alpha=0.3, linestyle='--')\n        ax.set_ylim(85, 102)\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'hand_visibility_by_camera.png'), dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        # 2.2 Average Jitter by Camera\n        fig, ax = plt.subplots(figsize=(8, 8))\n        if not df_jitter.empty:\n            camera_jitter.plot(kind='bar', ax=ax, width=0.8)\n            ax.set_title('Average Jitter by Camera', fontsize=14)\n            ax.set_ylabel('Jitter Value', fontsize=12)\n            ax.set_xlabel('')\n            ax.legend(title='Shutter Speed', fontsize=10)\n            ax.grid(axis='y', alpha=0.3, linestyle='--')\n            ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'jitter_by_camera.png'), dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        # 2.3 Jitter Distribution by Hand\n        fig, ax = plt.subplots(figsize=(8, 8))\n        if not df_jitter.empty:\n            sns.boxplot(data=df_jitter, x='shutter_speed', y='jitter', \n                       hue='hand', ax=ax, palette=['#3498db', '#e74c3c'])\n            ax.set_title('Jitter Distribution by Hand', fontsize=14)\n            ax.set_ylabel('Jitter Value', fontsize=12)\n            ax.set_xlabel('')\n            ax.legend(title='Hand', fontsize=10)\n            ax.grid(axis='y', alpha=0.3, linestyle='--')\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'jitter_distribution.png'), dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        # 2.4 Visibility Improvement\n        fig, ax = plt.subplots(figsize=(8, 8))\n        if improvements:\n            im = ax.imshow(imp_pivot, cmap='RdYlGn', aspect='auto', vmin=-2, vmax=15)\n            ax.set_title('Visibility Improvement (%)\\n(1/250 vs 1/60)', fontsize=14)\n            ax.set_yticks(range(len(imp_pivot.index)))\n            ax.set_yticklabels(imp_pivot.index)\n            ax.set_xticks(range(len(imp_pivot.columns)))\n            ax.set_xticklabels(imp_pivot.columns)\n            \n            for i in range(len(imp_pivot.index)):\n                for j in range(len(imp_pivot.columns)):\n                    text = ax.text(j, i, f'{imp_pivot.iloc[i, j]:.1f}%',\n                                   ha='center', va='center', color='black', fontweight='bold')\n            \n            plt.colorbar(im, ax=ax, label='Improvement (%)')\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'visibility_improvement.png'), dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        # 2.5 Summary Table\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.axis('tight')\n        ax.axis('off')\n        \n        table = ax.table(cellText=summary_data,\n                         colLabels=['Camera', 'Shutter Speed', 'Left Visibility', 'Right Visibility', \n                                   'Left Jitter', 'Right Jitter'],\n                         cellLoc='center',\n                         loc='center')\n        \n        table.auto_set_font_size(False)\n        table.set_fontsize(11)\n        table.auto_set_column_width(col=list(range(len(summary_data[0]))))\n        \n        for i in range(len(summary_data[0])):\n            table[(0, i)].set_facecolor('#4a86e8')\n            table[(0, i)].set_text_props(weight='bold', color='white')\n            table[(0, i)].set_height(0.06)\n        \n        for i in range(1, len(summary_data) + 1):\n            for j in range(len(summary_data[0])):\n                if i % 2 == 0:\n                    table[(i, j)].set_facecolor('#f0f0f0')\n                table[(i, j)].set_height(0.05)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'summary_table.png'), dpi=300, bbox_inches='tight', facecolor='white')\n        plt.close()\n        \n        print(f\"\\nVisualizations saved to {output_dir}:\")\n        print(\"\\nComprehensive:\")\n        print(\"1. comprehensive_performance_analysis.png - All metrics in one figure\")\n        print(\"\\nIndividual Components:\")\n        print(\"2. hand_visibility_by_camera.png\")\n        print(\"3. jitter_by_camera.png\")\n        print(\"4. jitter_distribution.png\")\n        print(\"5. visibility_improvement.png\")\n        print(\"6. summary_table.png\")\n    \n    def save_detailed_report(self, output_path=\"./processed/detailed_report.csv\"):\n        \"\"\"Save detailed metrics to CSV\"\"\"\n        data_rows = []\n        \n        for video_name, metrics in self.results.items():\n            # Summarize frame-by-frame data\n            frames_with_both_hands = sum(1 for l, r in zip(metrics['hand_visibility']['left'], \n                                                          metrics['hand_visibility']['right']) \n                                        if l == 1 and r == 1)\n            \n            total_frames = metrics['total_frames']\n            both_hands_percentage = (frames_with_both_hands / total_frames) * 100 if total_frames &gt; 0 else 0\n            \n            row = {\n                'video': video_name,\n                'shutter_speed': metrics['shutter_speed'],\n                'camera': metrics['camera'],\n                'fps': metrics['fps'],\n                'total_frames': total_frames,\n                'left_hand_visibility': metrics['hand_visibility_percentage']['left'],\n                'right_hand_visibility': metrics['hand_visibility_percentage']['right'],\n                'both_hands_visible': both_hands_percentage,\n                'left_hand_confidence': metrics['average_hand_confidence']['left'],\n                'right_hand_confidence': metrics['average_hand_confidence']['right'],\n                'left_hand_jitter': metrics['jitter']['left'],\n                'right_hand_jitter': metrics['jitter']['right'],\n                'average_pose_confidence': metrics['average_pose_confidence'],\n                #'average_face_confidence': metrics['average_face_confidence']\n            }\n            data_rows.append(row)\n        \n        df = pd.DataFrame(data_rows)\n        df.to_csv(output_path, index=False)\n        \n        # Create summary statistics\n        summary = df.groupby(['shutter_speed', 'camera']).agg({\n            'left_hand_visibility': 'mean',\n            'right_hand_visibility': 'mean',\n            'left_hand_confidence': 'mean',\n            'right_hand_confidence': 'mean',\n            'left_hand_jitter': 'mean',\n            'right_hand_jitter': 'mean'\n        }).round(2)\n        \n        summary.to_csv(output_path.replace('.csv', '_summary.csv'))\n        \n        print(f\"\\nDetailed report saved to: {output_path}\")\n        print(f\"Summary report saved to: {output_path.replace('.csv', '_summary.csv')}\")\n        \n        return df, summary\n\n\n# Initialize diagnostics\ndiagnostics = ShutterSpeedDiagnostics('./Data_Local/')\n\n# Run analysis\ndiagnostics.run_analysis()\n\n# Generate visualizations\ndiagnostics.generate_visualizations()\n\n# Save detailed report\ndf, summary = diagnostics.save_detailed_report()\n# save\ndf.to_csv('./Processed/diagnostics_report.csv', index=False)\n\n# Print summary\nprint(\"\\nSUMMARY STATISTICS:\")\nprint(summary)\n\n# Print comparative analysis\nprint(\"\\nSHUTTER SPEED COMPARISON:\")\nfor metric in ['left_hand_visibility', 'right_hand_visibility', 'left_hand_jitter', 'right_hand_jitter']:\n    print(f\"\\n{metric}:\")\n    comparison = df.pivot_table(values=metric, index='camera', columns='shutter_speed', aggfunc='mean')\n    comparison['difference'] = comparison['1/250'] - comparison['1/60']\n    comparison['percentage_improvement'] = (comparison['difference'] / comparison['1/60']) * 100\n    print(comparison.round(2))\n\nAnalyzing slow shutter speed videos (1/60)...\nProcessed 30/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 60/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 90/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 120/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 150/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 180/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 210/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 240/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 270/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 300/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 330/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 360/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 390/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 420/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 450/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 480/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 510/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 540/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 570/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 600/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 630/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 660/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 690/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 720/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 750/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 780/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 810/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 840/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 870/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 900/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 930/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 960/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 990/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 1020/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 1050/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 1080/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 1110/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 1140/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 1170/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 1200/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 1230/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 1260/1279 frames for ./Data_Local\\oneover60_cam1.mp4\nProcessed 30/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 60/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 90/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 120/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 150/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 180/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 210/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 240/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 270/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 300/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 330/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 360/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 390/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 420/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 450/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 480/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 510/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 540/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 570/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 600/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 630/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 660/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 690/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 720/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 750/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 780/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 810/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 840/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 870/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 900/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 930/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 960/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 990/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 1020/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 1050/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 1080/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 1110/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 1140/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 1170/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 1200/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 1230/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 1260/1279 frames for ./Data_Local\\oneover60_cam2.mp4\nProcessed 30/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 60/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 90/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 120/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 150/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 180/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 210/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 240/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 270/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 300/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 330/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 360/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 390/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 420/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 450/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 480/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 510/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 540/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 570/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 600/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 630/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 660/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 690/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 720/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 750/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 780/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 810/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 840/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 870/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 900/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 930/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 960/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 990/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 1020/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 1050/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 1080/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 1110/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 1140/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 1170/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 1200/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 1230/1279 frames for ./Data_Local\\oneover60_cam3.mp4\nProcessed 1260/1279 frames for ./Data_Local\\oneover60_cam3.mp4\n\nAnalyzing fast shutter speed videos (1/250)...\nProcessed 30/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 60/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 90/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 120/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 150/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 180/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 210/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 240/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 270/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 300/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 330/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 360/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 390/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 420/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 450/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 480/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 510/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 540/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 570/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 600/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 630/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 660/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 690/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 720/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 750/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 780/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 810/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 840/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 870/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 900/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 930/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 960/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 990/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1020/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1050/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1080/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1110/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1140/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1170/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1200/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1230/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1260/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1290/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1320/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1350/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1380/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1410/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 1440/1453 frames for ./Data_Local\\oneover250_cam1.mp4\nProcessed 30/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 60/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 90/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 120/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 150/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 180/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 210/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 240/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 270/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 300/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 330/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 360/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 390/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 420/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 450/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 480/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 510/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 540/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 570/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 600/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 630/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 660/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 690/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 720/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 750/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 780/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 810/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 840/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 870/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 900/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 930/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 960/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 990/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1020/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1050/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1080/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1110/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1140/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1170/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1200/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1230/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1260/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1290/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1320/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1350/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1380/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1410/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 1440/1453 frames for ./Data_Local\\oneover250_cam2.mp4\nProcessed 30/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 60/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 90/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 120/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 150/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 180/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 210/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 240/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 270/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 300/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 330/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 360/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 390/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 420/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 450/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 480/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 510/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 540/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 570/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 600/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 630/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 660/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 690/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 720/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 750/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 780/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 810/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 840/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 870/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 900/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 930/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 960/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 990/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1020/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1050/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1080/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1110/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1140/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1170/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1200/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1230/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1260/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1290/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1320/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1350/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1380/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1410/1453 frames for ./Data_Local\\oneover250_cam3.mp4\nProcessed 1440/1453 frames for ./Data_Local\\oneover250_cam3.mp4\n\nVisualizations saved to analysis_output:\n\nComprehensive:\n1. comprehensive_performance_analysis.png - All metrics in one figure\n\nIndividual Components:\n2. hand_visibility_by_camera.png\n3. jitter_by_camera.png\n4. jitter_distribution.png\n5. visibility_improvement.png\n6. summary_table.png\n\nDetailed report saved to: ./processed/detailed_report.csv\nSummary report saved to: ./processed/detailed_report_summary.csv\n\nSUMMARY STATISTICS:\n                      left_hand_visibility  right_hand_visibility  \\\nshutter_speed camera                                                \n1/250         cam1                  100.00                  99.59   \n              cam2                   99.93                  99.79   \n              cam3                   98.55                  98.76   \n1/60          cam1                  100.00                  88.90   \n              cam2                   99.92                  90.70   \n              cam3                  100.00                  91.48   \n\n                      left_hand_confidence  right_hand_confidence  \\\nshutter_speed camera                                                \n1/250         cam1                     NaN                    NaN   \n              cam2                     NaN                    NaN   \n              cam3                     NaN                    NaN   \n1/60          cam1                     NaN                    NaN   \n              cam2                     NaN                    NaN   \n              cam3                     NaN                    NaN   \n\n                      left_hand_jitter  right_hand_jitter  \nshutter_speed camera                                       \n1/250         cam1                 0.0               0.00  \n              cam2                 0.0               0.00  \n              cam3                 0.0               0.00  \n1/60          cam1                 0.0               0.01  \n              cam2                 0.0               0.01  \n              cam3                 0.0               0.01  \n\nSHUTTER SPEED COMPARISON:\n\nleft_hand_visibility:\nshutter_speed   1/250    1/60  difference  percentage_improvement\ncamera                                                           \ncam1           100.00  100.00        0.00                    0.00\ncam2            99.93   99.92        0.01                    0.01\ncam3            98.55  100.00       -1.45                   -1.45\n\nright_hand_visibility:\nshutter_speed  1/250   1/60  difference  percentage_improvement\ncamera                                                         \ncam1           99.59  88.90       10.69                   12.02\ncam2           99.79  90.70        9.10                   10.03\ncam3           98.76  91.48        7.28                    7.96\n\nleft_hand_jitter:\nshutter_speed  1/250  1/60  difference  percentage_improvement\ncamera                                                        \ncam1             0.0   0.0         0.0                   41.28\ncam2             0.0   0.0         0.0                   41.03\ncam3             0.0   0.0         0.0                   37.55\n\nright_hand_jitter:\nshutter_speed  1/250  1/60  difference  percentage_improvement\ncamera                                                        \ncam1             0.0  0.01        -0.0                  -58.90\ncam2             0.0  0.01        -0.0                  -51.84\ncam3             0.0  0.01        -0.0                  -51.60\n\n\nD:\\Programs\\temp\\ipykernel_84792\\3472539026.py:327: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()"
  },
  {
    "objectID": "4a_PROCESSED/Script_FILTERING/MML_Multimodal_Filtering.html",
    "href": "4a_PROCESSED/Script_FILTERING/MML_Multimodal_Filtering.html",
    "title": "MML: Multimodal Data Cleaning",
    "section": "",
    "text": "This script will process and filter all the modalities - Audio: from raw (denoised) to pitch - ECG: filtered signal with bandpass and notch filer - EMG1 EMG 2: filtered with - RESP: filtered wit\n\n0. Import all necessary packages\n\nimport os\nfrom os import listdir\nimport numpy as np\nimport csv #csv saving\nimport pandas as pd\nimport math #basic operations\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter, filtfilt, iirnotch\nimport glob\nimport matplotlib.pyplot as plt\nimport tkinter # GUI toolkit to open and save files\nfrom tkinter import filedialog, messagebox  # GUI toolkit to open and save files\nfrom scipy.signal import butter, filtfilt, iirnotch\nimport librosa\nimport librosa.display\nimport tempfile\nimport shutil\nimport tqdm\nfrom scipy.ndimage import uniform_filter1d\nimport emd\n\n\nprint(\"Everything imported successfully\")\n\nEverything imported successfully\n\n\n\n1. Defyining key Directories, Variables & Functions\n\ninput_folder = r\"D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\raw_trials\"\noutput_folder = r\"D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\"\nprint(\"Input folder:\", input_folder)\nprint(\"Output folder:\", output_folder)  \n\n## --- FUNCTIONS --- ##\n\n### ------------------------------------------\n### Core Filtering Functions\n### ------------------------------------------\n\ndef butter_filter(data, cutoff, fs, order=4, filter_type='low'):\n    \"\"\"\n    General-purpose Butterworth filter (low, high, or bandpass) with zero-phase filtering.\n    Padding is applied to avoid edge artifacts.\n\n    Parameters:\n    - data: Signal to filter (1D array)\n    - cutoff: Cutoff frequency (float for low/high, tuple for bandpass)\n    - fs: Sampling rate in Hz\n    - order: Filter order\n    - filter_type: 'low', 'high', or 'band'\n\n    Returns:\n    - Filtered signal (same length as input)\n    \"\"\"\n    nyquist = 0.5 * fs\n    if filter_type == 'band':\n        low, high = cutoff\n        normal_cutoff = [low / nyquist, high / nyquist]\n    else:\n        normal_cutoff = cutoff / nyquist\n\n    b, a = butter(order, normal_cutoff, btype=filter_type, analog=False)\n    padded_data = np.pad(data, (1000, 1000), mode='edge')\n    filtered_data = filtfilt(b, a, padded_data)\n    return filtered_data[1000:-1000]\n\ndef notch_filter(signal, fs, notch_freq=50, quality_factor=30):\n    \"\"\"\n    Notch filter to remove powerline interference at specified frequency.\n    \n    Parameters:\n    - signal: 1D array of the signal\n    - fs: Sampling rate\n    - notch_freq: Frequency to notch out (e.g., 50 or 60 Hz)\n    - quality_factor: Q-factor determining notch sharpness\n    \n    Returns:\n    - Filtered signal\n    \"\"\"\n    b, a = iirnotch(notch_freq / (fs / 2), quality_factor)\n    return filtfilt(b, a, signal)\n\n\n### ------------------------------------------\n### Signal-Specific Processing Functions\n### ------------------------------------------\n\ndef process_ecg(ecg_signal, fs, cutoff_high=0.5, cutoff_low=40, notch_freq=50, quality_factor=30):\n    \"\"\"\n    Cleans ECG signal by applying high-pass, low-pass, and notch filters.\n    \"\"\"\n    high_passed = butter_filter(ecg_signal, cutoff_high, fs, order=4, filter_type='high')\n    low_passed = butter_filter(high_passed, cutoff_low, fs, order=4, filter_type='low')\n    filtered = notch_filter(low_passed, fs, notch_freq, quality_factor)\n    return filtered\n\ndef process_emg(emg_signal, fs, cutoff_high=20, cutoff_low=10):\n    \"\"\"\n    Processes EMG by high-pass filtering, rectifying, then low-pass filtering.\n    \"\"\"\n    high_passed = butter_filter(emg_signal, cutoff_high, fs, order=4, filter_type='high')\n    rectified = np.abs(high_passed)\n    return butter_filter(rectified, cutoff_low, fs, order=4, filter_type='low')\n\ndef process_respiration(resp_signal, fs, lowpass_cutoff=1.0):\n    \"\"\"\n    Low-pass filters respiration signal to retain only breathing patterns.\n    \"\"\"\n    return butter_filter(resp_signal, lowpass_cutoff, fs, order=4, filter_type='low')\n\n\n### ------------------------------------------\n### Audio Feature Extraction\n### ------------------------------------------\n\ndef extract_pitch_yin(audio, sr, fmin=50, fmax=400, frame_length=2048, hop_length=512, trough_threshold=0.1):\n    \"\"\"\n    Extracts pitch (F0) from audio using librosa's YIN algorithm.\n    Returns both pitch values and time vector.\n    \"\"\"\n    f0 = librosa.yin(audio, fmin=fmin, fmax=fmax, sr=sr,\n                     frame_length=frame_length, hop_length=hop_length,\n                     trough_threshold=trough_threshold)\n    times = librosa.times_like(f0, sr=sr, hop_length=hop_length)\n    return f0, times\n\ndef amp_envelope(audiofilename):\n    \"\"\"\n    Extracts amplitude envelope from audio by applying bandpass, rectification,\n    lowpass filtering, and scaling to 0–1.\n    \"\"\"\n    audio, sr = librosa.load(audiofilename, sr=None)\n    # Apply bandpass filter between 400–4000 Hz\n    data = butter_filter(audio, 400, sr, order=2, filter_type='high')\n    data = butter_filter(data, 4000, sr, order=2, filter_type='low')\n    # Rectify and smooth\n    data = np.abs(data)\n    data = butter_filter(data, 10, sr, order=2, filter_type='low')\n    # Normalize\n    return (data - np.min(data)) / (np.max(data) - np.min(data)), sr\n\n\n# Empirical Mode Decomposition\ndef my_get_next_imf(x, zoom=None, sd_thresh=0.1):\n    proto_imf = x.copy()\n    continue_sift = True\n    niters = 0\n\n    if zoom is None:\n        zoom = (0, x.shape[0])\n\n    while continue_sift:\n        niters += 1\n        upper_env = emd.sift.interp_envelope(proto_imf, mode='upper')\n        lower_env = emd.sift.interp_envelope(proto_imf, mode='lower')\n        avg_env = (upper_env+lower_env) / 2\n        stop, val = emd.sift.stop_imf_sd(proto_imf-avg_env, proto_imf, sd=sd_thresh)\n        proto_imf = proto_imf - avg_env\n        if stop:\n            continue_sift = False\n\n    return proto_imf\n\n\n\n### ---KEYWORDS--- ###\nparticipants = ['P1', 'P2']\naudio_keyword = 'denoised'\n\n\n\nInput folder: D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\raw_trials\nOutput folder: D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\n\n\n\n\n\nExtracting Amplitude from the Raw Audio.\n\nfor root, dirs, files in os.walk(input_folder):  # Loop through all files in the input directory\n    for file in files:                           # Loop through all files in the input directory\n        for P in participants:                   # Loop through P1 and P2\n            # Find the LSL csv file\n            if P in file and 'Mic' in file and file.endswith(\".csv\"):   # Check if the file contains the keyword and is a .csv file\n                print(\"Processing LSL file: \", os.path.basename(file))\n                LSL_filename = os.path.join(root, file)\n                LSL_data = pd.read_csv(LSL_filename)\n                LSL_Time = LSL_data['LSL_Time']\n                # Get the start and end time of the LSL\n                start_time = LSL_Time.iloc[0]\n                end_time = LSL_Time.iloc[-1]\n                duration = end_time - start_time  # Total duration of the LSL time\n\n            # Find the corresponding audio file\n            if P in file and audio_keyword in file and file.endswith(\".wav\"):\n                print(\"Processing audio file: \", os.path.basename(file))\n                audio_filename = os.path.join(root, file)\n                \n                # Extract amplitude envelope using the predefined function\n                print(\"Extracting amplitude envelope...\")\n                envelope_normalized, sr = amp_envelope(audio_filename)\n                print(\"The sample rate of the audio file is: \", sr)\n\n                # Generate time array\n                times = np.linspace(0, len(envelope_normalized) / sr, len(envelope_normalized))\n\n                # Save the amplitude envelope to data frame\n                envelope_df = pd.DataFrame({\n                    'Time': times,\n                    'Amplitude_Envelope': envelope_normalized\n                })\n\n                # Create the output CSV file\n                filename_base = os.path.basename(file)\n                parts = filename_base.split('_')\n                new_name = '_'.join(parts[:7]).replace('Mic', 'AudioEnvelope')\n                output_filename = os.path.join(output_folder, new_name + '.csv')\n                envelope_df.to_csv(output_filename, index=False)\n                print(\"Amplitude envelope data saved to: \", output_filename)\n\n                ## PLOTTING ##\n                audio_data, _ = librosa.load(audio_filename, sr=sr)\n                plt.figure(figsize=(12, 8))\n                plt.subplot(2, 1, 1)\n                plt.plot(np.linspace(0, len(audio_data) / sr, len(audio_data)), audio_data, label='Raw Audio', color='gray')\n                plt.title('Raw Audio Signal')\n\n                plt.subplot(2, 1, 2)\n                plt.plot(times, envelope_normalized, label='Amplitude Envelope', color='blue')\n                plt.title('Amplitude Envelope')\n                plt.xlabel('Time (s)')\n                plt.tight_layout()\n                plt.show()\n\n\n\n\nECG Filtering with\n\nsampling_rate = 1000 \n\nfor root, dirs, files in os.walk(input_folder):  # Loop through all files in the input directory\n   \n    for file in files:                                  # Loop through all files in the input directory\n        \n        for P in participants:                        # Loop through P1 and P2 \n           \n            # Find the LSL csv file \n            if P in file and 'PLUX' in file and file.endswith(\".csv\"):   # Check if the file contains the keyword and is a .wav file\n                print(\"Processing LSL file: \", os.path.basename(file))\n\n                LSL_filename = os.path.join(root, file)\n                LSL_data = pd.read_csv(LSL_filename)\n\n                LSL_Time = LSL_data['LSL_Time']\n\n                # ECG is in the third column of the LSL data\n                ECG_data = LSL_data.iloc[:, 2].values\n\n                print(\"the sample rate of the ECG file is: \", sampling_rate)\n\n\n                # Filter the ECG data with a high pass and low pass filter\n                ECG_high_pass = butter_filter(ECG_data, 0.75, sampling_rate, order=4, filter_type='high')\n                ECG_low_pass = butter_filter(ECG_high_pass, 3, sampling_rate, order=4, filter_type='low')\n\n\n    #           # ALTERNATIVE WAY TO FILTER THE ECG DATA\n                # Preprocess the ECG data using bandpas and notch filters\n                ECG_filtered = process_ecg(ECG_data, sampling_rate, 0.5, 20, 50, 30)  # 0.5 Hz high-pass, 40 Hz low-pass, 50 Hz notch, 30 Q-factor\n\n\n                # Apply EMD: extract and remove first IMF (high-freq noise)\n                ECG_imf1 = my_get_next_imf(ECG_filtered, sd_thresh=0.1)\n\n                ECG_cleaned = my_get_next_imf(ECG_filtered - ECG_imf1)\n\n                # from scipy.signal import savgol_filter\n                # window_length must be odd and &gt; polyorder\n                # ecg_smooth = savgol_filter(ECG_processed, window_length=251, polyorder=3)\n\n\n                # Plot both Raw and Filtered ECG signals in the same figure\n                plt.figure(figsize=(12, 6))\n                plt.plot(LSL_Time, ECG_data, label='Raw ECG', color='gray')\n                plt.plot(LSL_Time, ECG_cleaned, label='Filtered ECG', color='blue')\n                plt.title('ECG Signal')\n                plt.xlabel('Time (s)')\n                plt.ylabel('Amplitude')\n                plt.legend()\n                plt.show()\n\n\n\n                # # Plot the Raw and Filtered ECG signals in two subplots \n                # plt.figure(figsize=(12, 8))\n                # plt.subplot(2, 1, 1)\n                # plt.plot(LSL_Time, ECG_data, label='Raw ECG', color='gray')\n                # plt.title('Raw ECG Signal')\n                # plt.subplot(2, 1, 2)\n                # plt.plot(LSL_Time, ECG_low_pass, label='Filtered ECG', color='blue')\n                # plt.title('Filtered ECG Signal')\n                # plt.xlabel('Time (s)')\n                # plt.tight_layout()\n                # plt.show()\n\n                # Save the filtered ECG data to a CSV file \n                filename_base = os.path.basename(file)\n                parts = filename_base.split('_')\n                # Get the first 7 parts and join them back with underscores (i.e., until the trial number)\n                new_name = '_'.join(parts[:7])\n                # Replace \"PLUX\" with \"ECG\" in the base name\n                new_name = new_name.replace('PLUX', 'ECG')\n                # Save the filtered ECG data to a CSV file in the output folder\n                output_filename = os.path.join(output_folder, new_name + '.csv')\n                # Create a DataFrame with the filtered ECG data\n                ECG_df = pd.DataFrame({\n                    'Time': LSL_Time,\n                    'Filtered_ECG': ECG_low_pass\n                })\n                # Save the DataFrame to a CSV file\n                ECG_df.to_csv(output_filename, index=False)\n                print(\"Filtered ECG data saved to: \", output_filename)\n                \n\nprint(\"All files processed successfully. You can look in the directory: \", output_folder)\n# Close all plots\nplt.close('all')\n\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_NoVision_Movement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_NoVision_Movement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_NoVision_Movement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_NoVision_Movement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_NoVision_Movement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_NoVision_NoMovement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_NoVision_NoMovement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_NoVision_NoMovement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_NoVision_NoMovement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_NoVision_NoMovement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_Vision_Movement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_Vision_Movement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_Vision_Movement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_Vision_Movement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_Vision_Movement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_Vision_NoMovement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_Vision_NoMovement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_Vision_NoMovement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_Vision_NoMovement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P1_Vision_NoMovement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_NoVision_Movement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_NoVision_Movement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_NoVision_Movement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_NoVision_Movement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_NoVision_Movement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_NoVision_NoMovement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_NoVision_NoMovement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_NoVision_NoMovement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_NoVision_NoMovement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_NoVision_NoMovement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_Vision_Movement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_Vision_Movement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_Vision_Movement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_Vision_Movement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_Vision_Movement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_Vision_NoMovement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_Vision_NoMovement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_Vision_NoMovement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_Vision_NoMovement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging.csv\nthe sample rate of the ECG file is:  1000\nFiltered ECG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_ECG_P2_Vision_NoMovement_4.csv\nAll files processed successfully. You can look in the directory:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRespiration Cleaning\n\nsampling_rate = 1000 \n\nfor root, dirs, files in os.walk(input_folder):  # Loop through all files in the input directory\n   \n    for file in files:                                  # Loop through all files in the input directory\n        \n        for P in participants:                        # Loop through P1 and P2 \n           \n            # Find the LSL csv file \n            if P in file and 'PLUX' in file and file.endswith(\".csv\"):   # Check if the file contains the keyword and is a .wav file\n                print(\"Processing LSL file: \", os.path.basename(file))\n\n                LSL_filename = os.path.join(root, file)\n                LSL_data = pd.read_csv(LSL_filename)\n\n                LSL_Time = LSL_data['LSL_Time']\n\n                # Respiration is in the 6th column of the LSL data\n                Respiration_data = LSL_data.iloc[:, 5].values\n\n                # cleanign the respiration data with a low pass filter\n                Respiration_low_pass = butter_lowpass_filtfilt(Respiration_data, 1.0, 1000, order=4)\n\n                # Plot the Raw and Filtered Respiration signals the same plot \n                plt.figure(figsize=(12, 6))\n                plt.plot(LSL_Time, Respiration_data, label='Raw Respiration', color='gray')\n                plt.plot(LSL_Time, Respiration_low_pass, label='Filtered Respiration', color='blue')\n                plt.title('Respiration Signal')\n                plt.xlabel('Time (s)')\n                plt.ylabel('Amplitude')\n                plt.legend()\n                #plt.show()\n\n                \n\n                # Save the filtered Respiration data to a CSV file\n                filename_base = os.path.basename(file)\n                parts = filename_base.split('_')\n                # Get the first 7 parts and join them back with underscores (i.e., until the trial number)\n                new_name = '_'.join(parts[:7])\n                # Replace \"PLUX\" with \"Respiration\" in the base name\n                new_name = new_name.replace('PLUX', 'Respiration')\n                # Save the filtered Respiration data to a CSV file in the output folder\n                output_filename = os.path.join(output_folder, new_name + '.csv')\n                # Create a DataFrame with the filtered Respiration data\n                Respiration_df = pd.DataFrame({\n                    'Time': LSL_Time,\n                    'Filtered_Respiration': Respiration_low_pass\n                })\n                # Save the DataFrame to a CSV file\n                Respiration_df.to_csv(output_filename, index=False)\n                print(\"Filtered Respiration data saved to: \", output_filename)\nprint(\"All files processed successfully. You can look in the directory: \", output_folder)\n                \n\n\n\nEMG Filtering with High Pass, Rectifier and Low Pass\n\n# Define Butterworth filter function\ndef butter_filter(data, cutoff, fs, order=4, filter_type='low'):\n    nyquist = 0.5 * fs  # Nyquist frequency\n    normal_cutoff = cutoff / nyquist\n    b, a = butter(order, normal_cutoff, btype=filter_type, analog=False)\n    # Apply zero-phase filtering with padding to prevent edge effects\n    padded_data = np.pad(data, (1000, 1000), 'edge')\n    filtered_data = filtfilt(b, a, padded_data)\n    return filtered_data[1000:-1000]  # Remove padding\n\n# High-pass filter, rectify, and then low-pass filter EMG signals\ndef process_emg(emg_signal, fs, cutoff_high, cutoff_low):\n    # Apply high-pass filter\n    high_passed = butter_filter(emg_signal, cutoff_high, fs, order=4, filter_type='high')\n    # Rectify (full-wave rectification)\n    rectified = np.abs(high_passed)\n    # Apply low-pass filter\n    low_passed = butter_filter(rectified, cutoff_low, fs, order=4, filter_type='low')\n    return low_passed\n\n\nsampling_rate = 1000    \n\nfor root, dirs, files in os.walk(input_folder):  # Loop through all files in the input directory\n    for file in files:                                  # Loop through all files in the input directory\n        \n        for P in participants:                        # Loop through P1 and P2 \n           \n            # Find the LSL csv file \n            if P in file and 'PLUX' in file and file.endswith(\".csv\"):   # Check if the file contains the keyword and is a .wav file\n                print(\"Processing LSL file: \", os.path.basename(file))\n\n                LSL_filename = os.path.join(root, file)\n                LSL_data = pd.read_csv(LSL_filename)\n\n                LSL_Time = LSL_data['LSL_Time']\n\n                # EMG bicep is in the 4th column \n                EMG_bicep = LSL_data.iloc[:, 3].values\n\n                # EMG tricep is in the 5th column\n                EMG_tricep = LSL_data.iloc[:, 4].values\n            \n\n                # Filter the EMG data with high and low pass filters\n                EMG_bicep_processed = process_emg(EMG_bicep, sampling_rate, 30, 20)  # 30 Hz high-pass, 20 Hz low-pass\n                EMG_tricep_processed = process_emg(EMG_tricep, sampling_rate, 30, 20)  # 30 Hz high-pass, 20 Hz low-pass\n\n                \n                \n                # Plot the Raw and Filtered EMG signals in two subplots\n                plt.figure(figsize=(12, 8))\n                plt.subplot(2, 1, 1)\n                plt.plot(LSL_Time, EMG_bicep, label='Raw EMG Bicep', color='gray')\n                plt.plot(LSL_Time, EMG_bicep_processed, label='Filtered EMG Bicep', color='blue')\n                plt.ylim(-0.8, 0.8)\n                plt.title('EMG Bicep Signal')\n                plt.subplot(2, 1, 2)\n                plt.plot(LSL_Time, EMG_tricep, label='Raw EMG Tricep', color='gray')\n                plt.plot(LSL_Time, EMG_tricep_processed, label='Filtered EMG Tricep', color='blue')\n                plt.title('EMG Tricep Signal')\n                plt.xlabel('Time (s)')\n                #set the y-axis limits to the same range for both subplots\n                plt.ylim(-0.8, 0.8)\n                plt.tight_layout()\n                plt.show()\n\n                # Save the filtered EMG data to a CSV file\n                filename_base = os.path.basename(file)\n                parts = filename_base.split('_')\n                # Get the first 7 parts and join them back with underscores (i.e., until the trial number)\n                new_name = '_'.join(parts[:7])\n                # Replace \"PLUX\" with \"EMG\" in the base name\n                new_name = new_name.replace('PLUX', 'EMG')\n                # Save the filtered EMG data to a CSV file in the output folder\n                output_filename = os.path.join(output_folder, new_name + '.csv')\n                # Create a DataFrame with the filtered EMG data\n                EMG_df = pd.DataFrame({\n                    'Time': LSL_Time,\n                    'Filtered_EMG_Bicep': EMG_bicep_processed,\n                    'Filtered_EMG_Tricep': EMG_tricep_processed\n                })\n                # Save the DataFrame to a CSV file\n                EMG_df.to_csv(output_filename, index=False)\n                print(\"Filtered EMG data saved to: \", output_filename)\n\nprint(\"All files processed successfully. You can look in the directory: \", output_folder)\n\n                \n\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_NoVision_Movement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_NoVision_Movement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_NoVision_Movement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_NoVision_Movement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_NoVision_Movement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_NoVision_NoMovement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_NoVision_NoMovement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_NoVision_NoMovement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_NoVision_NoMovement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_NoVision_NoMovement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_Vision_Movement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_Vision_Movement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_Vision_Movement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_Vision_Movement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_Vision_Movement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_Vision_NoMovement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_Vision_NoMovement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_Vision_NoMovement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_Vision_NoMovement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P1_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P1_Vision_NoMovement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_NoVision_Movement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_NoVision_Movement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_NoVision_Movement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_NoVision_Movement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_NoVision_Movement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_NoVision_NoMovement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_NoVision_NoMovement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_NoVision_NoMovement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_NoVision_NoMovement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_NoVision_NoMovement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_Vision_Movement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_Vision_Movement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_Vision_Movement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_Vision_Movement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_Vision_Movement_4.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_Vision_NoMovement_0.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_Vision_NoMovement_1.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_Vision_NoMovement_2.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_Vision_NoMovement_3.csv\nProcessing LSL file:  T1_experiment_PLUX_P2_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging.csv\nFiltered EMG data saved to:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\\T1_experiment_EMG_P2_Vision_NoMovement_4.csv\nAll files processed successfully. You can look in the directory:  D:\\Mobile-Multimodal-Lab\\4_PROCESSED\\filtered_trials\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# PITCH ENVELOPE EXTRACTION FROM AUDIO FLES (OLD, WE'RE WORKIGN WITH AMPLITUDE INSTEAD)\n\nfor root, dirs, files in os.walk(input_folder):  # Loop through all files in the input directory\n   \n    for file in files:                                  # Loop through all files in the input directory\n        \n        for P in participants:                        # Loop through P1 and P2 \n           \n            # Find the LSL csv file \n            if P in file and 'Mic' in file and file.endswith(\".csv\"):   # Check if the file contains the keyword and is a .wav file\n                print(\"Processing LSL file: \", os.path.basename(file))\n\n                LSL_filename = os.path.join(root, file)\n                \n                LSL_data = pd.read_csv(LSL_filename)\n                \n                LSL_Time = LSL_data['LSL_Time']\n\n                # Get the start and end time of the LSL \n                start_time = LSL_Time.iloc[0]\n                end_time = LSL_Time.iloc[-1]\n                duration = end_time - start_time  # Total duration of the LSL time\n                \n            # Find the corresponding audio file\n            if P in file and audio_keyword in file and file.endswith(\".wav\"):\n                print(\"Processing audio file: \", os.path.basename(file))\n\n                audio_filename = os.path.join(root, file)\n                audio_data, sr = librosa.load(audio_filename, sr=None)\n                print(\"the sample rate of the audio file is: \", sr)\n\n                # Extract pitch using the YIN algorithm\n                print(\"Extracting pitch using YIN algorithm...\")\n                f0, times = extract_pitch_yin(audio_data, sr)\n\n                # Smoothing the pitch contour with a moving average filter\n                print(\"Smoothing the pitch contour...\")\n                f0_smooth = uniform_filter1d(f0, size=5)\n\n\n                # Save the Pitch and Smoothed Pitch to data frame\n                pitch_df = pd.DataFrame({\n                    'Time': times ,\n                    'Pitch': f0,\n                    'Smooth_Pitch': f0_smooth\n                })\n\n                # Create the output CSV file\n                filename_base = os.path.basename(file)  # Just the filename, not full path\n                parts = filename_base.split('_')  # Split by underscore\n                # Get the first 7 parts and join them back with underscores (i.e., until the trial number)\n                new_name = '_'.join(parts[:7])\n                # Replace \"Mic\" with \"AudioPitch\" in the base name\n                new_name = new_name.replace('Mic', 'AudioPitch')\n\n                # save the ptich data to a CSV file in teh output folder \n                output_filename = os.path.join(output_folder, new_name + '.csv')\n                pitch_df.to_csv(output_filename, index=False)\n                print(\"Pitch data saved to: \", output_filename)\n\n\n                ## PLOTTING ##\n                # Create a plot 3 subplots: the raw audio, the pitch contour, and the smoothed pitch contour\n                plt.figure(figsize=(12, 8))\n                plt.subplot(3, 1, 1)\n                plt.plot(LSL_Time, audio_data, label='Raw Audio', color='gray')\n                plt.title('Raw Audio Signal')\n\n                plt.subplot(3, 1, 2)\n                plt.plot(times, f0, label='Pitch Contour', color='blue')\n                plt.title('Pitch Contour')\n                plt.subplot(3, 1, 3)\n\n                plt.plot(times, f0_smooth, label='Smoothed Pitch Contour', color='red')\n                plt.title('Smoothed Pitch Contour')\n                plt.xlabel('Time (s)')\n                plt.tight_layout()\n                plt.show()\n\n        \n                0/0\n\n\n\n\n# Added Wim: Creating one big file with all the data\n\n\nimport os\nimport glob as glob\nimport pandas as pd\nimport numpy as np\nfrom scipy import interpolate\n\n# Merging all the CSV files per modality\nfiltered_folder = '../filtered_trials'  # Folder where the filtered CSV files are saved\noutput_folder = '../merged_filteredtimeseries/'  # Folder where the merged CSV file will be saved\noutput_filename = os.path.join(output_folder, 'merged_filtered_data.csv')\n\n# check for the LSL times for the frames\nLSL_trial_folder = '../raw_trials'  # Folder where the raw trials are saved\n\n# Ensure output directory exists\nos.makedirs(output_folder, exist_ok=True)\n\ndef align_time_to_common_start(df, time_col='Time'):\n    \"\"\"\n    Align time column to start from 0\n    \"\"\"\n    if df.empty or time_col not in df.columns:\n        return df\n    \n    df = df.copy()\n    df[time_col] = df[time_col] - df[time_col].min()\n    return df\n\ndef create_common_time_grid(dfs, time_col='Time', target_freq=1000):\n    \"\"\"\n    Create a common time grid that encompasses all dataframes\n    \"\"\"\n    if not dfs or all(df.empty for df in dfs):\n        return np.array([])\n    \n    # Find the overall time range across all dataframes\n    min_times = []\n    max_times = []\n    \n    for df in dfs:\n        if not df.empty and time_col in df.columns:\n            clean_times = df[time_col].dropna()\n            if not clean_times.empty:\n                min_times.append(clean_times.min())\n                max_times.append(clean_times.max())\n    \n    if not min_times:\n        return np.array([])\n    \n    overall_min = min(min_times)\n    overall_max = max(max_times)\n    \n    # Create common time grid\n    time_step = 1.0 / target_freq\n    common_time = np.arange(overall_min, overall_max + time_step, time_step)\n    \n    return common_time\n\ndef resample_to_common_grid(df, common_time, time_col='Time'):\n    \"\"\"\n    Resample dataframe to the common time grid\n    \"\"\"\n    if df.empty or time_col not in df.columns or len(common_time) == 0:\n        # Return empty dataframe with common time grid\n        result = pd.DataFrame({time_col: common_time})\n        return result\n    \n    # Remove rows with NaN times\n    df_clean = df.dropna(subset=[time_col]).copy()\n    if df_clean.empty:\n        result = pd.DataFrame({time_col: common_time})\n        return result\n    \n    # Sort by time\n    df_clean = df_clean.sort_values(time_col).reset_index(drop=True)\n    \n    # Create result dataframe with common time grid\n    result = pd.DataFrame({time_col: common_time})\n    \n    # Interpolate each numeric column\n    for col in df_clean.columns:\n        if col != time_col and pd.api.types.is_numeric_dtype(df_clean[col]):\n            # Remove NaN values for interpolation\n            valid_mask = ~df_clean[col].isna()\n            if valid_mask.sum() &gt; 1:  # Need at least 2 points to interpolate\n                try:\n                    f = interpolate.interp1d(\n                        df_clean.loc[valid_mask, time_col], \n                        df_clean.loc[valid_mask, col], \n                        kind='linear', \n                        bounds_error=False, \n                        fill_value='extrapolate'\n                    )\n                    result[col] = f(common_time)\n                except Exception as e:\n                    print(f\"Warning: Failed to interpolate column {col}: {e}\")\n                    result[col] = np.nan\n            else:\n                result[col] = np.nan\n        elif col != time_col:\n            # For non-numeric columns, forward fill\n            result[col] = df_clean[col].iloc[0] if len(df_clean) &gt; 0 else np.nan\n    \n    return result\n\ndef merge_participant_data(df1, df2, time_col='Time', suffixes=('_P1', '_P2')):\n    \"\"\"\n    Merge data from two participants with proper time alignment\n    \"\"\"\n    if df1.empty and df2.empty:\n        return pd.DataFrame()\n    elif df1.empty:\n        return df2.add_suffix(suffixes[1]) if not df2.empty else pd.DataFrame()\n    elif df2.empty:\n        return df1.add_suffix(suffixes[0]) if not df1.empty else pd.DataFrame()\n    \n    # Align both dataframes to start from time 0\n    df1_aligned = align_time_to_common_start(df1, time_col)\n    df2_aligned = align_time_to_common_start(df2, time_col)\n    \n    # Merge on time\n    merged = pd.merge(df1_aligned, df2_aligned, on=time_col, how='outer', suffixes=suffixes)\n    \n    # Sort by time\n    merged = merged.sort_values(time_col).reset_index(drop=True)\n    \n    return merged\n\n# Initialize the final merged dataframe\nmerged_data_full = pd.DataFrame()\n\nparticipants = ['P1', 'P2']\ntrials = ['_0', '_1', '_2', '_3', '_4'] \nconditions = ['_NoVision_Movement', '_Vision_Movement', '_NoVision_NoMovement', '_Vision_NoMovement']\n\nfor condition in conditions:\n    for trial in trials:\n        print(f\"\\nProcessing {condition} {trial}\")\n        \n        # Find all CSV files for the current condition and trial\n        csvfilesenvelope = glob.glob(os.path.join(filtered_folder, f\"*AudioEnvelope*{condition}*{trial}.csv\"))\n        csvfilesheartrate = glob.glob(os.path.join(filtered_folder, f\"*heart_rate*{condition}*{trial}.csv\"))\n        csvfilesrespiration = glob.glob(os.path.join(filtered_folder, f\"*Respiration*{condition}*{trial}.csv\"))\n        csvfilesemg = glob.glob(os.path.join(filtered_folder, f\"*EMG*{condition}*{trial}.csv\"))\n        csvfilesmt = glob.glob(os.path.join(filtered_folder, f\"*3DMotionTracking*{condition}*{trial}.csv\"))\n        \n        print(f\"Found files - Envelope: {len(csvfilesenvelope)}, HR: {len(csvfilesheartrate)}, \"\n              f\"Resp: {len(csvfilesrespiration)}, EMG: {len(csvfilesemg)}, MT: {len(csvfilesmt)}\")\n        \n \n        # Initialize modality dataframes - store them before resampling\n        raw_modality_dfs = []\n        \n        # Process Audio Envelope\n        if len(csvfilesenvelope) == 2:\n            try:\n                envelope_p1 = pd.read_csv(csvfilesenvelope[0])\n                envelope_p2 = pd.read_csv(csvfilesenvelope[1])\n                envelope_merged = merge_participant_data(envelope_p1, envelope_p2)\n                \n                # Ensure proper column names\n                expected_cols = ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n                envelope_merged = envelope_merged.reindex(columns=expected_cols, fill_value=np.nan)\n                \n                raw_modality_dfs.append(envelope_merged)\n                print(f\"✓ Prepared envelope data: {len(envelope_merged)} samples\")\n            except Exception as e:\n                print(f\"✗ Failed to process envelope data: {e}\")\n        \n        # Process Heart Rate\n        if len(csvfilesheartrate) == 2:\n            try:\n                hr_p1 = pd.read_csv(csvfilesheartrate[0])\n                hr_p2 = pd.read_csv(csvfilesheartrate[1])\n                hr_merged = merge_participant_data(hr_p1, hr_p2)\n                \n                expected_cols = ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n                hr_merged = hr_merged.reindex(columns=expected_cols, fill_value=np.nan)\n                \n                raw_modality_dfs.append(hr_merged)\n                print(f\"✓ Prepared heart rate data: {len(hr_merged)} samples\")\n            except Exception as e:\n                print(f\"✗ Failed to process heart rate data: {e}\")\n        \n        # Process Respiration\n        if len(csvfilesrespiration) == 2:\n            try:\n                resp_p1 = pd.read_csv(csvfilesrespiration[0])\n                resp_p2 = pd.read_csv(csvfilesrespiration[1])\n                resp_merged = merge_participant_data(resp_p1, resp_p2)\n                \n                expected_cols = ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n                resp_merged = resp_merged.reindex(columns=expected_cols, fill_value=np.nan)\n                \n                raw_modality_dfs.append(resp_merged)\n                print(f\"✓ Prepared respiration data: {len(resp_merged)} samples\")\n            except Exception as e:\n                print(f\"✗ Failed to process respiration data: {e}\")\n        \n        # Process EMG\n        if len(csvfilesemg) == 2:\n            try:\n                emg_p1 = pd.read_csv(csvfilesemg[0])\n                emg_p2 = pd.read_csv(csvfilesemg[1])\n                emg_merged = merge_participant_data(emg_p1, emg_p2)\n                \n                expected_cols = ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1',\n                               'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n                emg_merged = emg_merged.reindex(columns=expected_cols, fill_value=np.nan)\n                \n                raw_modality_dfs.append(emg_merged)\n                print(f\"✓ Prepared EMG data: {len(emg_merged)} samples\")\n            except Exception as e:\n                print(f\"✗ Failed to process EMG data: {e}\")\n        \n\n        # Process Motion Tracking (with LSL alignment)\n        if len(csvfilesmt) == 2:\n            try:\n                mt_p1 = pd.read_csv(csvfilesmt[0])\n                mt_p2 = pd.read_csv(csvfilesmt[1])\n                \n                print(f\"Motion tracking P1 shape: {mt_p1.shape}\")\n                print(f\"Motion tracking P2 shape: {mt_p2.shape}\")\n                \n                # Remove leading underscore from condition and trial for matching\n                condition_clean = condition.lstrip('_')\n                trial_clean = trial.lstrip('_')\n                \n                csvfilesLSL1 = glob.glob(os.path.join(LSL_trial_folder, f\"*Video*P1*{condition_clean}*{trial_clean}*.csv\"))\n                csvfilesLSL2 = glob.glob(os.path.join(LSL_trial_folder, f\"*Video*P2*{condition_clean}*{trial_clean}*.csv\"))\n\n                if len(csvfilesLSL1) &gt;= 1 and len(csvfilesLSL2) &gt;= 1:\n                    print(f\"✓ Found LSL files - processing with time alignment...\")\n                    \n                    lsl_p1 = pd.read_csv(csvfilesLSL1[0])\n                    lsl_p2 = pd.read_csv(csvfilesLSL2[0])\n                    \n                    print(f\"LSL P1 shape: {lsl_p1.shape}, columns: {list(lsl_p1.columns)}\")\n                    print(f\"LSL P2 shape: {lsl_p2.shape}, columns: {list(lsl_p2.columns)}\")\n                    \n                    # Fix LSL column naming and centering\n                    # Assume first column is time, second is frame\n                    lsl_p1_cols = list(lsl_p1.columns)\n                    lsl_p2_cols = list(lsl_p2.columns)\n                    \n                    # Rename columns properly\n                    lsl_p1.columns = ['LSL_Time', 'Frame'] + lsl_p1_cols[2:]\n                    lsl_p2.columns = ['LSL_Time', 'Frame'] + lsl_p2_cols[2:]\n                    \n                    # Center time to start from 0\n                    lsl_p1['LSL_Time'] = lsl_p1['LSL_Time'] - lsl_p1['LSL_Time'].min()\n                    lsl_p2['LSL_Time'] = lsl_p2['LSL_Time'] - lsl_p2['LSL_Time'].min()\n                    # also make sure the frame indices start from 1\n                    lsl_p1['Frame'] = lsl_p1['Frame'] - lsl_p1['Frame'].min()\n                    lsl_p2['Frame'] = lsl_p2['Frame'] - lsl_p2['Frame'].min()\n                    \n                    print(f\"LSL time ranges - P1: {lsl_p1['LSL_Time'].min():.3f} to {lsl_p1['LSL_Time'].max():.3f}\")\n                    print(f\"LSL time ranges - P2: {lsl_p2['LSL_Time'].min():.3f} to {lsl_p2['LSL_Time'].max():.3f}\")\n                    print(f\"LSL frame ranges - P1: {lsl_p1['Frame'].min()} to {lsl_p1['Frame'].max()}\")\n                    print(f\"LSL frame ranges - P2: {lsl_p2['Frame'].min()} to {lsl_p2['Frame'].max()}\")\n                    \n                    # Create unique frame-time mappings (handle duplicates)\n                    lsl_p1_unique = lsl_p1.groupby('Frame')['LSL_Time'].first().reset_index()\n                    lsl_p2_unique = lsl_p2.groupby('Frame')['LSL_Time'].first().reset_index()\n                    \n                    print(f\"Unique LSL mappings - P1: {len(lsl_p1_unique)}, P2: {len(lsl_p2_unique)}\")\n                    \n                    # Add sequential frame index to motion tracking data (starting from 0)\n                    mt_p1['Frame'] = range(len(mt_p1))\n                    mt_p2['Frame'] = range(len(mt_p2))\n                    \n                    print(f\"Motion frame ranges - P1: 0 to {len(mt_p1)-1}, P2: 0 to {len(mt_p2)-1}\")\n                    \n                    # Merge motion tracking with LSL times\n                    mt_p1_timed = pd.merge(mt_p1, lsl_p1_unique, on='Frame', how='left')\n                    mt_p2_timed = pd.merge(mt_p2, lsl_p2_unique, on='Frame', how='left')\n                    \n                    print(f\"After LSL merge - P1: {mt_p1_timed.shape}, P2: {mt_p2_timed.shape}\")\n                    \n                    # Check how many frames got valid timestamps\n                    p1_valid_times = mt_p1_timed['LSL_Time'].notna().sum()\n                    p2_valid_times = mt_p2_timed['LSL_Time'].notna().sum()\n                    \n                    print(f\"Valid timestamps after merge - P1: {p1_valid_times}/{len(mt_p1_timed)}, P2: {p2_valid_times}/{len(mt_p2_timed)}\")\n                    \n                    if p1_valid_times == 0 or p2_valid_times == 0:\n                        print(\"⚠️  No valid timestamps found - frame indices might not match LSL data\")\n                        print(\"Falling back to synthetic time...\")\n                        \n                        # Use synthetic time based on assumed frame rate\n                        frame_rate = 30  # Adjust as needed\n                        mt_p1_timed['LSL_Time'] = mt_p1_timed['Frame'] / frame_rate\n                        mt_p2_timed['LSL_Time'] = mt_p2_timed['Frame'] / frame_rate\n                        \n                        print(f\"Using synthetic time at {frame_rate} FPS\")\n                    \n                    # Rename LSL_Time to Time\n                    mt_p1_timed = mt_p1_timed.rename(columns={'LSL_Time': 'Time'})\n                    mt_p2_timed = mt_p2_timed.rename(columns={'LSL_Time': 'Time'})\n                    \n                    # Select key motion tracking positions\n                    key_positions = [\n                        \"right_index_x\", \"right_index_y\", \"right_index_z\"\n                    ]\n                    \n                    available_positions_p1 = [col for col in key_positions if col in mt_p1_timed.columns]\n                    available_positions_p2 = [col for col in key_positions if col in mt_p2_timed.columns]\n                    \n                    print(f\"Available motion positions - P1: {available_positions_p1}\")\n                    print(f\"Available motion positions - P2: {available_positions_p2}\")\n                    \n                    # Filter columns\n                    mt_p1_filtered = mt_p1_timed[['Time'] + available_positions_p1].copy()\n                    mt_p2_filtered = mt_p2_timed[['Time'] + available_positions_p2].copy()\n                    \n                    # Remove rows where Time is NaN (if any still remain)\n                    mt_p1_filtered = mt_p1_filtered.dropna(subset=['Time'])\n                    mt_p2_filtered = mt_p2_filtered.dropna(subset=['Time'])\n                    \n                    print(f\"After time filtering - P1: {len(mt_p1_filtered)}, P2: {len(mt_p2_filtered)}\")\n                    print(f\"Time ranges after filtering - P1: {mt_p1_filtered['Time'].min():.3f} to {mt_p1_filtered['Time'].max():.3f}\")\n                    print(f\"Time ranges after filtering - P2: {mt_p2_filtered['Time'].min():.3f} to {mt_p2_filtered['Time'].max():.3f}\")\n                    \n                    # Merge participants\n                    mt_merged = merge_participant_data(mt_p1_filtered, mt_p2_filtered)\n                    \n                    print(f\"After participant merge: {mt_merged.shape}\")\n                    \n                    if not mt_merged.empty and 'Time' in mt_merged.columns:\n                        # Verify we have valid time data\n                        valid_times = mt_merged['Time'].notna().sum()\n                        print(f\"Valid times in merged data: {valid_times}/{len(mt_merged)}\")\n                        \n                        if valid_times &gt; 0:\n                            raw_modality_dfs.append(mt_merged)\n                            print(f\"✓ Prepared motion tracking data and found LSL: {len(mt_merged)} samples\")\n                            motion_cols = [col for col in mt_merged.columns if col != 'Time']\n                            print(f\"  Motion columns: {motion_cols}\")\n                        else:\n                            print(f\"✗ Motion tracking has no valid timestamps\")\n                    else:\n                        print(f\"✗ Motion tracking merge resulted in empty dataframe or no Time column\")\n                \n                else:\n                    print(f\"✗ LSL files not found for motion tracking alignment\")\n                    print(f\"  Trying without LSL alignment using synthetic time...\")\n                    \n                    # Use synthetic time based on frame rate\n                    frame_rate = 30  # Adjust based on your video frame rate\n                    mt_p1['Time'] = np.arange(len(mt_p1)) / frame_rate\n                    mt_p2['Time'] = np.arange(len(mt_p2)) / frame_rate\n                    \n                    # Select key positions\n                    key_positions = [\n                        \"right_wrist_x\", \"right_wrist_y\", \"right_wrist_z\",\n                        \"left_wrist_x\", \"left_wrist_y\", \"left_wrist_z\",\n                        \"right_index_x\", \"right_index_y\", \"right_index_z\",\n                        \"left_index_x\", \"left_index_y\", \"left_index_z\"\n                    ]\n                    \n                    available_positions_p1 = [col for col in key_positions if col in mt_p1.columns]\n                    available_positions_p2 = [col for col in key_positions if col in mt_p2.columns]\n                    \n                    print(f\"Using synthetic time - Available positions P1: {available_positions_p1}\")\n                    print(f\"Using synthetic time - Available positions P2: {available_positions_p2}\")\n                    \n                    mt_p1_filtered = mt_p1[['Time'] + available_positions_p1]\n                    mt_p2_filtered = mt_p2[['Time'] + available_positions_p2]\n                    \n                    mt_merged = merge_participant_data(mt_p1_filtered, mt_p2_filtered)\n                    \n                    if not mt_merged.empty:\n                        raw_modality_dfs.append(mt_merged)\n                        print(f\"✓ Prepared motion tracking data (synthetic time): {len(mt_merged)} samples\")\n                    \n            except Exception as e:\n                print(f\"✗ Failed to process motion tracking data: {e}\")\n                import traceback\n                traceback.print_exc()\n\n        else:\n            print(f\"✗ Expected 2 motion tracking files, found {len(csvfilesmt)}\")\n        \n\n        # Create common time grid and resample all modalities to it\n        if raw_modality_dfs:\n            print(f\"\\n🔍 FINAL MERGE DEBUG START\")\n            print(f\"Number of modalities to merge: {len(raw_modality_dfs)}\")\n            \n            # Show what each modality contains\n            for i, df in enumerate(raw_modality_dfs):\n                print(f\"  Modality {i+1}: {df.shape} - Columns: {list(df.columns)}\")\n                if not df.empty:\n                    print(f\"    Time range: {df['Time'].min():.3f} to {df['Time'].max():.3f}\")\n                    print(f\"    Sample times: {df['Time'].head(3).tolist()}\")\n                else:\n                    print(f\"    ⚠️  Empty dataframe!\")\n            \n            print(f\"\\nCreating common time grid for {len(raw_modality_dfs)} modalities...\")\n            \n            # Create common time grid that encompasses all modalities\n            common_time = create_common_time_grid(raw_modality_dfs, target_freq=1000)\n            \n            if len(common_time) == 0:\n                print(\"✗ Failed to create common time grid\")\n                # Try to understand why\n                print(\"Debugging common time grid creation:\")\n                for i, df in enumerate(raw_modality_dfs):\n                    if not df.empty and 'Time' in df.columns:\n                        clean_times = df['Time'].dropna()\n                        if not clean_times.empty:\n                            print(f\"  Modality {i+1}: min={clean_times.min():.3f}, max={clean_times.max():.3f}\")\n                        else:\n                            print(f\"  Modality {i+1}: No valid times after dropna\")\n                    else:\n                        print(f\"  Modality {i+1}: Empty or no Time column\")\n                continue\n            \n            print(f\"✓ Common time grid created: {len(common_time)} samples from {common_time[0]:.3f}s to {common_time[-1]:.3f}s\")\n            \n            # Resample all modalities to the common time grid\n            resampled_dfs = []\n            for i, df in enumerate(raw_modality_dfs):\n                print(f\"  Resampling modality {i+1}...\")\n                print(f\"    Input: {df.shape} - Columns: {list(df.columns)}\")\n                \n                resampled_df = resample_to_common_grid(df, common_time)\n                resampled_dfs.append(resampled_df)\n                \n                print(f\"    Output: {resampled_df.shape} - Columns: {list(resampled_df.columns)}\")\n                \n                # Check for motion tracking columns specifically\n                motion_cols = [col for col in resampled_df.columns if any(x in col.lower() for x in ['wrist', 'index', 'shoulder'])]\n                if motion_cols:\n                    print(f\"    🎯 Motion columns found: {motion_cols}\")\n                \n                # Check if data is all NaN\n                non_time_cols = [col for col in resampled_df.columns if col != 'Time']\n                if non_time_cols:\n                    nan_counts = resampled_df[non_time_cols].isna().sum()\n                    total_rows = len(resampled_df)\n                    print(f\"    Data quality: {total_rows - nan_counts.max()}/{total_rows} non-NaN rows\")\n            \n            print(f\"\\n📊 Starting final merge of {len(resampled_dfs)} resampled dataframes...\")\n            \n            # Now merge all resampled modalities (they all have the same time grid)\n            trial_data = resampled_dfs[0].copy()\n            print(f\"  Base dataframe: {trial_data.shape} - Columns: {list(trial_data.columns)}\")\n            \n            for i, df in enumerate(resampled_dfs[1:], 1):\n                print(f\"  Merging dataframe {i+1}: {df.shape}\")\n                print(f\"    Columns to add: {[col for col in df.columns if col != 'Time']}\")\n                \n                # Since they all have the same time grid, we can merge on Time\n                before_shape = trial_data.shape\n                trial_data = pd.merge(trial_data, df, on='Time', how='left')\n                after_shape = trial_data.shape\n                \n                print(f\"    After merge: {before_shape} -&gt; {after_shape}\")\n                \n                # Check for motion tracking columns in the merged result\n                motion_cols = [col for col in trial_data.columns if any(x in col.lower() for x in ['wrist', 'index', 'shoulder'])]\n                if motion_cols:\n                    print(f\"    🎯 Motion columns in merged data: {motion_cols}\")\n            \n            print(f\"🎉 Final merged data: {len(trial_data)} samples, {len(trial_data.columns)} columns\")\n            print(f\"All columns: {list(trial_data.columns)}\")\n            \n            # Specifically check for motion tracking columns\n            motion_cols = [col for col in trial_data.columns if any(x in col.lower() for x in ['wrist', 'index', 'shoulder', 'right_', 'left_'])]\n            if motion_cols:\n                print(f\"🎯 MOTION TRACKING COLUMNS FOUND: {motion_cols}\")\n                \n                # Check if they have data\n                for col in motion_cols:\n                    non_nan_count = trial_data[col].notna().sum()\n                    print(f\"    {col}: {non_nan_count}/{len(trial_data)} non-NaN values\")\n            else:\n                print(f\"❌ NO MOTION TRACKING COLUMNS FOUND IN FINAL DATA!\")\n                print(f\"   This suggests the motion data was lost during resampling or merging\")\n            \n            # Add condition information\n            conditionvision = \"Vision\" if \"_Vision\" in condition else \"NoVision_Movement\"\n            conditionmovement = \"Movement\" if \"_Movement\" in condition else \"NoMovement\"\n\n            trial_data['ConditionVision'] = conditionvision\n            trial_data['ConditionMovement'] = conditionmovement\n            # remove _ from trial\n            trial = trial.replace('_', '')\n            trial_data['Trial'] = trial\n            \n            # Save the dataset to a CSV file\n            output_trial_filename = os.path.join(output_folder, f\"{condition.replace('_', '')}_Trial{trial}.csv\")\n            trial_data.to_csv(output_trial_filename, index=False)\n            print(f\"✓ Trial data saved to: {output_trial_filename}\")\n            print(f\"  Shape: {trial_data.shape}\")\n            \n            # Show sample of time alignment\n            print(f\"  Time alignment check - First 5 timestamps: {trial_data['Time'].head().tolist()}\")\n            print(f\"  Time step consistency: {np.diff(trial_data['Time'].head(10)).round(6).tolist()}\")\n            \n            print(f\"🔍 FINAL MERGE DEBUG END\\n\")\n            \n        else:\n            print(\"✗ No modality data found for this trial\")\n\nprint(\"Processing complete!\")\n            \n\n\nProcessing _NoVision_Movement _0\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195405 samples\n✓ Prepared heart rate data: 24429 samples\n✓ Prepared respiration data: 24429 samples\n✓ Prepared EMG data: 24429 samples\nMotion tracking P1 shape: (662, 99)\nMotion tracking P2 shape: (734, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5937, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5885, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.208\nLSL time ranges - P2: 0.000 to 12.211\nLSL frame ranges - P1: 0.0 to 661.0\nLSL frame ranges - P2: 0.0 to 733.0\nUnique LSL mappings - P1: 662, P2: 734\nMotion frame ranges - P1: 0 to 661, P2: 0 to 733\nAfter LSL merge - P1: (662, 101), P2: (734, 101)\nValid timestamps after merge - P1: 662/662, P2: 734/734\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 662, P2: 734\nTime ranges after filtering - P1: 0.000 to 12.198\nTime ranges after filtering - P2: 0.000 to 12.202\nAfter participant merge: (1395, 7)\nValid times in merged data: 1395/1395\n✓ Prepared motion tracking data and found LSL: 1395 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195405, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.213\n    Sample times: [0.0, 6.25003198501566e-05, 0.0001250006397003]\n  Modality 2: (24429, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.211\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24429, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.211\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24429, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.211\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1395, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.202\n    Sample times: [0.0, 0.004150417516029847, 0.012339879370301787]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12214 samples from 0.000s to 12.213s\n  Resampling modality 1...\n    Input: (195405, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12214, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12214/12214 non-NaN rows\n  Resampling modality 2...\n    Input: (24429, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12214, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12214/12214 non-NaN rows\n  Resampling modality 3...\n    Input: (24429, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12214, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12214/12214 non-NaN rows\n  Resampling modality 4...\n    Input: (24429, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12214, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12214/12214 non-NaN rows\n  Resampling modality 5...\n    Input: (1395, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12214, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12214/12214 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12214, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12214, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12214, 3) -&gt; (12214, 5)\n  Merging dataframe 3: (12214, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12214, 5) -&gt; (12214, 7)\n  Merging dataframe 4: (12214, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12214, 7) -&gt; (12214, 11)\n  Merging dataframe 5: (12214, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12214, 11) -&gt; (12214, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12214 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12214/12214 non-NaN values\n    right_index_y_P1: 12214/12214 non-NaN values\n    right_index_z_P1: 12214/12214 non-NaN values\n    right_index_x_P2: 12214/12214 non-NaN values\n    right_index_y_P2: 12214/12214 non-NaN values\n    right_index_z_P2: 12214/12214 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/NoVisionMovement_Trial0.csv\n  Shape: (12214, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _NoVision_Movement _1\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195330 samples\n✓ Prepared heart rate data: 24419 samples\n✓ Prepared respiration data: 24419 samples\n✓ Prepared EMG data: 24419 samples\nMotion tracking P1 shape: (688, 99)\nMotion tracking P2 shape: (729, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5935, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.204\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 687.0\nLSL frame ranges - P2: 0.0 to 728.0\nUnique LSL mappings - P1: 688, P2: 729\nMotion frame ranges - P1: 0 to 687, P2: 0 to 728\nAfter LSL merge - P1: (688, 101), P2: (729, 101)\nValid timestamps after merge - P1: 688/688, P2: 729/729\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 688, P2: 729\nTime ranges after filtering - P1: 0.000 to 12.190\nTime ranges after filtering - P2: 0.000 to 12.196\nAfter participant merge: (1416, 7)\nValid times in merged data: 1416/1416\n✓ Prepared motion tracking data and found LSL: 1416 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195330, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.208\n    Sample times: [0.0, 6.250031997296868e-05, 0.0001250006399459]\n  Modality 2: (24419, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 3: (24419, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 4: (24419, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 5: (1416, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.196\n    Sample times: [0.0, 0.002075208758469671, 0.010283232807523746]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12210 samples from 0.000s to 12.209s\n  Resampling modality 1...\n    Input: (195330, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12210, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 2...\n    Input: (24419, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12210, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 3...\n    Input: (24419, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12210, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 4...\n    Input: (24419, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12210, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 5...\n    Input: (1416, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12210, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12210/12210 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12210, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12210, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12210, 3) -&gt; (12210, 5)\n  Merging dataframe 3: (12210, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12210, 5) -&gt; (12210, 7)\n  Merging dataframe 4: (12210, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12210, 7) -&gt; (12210, 11)\n  Merging dataframe 5: (12210, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12210, 11) -&gt; (12210, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12210 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12210/12210 non-NaN values\n    right_index_y_P1: 12210/12210 non-NaN values\n    right_index_z_P1: 12210/12210 non-NaN values\n    right_index_x_P2: 12210/12210 non-NaN values\n    right_index_y_P2: 12210/12210 non-NaN values\n    right_index_z_P2: 12210/12210 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/NoVisionMovement_Trial1.csv\n  Shape: (12210, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _NoVision_Movement _2\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 390886 samples\n✓ Prepared heart rate data: 24433 samples\n✓ Prepared respiration data: 24433 samples\n✓ Prepared EMG data: 24433 samples\nMotion tracking P1 shape: (680, 99)\nMotion tracking P2 shape: (733, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5939, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5886, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.212\nLSL time ranges - P2: 0.000 to 12.213\nLSL frame ranges - P1: 0.0 to 679.0\nLSL frame ranges - P2: 0.0 to 732.0\nUnique LSL mappings - P1: 680, P2: 733\nMotion frame ranges - P1: 0 to 679, P2: 0 to 732\nAfter LSL merge - P1: (680, 101), P2: (733, 101)\nValid timestamps after merge - P1: 680/680, P2: 733/733\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 680, P2: 733\nTime ranges after filtering - P1: 0.000 to 12.212\nTime ranges after filtering - P2: 0.000 to 12.208\nAfter participant merge: (1412, 7)\nValid times in merged data: 1412/1412\n✓ Prepared motion tracking data and found LSL: 1412 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (390886, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.215\n    Sample times: [0.0, 6.250031978633156e-05, 6.25003197879678e-05]\n  Modality 2: (24433, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.213\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24433, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.213\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24433, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.213\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1412, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.212\n    Sample times: [0.0, 0.012339879369392293, 0.012451252549908531]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12217 samples from 0.000s to 12.216s\n  Resampling modality 1...\n    Input: (390886, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12217, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12217/12217 non-NaN rows\n  Resampling modality 2...\n    Input: (24433, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12217, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12217/12217 non-NaN rows\n  Resampling modality 3...\n    Input: (24433, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12217, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12217/12217 non-NaN rows\n  Resampling modality 4...\n    Input: (24433, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12217, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12217/12217 non-NaN rows\n  Resampling modality 5...\n    Input: (1412, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12217, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12217/12217 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12217, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12217, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12217, 3) -&gt; (12217, 5)\n  Merging dataframe 3: (12217, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12217, 5) -&gt; (12217, 7)\n  Merging dataframe 4: (12217, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12217, 7) -&gt; (12217, 11)\n  Merging dataframe 5: (12217, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12217, 11) -&gt; (12217, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12217 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12217/12217 non-NaN values\n    right_index_y_P1: 12217/12217 non-NaN values\n    right_index_z_P1: 12217/12217 non-NaN values\n    right_index_x_P2: 12217/12217 non-NaN values\n    right_index_y_P2: 12217/12217 non-NaN values\n    right_index_z_P2: 12217/12217 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/NoVisionMovement_Trial2.csv\n  Shape: (12217, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _NoVision_Movement _3\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195326 samples\n✓ Prepared heart rate data: 24419 samples\n✓ Prepared respiration data: 24419 samples\n✓ Prepared EMG data: 24419 samples\nMotion tracking P1 shape: (681, 99)\nMotion tracking P2 shape: (732, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5936, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5883, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.206\nLSL time ranges - P2: 0.000 to 12.206\nLSL frame ranges - P1: 0.0 to 680.0\nLSL frame ranges - P2: 0.0 to 731.0\nUnique LSL mappings - P1: 681, P2: 732\nMotion frame ranges - P1: 0 to 680, P2: 0 to 731\nAfter LSL merge - P1: (681, 101), P2: (732, 101)\nValid timestamps after merge - P1: 681/681, P2: 732/732\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 681, P2: 732\nTime ranges after filtering - P1: 0.000 to 12.204\nTime ranges after filtering - P2: 0.000 to 12.206\nAfter participant merge: (1412, 7)\nValid times in merged data: 1412/1412\n✓ Prepared motion tracking data and found LSL: 1412 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195326, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.208\n    Sample times: [0.0, 6.250031997952131e-05, 0.000125000639959]\n  Modality 2: (24419, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 3: (24419, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 4: (24419, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 5: (1412, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.006169939684696146, 0.012451252549908531]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12209 samples from 0.000s to 12.208s\n  Resampling modality 1...\n    Input: (195326, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 2...\n    Input: (24419, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 3...\n    Input: (24419, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 4...\n    Input: (24419, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12209, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 5...\n    Input: (1412, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12209, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12209/12209 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12209, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12209, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12209, 3) -&gt; (12209, 5)\n  Merging dataframe 3: (12209, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12209, 5) -&gt; (12209, 7)\n  Merging dataframe 4: (12209, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12209, 7) -&gt; (12209, 11)\n  Merging dataframe 5: (12209, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12209, 11) -&gt; (12209, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12209 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12209/12209 non-NaN values\n    right_index_y_P1: 12209/12209 non-NaN values\n    right_index_z_P1: 12209/12209 non-NaN values\n    right_index_x_P2: 12209/12209 non-NaN values\n    right_index_y_P2: 12209/12209 non-NaN values\n    right_index_z_P2: 12209/12209 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/NoVisionMovement_Trial3.csv\n  Shape: (12209, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _NoVision_Movement _4\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195319 samples\n✓ Prepared heart rate data: 24419 samples\n✓ Prepared respiration data: 24419 samples\n✓ Prepared EMG data: 24419 samples\nMotion tracking P1 shape: (681, 99)\nMotion tracking P2 shape: (733, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5935, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.204\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 680.0\nLSL frame ranges - P2: 0.0 to 732.0\nUnique LSL mappings - P1: 681, P2: 733\nMotion frame ranges - P1: 0 to 680, P2: 0 to 732\nAfter LSL merge - P1: (681, 101), P2: (733, 101)\nValid timestamps after merge - P1: 681/681, P2: 733/733\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 681, P2: 733\nTime ranges after filtering - P1: 0.000 to 12.196\nTime ranges after filtering - P2: 0.000 to 12.194\nAfter participant merge: (1413, 7)\nValid times in merged data: 1413/1413\n✓ Prepared motion tracking data and found LSL: 1413 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195319, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 6.250031999098906e-05, 0.0001250006399819]\n  Modality 2: (24419, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24419, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24419, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1413, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.196\n    Sample times: [0.0, 0.002056646561868547, 0.016601670066847873]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12209 samples from 0.000s to 12.208s\n  Resampling modality 1...\n    Input: (195319, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 2...\n    Input: (24419, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 3...\n    Input: (24419, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 4...\n    Input: (24419, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12209, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 5...\n    Input: (1413, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12209, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12209/12209 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12209, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12209, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12209, 3) -&gt; (12209, 5)\n  Merging dataframe 3: (12209, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12209, 5) -&gt; (12209, 7)\n  Merging dataframe 4: (12209, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12209, 7) -&gt; (12209, 11)\n  Merging dataframe 5: (12209, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12209, 11) -&gt; (12209, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12209 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12209/12209 non-NaN values\n    right_index_y_P1: 12209/12209 non-NaN values\n    right_index_z_P1: 12209/12209 non-NaN values\n    right_index_x_P2: 12209/12209 non-NaN values\n    right_index_y_P2: 12209/12209 non-NaN values\n    right_index_z_P2: 12209/12209 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/NoVisionMovement_Trial4.csv\n  Shape: (12209, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _Vision_Movement _0\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 390666 samples\n✓ Prepared heart rate data: 24421 samples\n✓ Prepared respiration data: 24421 samples\n✓ Prepared EMG data: 24421 samples\nMotion tracking P1 shape: (688, 99)\nMotion tracking P2 shape: (735, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5937, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5885, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.208\nLSL time ranges - P2: 0.000 to 12.211\nLSL frame ranges - P1: 0.0 to 661.0\nLSL frame ranges - P2: 0.0 to 733.0\nUnique LSL mappings - P1: 662, P2: 734\nMotion frame ranges - P1: 0 to 687, P2: 0 to 734\nAfter LSL merge - P1: (688, 101), P2: (735, 101)\nValid timestamps after merge - P1: 662/688, P2: 734/735\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 662, P2: 734\nTime ranges after filtering - P1: 0.000 to 12.198\nTime ranges after filtering - P2: 0.000 to 12.202\nAfter participant merge: (1395, 7)\nValid times in merged data: 1395/1395\n✓ Prepared motion tracking data and found LSL: 1395 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (390666, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.208\n    Sample times: [0.0, 6.250031996641632e-05, 6.250031996805439e-05]\n  Modality 2: (24421, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24421, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24421, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1395, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.202\n    Sample times: [0.0, 0.004150417516029847, 0.012339879370301787]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12210 samples from 0.000s to 12.209s\n  Resampling modality 1...\n    Input: (390666, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12210, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 2...\n    Input: (24421, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12210, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 3...\n    Input: (24421, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12210, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 4...\n    Input: (24421, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12210, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 5...\n    Input: (1395, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12210, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12210/12210 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12210, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12210, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12210, 3) -&gt; (12210, 5)\n  Merging dataframe 3: (12210, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12210, 5) -&gt; (12210, 7)\n  Merging dataframe 4: (12210, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12210, 7) -&gt; (12210, 11)\n  Merging dataframe 5: (12210, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12210, 11) -&gt; (12210, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12210 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12210/12210 non-NaN values\n    right_index_y_P1: 12210/12210 non-NaN values\n    right_index_z_P1: 12210/12210 non-NaN values\n    right_index_x_P2: 12210/12210 non-NaN values\n    right_index_y_P2: 12210/12210 non-NaN values\n    right_index_z_P2: 12210/12210 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/VisionMovement_Trial0.csv\n  Shape: (12210, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _Vision_Movement _1\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195355 samples\n✓ Prepared heart rate data: 24423 samples\n✓ Prepared respiration data: 24423 samples\n✓ Prepared EMG data: 24423 samples\nMotion tracking P1 shape: (686, 99)\nMotion tracking P2 shape: (734, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5935, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.204\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 687.0\nLSL frame ranges - P2: 0.0 to 728.0\nUnique LSL mappings - P1: 688, P2: 729\nMotion frame ranges - P1: 0 to 685, P2: 0 to 733\nAfter LSL merge - P1: (686, 101), P2: (734, 101)\nValid timestamps after merge - P1: 686/686, P2: 729/734\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 686, P2: 729\nTime ranges after filtering - P1: 0.000 to 12.157\nTime ranges after filtering - P2: 0.000 to 12.196\nAfter participant merge: (1414, 7)\nValid times in merged data: 1414/1414\n✓ Prepared motion tracking data and found LSL: 1414 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195355, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.210\n    Sample times: [0.0, 6.250031993202085e-05, 0.000125000639864]\n  Modality 2: (24423, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.208\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722145271837]\n  Modality 3: (24423, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.208\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722145271837]\n  Modality 4: (24423, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.208\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722145271837]\n  Modality 5: (1414, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.196\n    Sample times: [0.0, 0.002075208758469671, 0.010283232807523746]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12211 samples from 0.000s to 12.210s\n  Resampling modality 1...\n    Input: (195355, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12211, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12211/12211 non-NaN rows\n  Resampling modality 2...\n    Input: (24423, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12211, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12211/12211 non-NaN rows\n  Resampling modality 3...\n    Input: (24423, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12211, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12211/12211 non-NaN rows\n  Resampling modality 4...\n    Input: (24423, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12211, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12211/12211 non-NaN rows\n  Resampling modality 5...\n    Input: (1414, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12211, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12211/12211 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12211, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12211, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12211, 3) -&gt; (12211, 5)\n  Merging dataframe 3: (12211, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12211, 5) -&gt; (12211, 7)\n  Merging dataframe 4: (12211, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12211, 7) -&gt; (12211, 11)\n  Merging dataframe 5: (12211, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12211, 11) -&gt; (12211, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12211 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12211/12211 non-NaN values\n    right_index_y_P1: 12211/12211 non-NaN values\n    right_index_z_P1: 12211/12211 non-NaN values\n    right_index_x_P2: 12211/12211 non-NaN values\n    right_index_y_P2: 12211/12211 non-NaN values\n    right_index_z_P2: 12211/12211 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/VisionMovement_Trial1.csv\n  Shape: (12211, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _Vision_Movement _2\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 390666 samples\n✓ Prepared heart rate data: 24420 samples\n✓ Prepared respiration data: 24420 samples\n✓ Prepared EMG data: 24420 samples\nMotion tracking P1 shape: (687, 99)\nMotion tracking P2 shape: (734, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5939, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5886, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.212\nLSL time ranges - P2: 0.000 to 12.213\nLSL frame ranges - P1: 0.0 to 679.0\nLSL frame ranges - P2: 0.0 to 732.0\nUnique LSL mappings - P1: 680, P2: 733\nMotion frame ranges - P1: 0 to 686, P2: 0 to 733\nAfter LSL merge - P1: (687, 101), P2: (734, 101)\nValid timestamps after merge - P1: 680/687, P2: 733/734\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 680, P2: 733\nTime ranges after filtering - P1: 0.000 to 12.212\nTime ranges after filtering - P2: 0.000 to 12.208\nAfter participant merge: (1412, 7)\nValid times in merged data: 1412/1412\n✓ Prepared motion tracking data and found LSL: 1412 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (390666, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.208\n    Sample times: [0.0, 6.250031996641632e-05, 6.250031996805439e-05]\n  Modality 2: (24420, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24420, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24420, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1412, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.212\n    Sample times: [0.0, 0.012339879369392293, 0.012451252549908531]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12214 samples from 0.000s to 12.213s\n  Resampling modality 1...\n    Input: (390666, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12214, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12214/12214 non-NaN rows\n  Resampling modality 2...\n    Input: (24420, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12214, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12214/12214 non-NaN rows\n  Resampling modality 3...\n    Input: (24420, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12214, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12214/12214 non-NaN rows\n  Resampling modality 4...\n    Input: (24420, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12214, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12214/12214 non-NaN rows\n  Resampling modality 5...\n    Input: (1412, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12214, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12214/12214 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12214, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12214, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12214, 3) -&gt; (12214, 5)\n  Merging dataframe 3: (12214, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12214, 5) -&gt; (12214, 7)\n  Merging dataframe 4: (12214, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12214, 7) -&gt; (12214, 11)\n  Merging dataframe 5: (12214, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12214, 11) -&gt; (12214, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12214 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12214/12214 non-NaN values\n    right_index_y_P1: 12214/12214 non-NaN values\n    right_index_z_P1: 12214/12214 non-NaN values\n    right_index_x_P2: 12214/12214 non-NaN values\n    right_index_y_P2: 12214/12214 non-NaN values\n    right_index_z_P2: 12214/12214 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/VisionMovement_Trial2.csv\n  Shape: (12214, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _Vision_Movement _3\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 1\n✓ Prepared envelope data: 390586 samples\n✓ Prepared heart rate data: 24415 samples\n✓ Prepared respiration data: 24415 samples\n✓ Prepared EMG data: 24415 samples\n✗ Expected 2 motion tracking files, found 1\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 4\n  Modality 1: (390586, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 6.250032003195199e-05, 6.250032003359072e-05]\n  Modality 2: (24415, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.204\n    Sample times: [0.0, 0.0009997719216698897, 0.000999772216346173]\n  Modality 3: (24415, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.204\n    Sample times: [0.0, 0.0009997719216698897, 0.000999772216346173]\n  Modality 4: (24415, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.204\n    Sample times: [0.0, 0.0009997719216698897, 0.000999772216346173]\n\nCreating common time grid for 4 modalities...\n✓ Common time grid created: 12207 samples from 0.000s to 12.206s\n  Resampling modality 1...\n    Input: (390586, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12207, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12207/12207 non-NaN rows\n  Resampling modality 2...\n    Input: (24415, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12207, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12207/12207 non-NaN rows\n  Resampling modality 3...\n    Input: (24415, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12207, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12207/12207 non-NaN rows\n  Resampling modality 4...\n    Input: (24415, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12207, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12207/12207 non-NaN rows\n\n📊 Starting final merge of 4 resampled dataframes...\n  Base dataframe: (12207, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12207, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12207, 3) -&gt; (12207, 5)\n  Merging dataframe 3: (12207, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12207, 5) -&gt; (12207, 7)\n  Merging dataframe 4: (12207, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12207, 7) -&gt; (12207, 11)\n🎉 Final merged data: 12207 samples, 11 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n❌ NO MOTION TRACKING COLUMNS FOUND IN FINAL DATA!\n   This suggests the motion data was lost during resampling or merging\n✓ Trial data saved to: ../merged_filteredtimeseries/VisionMovement_Trial3.csv\n  Shape: (12207, 14)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _Vision_Movement _4\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195262 samples\n✓ Prepared heart rate data: 24411 samples\n✓ Prepared respiration data: 24411 samples\n✓ Prepared EMG data: 24411 samples\nMotion tracking P1 shape: (662, 99)\nMotion tracking P2 shape: (735, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5935, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.204\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 680.0\nLSL frame ranges - P2: 0.0 to 732.0\nUnique LSL mappings - P1: 681, P2: 733\nMotion frame ranges - P1: 0 to 661, P2: 0 to 734\nAfter LSL merge - P1: (662, 101), P2: (735, 101)\nValid timestamps after merge - P1: 662/662, P2: 733/735\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 662, P2: 733\nTime ranges after filtering - P1: 0.000 to 11.857\nTime ranges after filtering - P2: 0.000 to 12.194\nAfter participant merge: (1394, 7)\nValid times in merged data: 1394/1394\n✓ Prepared motion tracking data and found LSL: 1394 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195262, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.204\n    Sample times: [0.0, 6.250032008439985e-05, 0.0001250006401687]\n  Modality 2: (24411, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.202\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722145271837]\n  Modality 3: (24411, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.202\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722145271837]\n  Modality 4: (24411, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.202\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722145271837]\n  Modality 5: (1394, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.194\n    Sample times: [0.0, 0.002056646561868547, 0.016601670066847873]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12205 samples from 0.000s to 12.204s\n  Resampling modality 1...\n    Input: (195262, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12205, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12205/12205 non-NaN rows\n  Resampling modality 2...\n    Input: (24411, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12205, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12205/12205 non-NaN rows\n  Resampling modality 3...\n    Input: (24411, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12205, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12205/12205 non-NaN rows\n  Resampling modality 4...\n    Input: (24411, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12205, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12205/12205 non-NaN rows\n  Resampling modality 5...\n    Input: (1394, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12205, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12205/12205 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12205, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12205, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12205, 3) -&gt; (12205, 5)\n  Merging dataframe 3: (12205, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12205, 5) -&gt; (12205, 7)\n  Merging dataframe 4: (12205, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12205, 7) -&gt; (12205, 11)\n  Merging dataframe 5: (12205, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12205, 11) -&gt; (12205, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12205 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12205/12205 non-NaN values\n    right_index_y_P1: 12205/12205 non-NaN values\n    right_index_z_P1: 12205/12205 non-NaN values\n    right_index_x_P2: 12205/12205 non-NaN values\n    right_index_y_P2: 12205/12205 non-NaN values\n    right_index_z_P2: 12205/12205 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/VisionMovement_Trial4.csv\n  Shape: (12205, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _NoVision_NoMovement _0\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195321 samples\n✓ Prepared heart rate data: 24419 samples\n✓ Prepared respiration data: 24419 samples\n✓ Prepared EMG data: 24419 samples\nMotion tracking P1 shape: (649, 99)\nMotion tracking P2 shape: (723, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5935, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.204\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 648.0\nLSL frame ranges - P2: 0.0 to 722.0\nUnique LSL mappings - P1: 649, P2: 723\nMotion frame ranges - P1: 0 to 648, P2: 0 to 722\nAfter LSL merge - P1: (649, 101), P2: (723, 101)\nValid timestamps after merge - P1: 649/649, P2: 723/723\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 649, P2: 723\nTime ranges after filtering - P1: 0.000 to 12.198\nTime ranges after filtering - P2: 0.000 to 12.198\nAfter participant merge: (1371, 7)\nValid times in merged data: 1371/1371\n✓ Prepared motion tracking data and found LSL: 1371 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195321, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.208\n    Sample times: [0.0, 6.250031998771248e-05, 0.0001250006399754]\n  Modality 2: (24419, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24419, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24419, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1371, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.198\n    Sample times: [0.0, 0.01439652593126084, 0.01660167006593838]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12209 samples from 0.000s to 12.208s\n  Resampling modality 1...\n    Input: (195321, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 2...\n    Input: (24419, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 3...\n    Input: (24419, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 4...\n    Input: (24419, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12209, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 5...\n    Input: (1371, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12209, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12209/12209 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12209, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12209, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12209, 3) -&gt; (12209, 5)\n  Merging dataframe 3: (12209, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12209, 5) -&gt; (12209, 7)\n  Merging dataframe 4: (12209, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12209, 7) -&gt; (12209, 11)\n  Merging dataframe 5: (12209, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12209, 11) -&gt; (12209, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12209 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12209/12209 non-NaN values\n    right_index_y_P1: 12209/12209 non-NaN values\n    right_index_z_P1: 12209/12209 non-NaN values\n    right_index_x_P2: 12209/12209 non-NaN values\n    right_index_y_P2: 12209/12209 non-NaN values\n    right_index_z_P2: 12209/12209 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/NoVisionNoMovement_Trial0.csv\n  Shape: (12209, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _NoVision_NoMovement _1\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 245321 samples\n✓ Prepared heart rate data: 24419 samples\n✓ Prepared respiration data: 24419 samples\n✓ Prepared EMG data: 24419 samples\nMotion tracking P1 shape: (648, 99)\nMotion tracking P2 shape: (731, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5935, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.204\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 647.0\nLSL frame ranges - P2: 0.0 to 730.0\nUnique LSL mappings - P1: 648, P2: 731\nMotion frame ranges - P1: 0 to 647, P2: 0 to 730\nAfter LSL merge - P1: (648, 101), P2: (731, 101)\nValid timestamps after merge - P1: 648/648, P2: 731/731\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 648, P2: 731\nTime ranges after filtering - P1: 0.000 to 12.190\nTime ranges after filtering - P2: 0.000 to 12.200\nAfter participant merge: (1378, 7)\nValid times in merged data: 1378/1378\n✓ Prepared motion tracking data and found LSL: 1378 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (245321, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.208\n    Sample times: [0.0, 6.25003199860742e-05, 6.250031998771248e-05]\n  Modality 2: (24419, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24419, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24419, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1378, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.200\n    Sample times: [0.0, 0.01439652593126084, 0.014526461307468708]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12209 samples from 0.000s to 12.208s\n  Resampling modality 1...\n    Input: (245321, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 2...\n    Input: (24419, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 3...\n    Input: (24419, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 4...\n    Input: (24419, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12209, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 5...\n    Input: (1378, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12209, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12209/12209 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12209, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12209, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12209, 3) -&gt; (12209, 5)\n  Merging dataframe 3: (12209, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12209, 5) -&gt; (12209, 7)\n  Merging dataframe 4: (12209, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12209, 7) -&gt; (12209, 11)\n  Merging dataframe 5: (12209, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12209, 11) -&gt; (12209, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12209 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12209/12209 non-NaN values\n    right_index_y_P1: 12209/12209 non-NaN values\n    right_index_z_P1: 12209/12209 non-NaN values\n    right_index_x_P2: 12209/12209 non-NaN values\n    right_index_y_P2: 12209/12209 non-NaN values\n    right_index_z_P2: 12209/12209 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/NoVisionNoMovement_Trial1.csv\n  Shape: (12209, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _NoVision_NoMovement _2\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 390662 samples\n✓ Prepared heart rate data: 24421 samples\n✓ Prepared respiration data: 24421 samples\n✓ Prepared EMG data: 24421 samples\nMotion tracking P1 shape: (655, 99)\nMotion tracking P2 shape: (733, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5936, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.206\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 654.0\nLSL frame ranges - P2: 0.0 to 732.0\nUnique LSL mappings - P1: 655, P2: 733\nMotion frame ranges - P1: 0 to 654, P2: 0 to 732\nAfter LSL merge - P1: (655, 101), P2: (733, 101)\nValid timestamps after merge - P1: 655/655, P2: 733/733\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 655, P2: 733\nTime ranges after filtering - P1: 0.000 to 12.192\nTime ranges after filtering - P2: 0.000 to 12.204\nAfter participant merge: (1387, 7)\nValid times in merged data: 1387/1387\n✓ Prepared motion tracking data and found LSL: 1387 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (390662, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.208\n    Sample times: [0.0, 6.250031996969247e-05, 6.250031997133056e-05]\n  Modality 2: (24421, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 3: (24421, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 4: (24421, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 5: (1387, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.204\n    Sample times: [0.0, 0.008226586246564693, 0.014526461307468708]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12210 samples from 0.000s to 12.209s\n  Resampling modality 1...\n    Input: (390662, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12210, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 2...\n    Input: (24421, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12210, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 3...\n    Input: (24421, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12210, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 4...\n    Input: (24421, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12210, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12210/12210 non-NaN rows\n  Resampling modality 5...\n    Input: (1387, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12210, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12210/12210 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12210, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12210, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12210, 3) -&gt; (12210, 5)\n  Merging dataframe 3: (12210, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12210, 5) -&gt; (12210, 7)\n  Merging dataframe 4: (12210, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12210, 7) -&gt; (12210, 11)\n  Merging dataframe 5: (12210, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12210, 11) -&gt; (12210, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12210 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12210/12210 non-NaN values\n    right_index_y_P1: 12210/12210 non-NaN values\n    right_index_z_P1: 12210/12210 non-NaN values\n    right_index_x_P2: 12210/12210 non-NaN values\n    right_index_y_P2: 12210/12210 non-NaN values\n    right_index_z_P2: 12210/12210 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/NoVisionNoMovement_Trial2.csv\n  Shape: (12210, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _NoVision_NoMovement _3\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 390604 samples\n✓ Prepared heart rate data: 24416 samples\n✓ Prepared respiration data: 24416 samples\n✓ Prepared EMG data: 24416 samples\nMotion tracking P1 shape: (639, 99)\nMotion tracking P2 shape: (734, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5935, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.204\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 638.0\nLSL frame ranges - P2: 0.0 to 733.0\nUnique LSL mappings - P1: 639, P2: 734\nMotion frame ranges - P1: 0 to 638, P2: 0 to 733\nAfter LSL merge - P1: (639, 101), P2: (734, 101)\nValid timestamps after merge - P1: 639/639, P2: 734/734\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 639, P2: 734\nTime ranges after filtering - P1: 0.000 to 12.198\nTime ranges after filtering - P2: 0.000 to 12.200\nAfter participant merge: (1372, 7)\nValid times in merged data: 1372/1372\n✓ Prepared motion tracking data and found LSL: 1372 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (390604, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 6.250032001720413e-05, 6.250032001884271e-05]\n  Modality 2: (24416, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.205\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24416, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.205\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24416, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.205\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1372, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.200\n    Sample times: [0.0, 0.006169939684696146, 0.00830083503296919]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12208 samples from 0.000s to 12.207s\n  Resampling modality 1...\n    Input: (390604, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12208, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12208/12208 non-NaN rows\n  Resampling modality 2...\n    Input: (24416, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12208, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12208/12208 non-NaN rows\n  Resampling modality 3...\n    Input: (24416, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12208, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12208/12208 non-NaN rows\n  Resampling modality 4...\n    Input: (24416, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12208, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12208/12208 non-NaN rows\n  Resampling modality 5...\n    Input: (1372, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12208, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12208/12208 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12208, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12208, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12208, 3) -&gt; (12208, 5)\n  Merging dataframe 3: (12208, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12208, 5) -&gt; (12208, 7)\n  Merging dataframe 4: (12208, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12208, 7) -&gt; (12208, 11)\n  Merging dataframe 5: (12208, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12208, 11) -&gt; (12208, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12208 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12208/12208 non-NaN values\n    right_index_y_P1: 12208/12208 non-NaN values\n    right_index_z_P1: 12208/12208 non-NaN values\n    right_index_x_P2: 12208/12208 non-NaN values\n    right_index_y_P2: 12208/12208 non-NaN values\n    right_index_z_P2: 12208/12208 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/NoVisionNoMovement_Trial3.csv\n  Shape: (12208, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _NoVision_NoMovement _4\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 390748 samples\n✓ Prepared heart rate data: 24425 samples\n✓ Prepared respiration data: 24425 samples\n✓ Prepared EMG data: 24425 samples\nMotion tracking P1 shape: (615, 99)\nMotion tracking P2 shape: (734, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5937, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5884, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.208\nLSL time ranges - P2: 0.000 to 12.208\nLSL frame ranges - P1: 0.0 to 614.0\nLSL frame ranges - P2: 0.0 to 733.0\nUnique LSL mappings - P1: 615, P2: 734\nMotion frame ranges - P1: 0 to 614, P2: 0 to 733\nAfter LSL merge - P1: (615, 101), P2: (734, 101)\nValid timestamps after merge - P1: 615/615, P2: 734/734\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 615, P2: 734\nTime ranges after filtering - P1: 0.000 to 12.196\nTime ranges after filtering - P2: 0.000 to 12.198\nAfter participant merge: (1348, 7)\nValid times in merged data: 1348/1348\n✓ Prepared motion tracking data and found LSL: 1348 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (390748, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.211\n    Sample times: [0.0, 6.250031989927012e-05, 6.25003199009075e-05]\n  Modality 2: (24425, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.209\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24425, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.209\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24425, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.209\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1348, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.198\n    Sample times: [0.0, 0.00830083503296919, 0.012339879369392293]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12212 samples from 0.000s to 12.211s\n  Resampling modality 1...\n    Input: (390748, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12212, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12212/12212 non-NaN rows\n  Resampling modality 2...\n    Input: (24425, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12212, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12212/12212 non-NaN rows\n  Resampling modality 3...\n    Input: (24425, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12212, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12212/12212 non-NaN rows\n  Resampling modality 4...\n    Input: (24425, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12212, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12212/12212 non-NaN rows\n  Resampling modality 5...\n    Input: (1348, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12212, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12212/12212 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12212, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12212, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12212, 3) -&gt; (12212, 5)\n  Merging dataframe 3: (12212, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12212, 5) -&gt; (12212, 7)\n  Merging dataframe 4: (12212, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12212, 7) -&gt; (12212, 11)\n  Merging dataframe 5: (12212, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12212, 11) -&gt; (12212, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12212 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12212/12212 non-NaN values\n    right_index_y_P1: 12212/12212 non-NaN values\n    right_index_z_P1: 12212/12212 non-NaN values\n    right_index_x_P2: 12212/12212 non-NaN values\n    right_index_y_P2: 12212/12212 non-NaN values\n    right_index_z_P2: 12212/12212 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/NoVisionNoMovement_Trial4.csv\n  Shape: (12212, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _Vision_NoMovement _0\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195316 samples\n✓ Prepared heart rate data: 24418 samples\n✓ Prepared respiration data: 24418 samples\n✓ Prepared EMG data: 24418 samples\nMotion tracking P1 shape: (683, 99)\nMotion tracking P2 shape: (733, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5935, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.204\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 648.0\nLSL frame ranges - P2: 0.0 to 722.0\nUnique LSL mappings - P1: 649, P2: 723\nMotion frame ranges - P1: 0 to 682, P2: 0 to 732\nAfter LSL merge - P1: (683, 101), P2: (733, 101)\nValid timestamps after merge - P1: 649/683, P2: 723/733\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 649, P2: 723\nTime ranges after filtering - P1: 0.000 to 12.198\nTime ranges after filtering - P2: 0.000 to 12.198\nAfter participant merge: (1371, 7)\nValid times in merged data: 1371/1371\n✓ Prepared motion tracking data and found LSL: 1371 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195316, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.207\n    Sample times: [0.0, 6.250031999590406e-05, 0.0001250006399918]\n  Modality 2: (24418, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24418, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24418, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.206\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1371, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.198\n    Sample times: [0.0, 0.01439652593126084, 0.01660167006593838]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12209 samples from 0.000s to 12.208s\n  Resampling modality 1...\n    Input: (195316, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 2...\n    Input: (24418, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 3...\n    Input: (24418, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12209, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 4...\n    Input: (24418, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12209, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12209/12209 non-NaN rows\n  Resampling modality 5...\n    Input: (1371, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12209, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12209/12209 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12209, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12209, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12209, 3) -&gt; (12209, 5)\n  Merging dataframe 3: (12209, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12209, 5) -&gt; (12209, 7)\n  Merging dataframe 4: (12209, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12209, 7) -&gt; (12209, 11)\n  Merging dataframe 5: (12209, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12209, 11) -&gt; (12209, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12209 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12209/12209 non-NaN values\n    right_index_y_P1: 12209/12209 non-NaN values\n    right_index_z_P1: 12209/12209 non-NaN values\n    right_index_x_P2: 12209/12209 non-NaN values\n    right_index_y_P2: 12209/12209 non-NaN values\n    right_index_z_P2: 12209/12209 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/VisionNoMovement_Trial0.csv\n  Shape: (12209, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _Vision_NoMovement _1\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195412 samples\n✓ Prepared heart rate data: 24430 samples\n✓ Prepared respiration data: 24430 samples\n✓ Prepared EMG data: 24430 samples\nMotion tracking P1 shape: (649, 99)\nMotion tracking P2 shape: (732, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5935, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.204\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 647.0\nLSL frame ranges - P2: 0.0 to 730.0\nUnique LSL mappings - P1: 648, P2: 731\nMotion frame ranges - P1: 0 to 648, P2: 0 to 731\nAfter LSL merge - P1: (649, 101), P2: (732, 101)\nValid timestamps after merge - P1: 648/649, P2: 731/732\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 648, P2: 731\nTime ranges after filtering - P1: 0.000 to 12.190\nTime ranges after filtering - P2: 0.000 to 12.200\nAfter participant merge: (1378, 7)\nValid times in merged data: 1378/1378\n✓ Prepared motion tracking data and found LSL: 1378 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195412, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.213\n    Sample times: [0.0, 6.250031983869894e-05, 0.0001250006396773]\n  Modality 2: (24430, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.212\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 3: (24430, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.212\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 4: (24430, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.212\n    Sample times: [0.0, 0.0009997719234888791, 0.0009997722154366784]\n  Modality 5: (1378, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.200\n    Sample times: [0.0, 0.01439652593126084, 0.014526461307468708]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12215 samples from 0.000s to 12.214s\n  Resampling modality 1...\n    Input: (195412, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12215, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12215/12215 non-NaN rows\n  Resampling modality 2...\n    Input: (24430, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12215, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12215/12215 non-NaN rows\n  Resampling modality 3...\n    Input: (24430, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12215, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12215/12215 non-NaN rows\n  Resampling modality 4...\n    Input: (24430, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12215, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12215/12215 non-NaN rows\n  Resampling modality 5...\n    Input: (1378, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12215, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12215/12215 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12215, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12215, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12215, 3) -&gt; (12215, 5)\n  Merging dataframe 3: (12215, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12215, 5) -&gt; (12215, 7)\n  Merging dataframe 4: (12215, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12215, 7) -&gt; (12215, 11)\n  Merging dataframe 5: (12215, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12215, 11) -&gt; (12215, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12215 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12215/12215 non-NaN values\n    right_index_y_P1: 12215/12215 non-NaN values\n    right_index_z_P1: 12215/12215 non-NaN values\n    right_index_x_P2: 12215/12215 non-NaN values\n    right_index_y_P2: 12215/12215 non-NaN values\n    right_index_z_P2: 12215/12215 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/VisionNoMovement_Trial1.csv\n  Shape: (12215, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _Vision_NoMovement _2\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195276 samples\n✓ Prepared heart rate data: 24413 samples\n✓ Prepared respiration data: 24413 samples\n✓ Prepared EMG data: 24413 samples\nMotion tracking P1 shape: (682, 99)\nMotion tracking P2 shape: (733, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5936, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.206\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 654.0\nLSL frame ranges - P2: 0.0 to 732.0\nUnique LSL mappings - P1: 655, P2: 733\nMotion frame ranges - P1: 0 to 681, P2: 0 to 732\nAfter LSL merge - P1: (682, 101), P2: (733, 101)\nValid timestamps after merge - P1: 655/682, P2: 733/733\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 655, P2: 733\nTime ranges after filtering - P1: 0.000 to 12.192\nTime ranges after filtering - P2: 0.000 to 12.204\nAfter participant merge: (1387, 7)\nValid times in merged data: 1387/1387\n✓ Prepared motion tracking data and found LSL: 1387 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195276, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.205\n    Sample times: [0.0, 6.25003200614518e-05, 0.0001250006401229]\n  Modality 2: (24413, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.203\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24413, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.203\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24413, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.203\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1387, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.204\n    Sample times: [0.0, 0.008226586246564693, 0.014526461307468708]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12206 samples from 0.000s to 12.205s\n  Resampling modality 1...\n    Input: (195276, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12206, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12206/12206 non-NaN rows\n  Resampling modality 2...\n    Input: (24413, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12206, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12206/12206 non-NaN rows\n  Resampling modality 3...\n    Input: (24413, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12206, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12206/12206 non-NaN rows\n  Resampling modality 4...\n    Input: (24413, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12206, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12206/12206 non-NaN rows\n  Resampling modality 5...\n    Input: (1387, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12206, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12206/12206 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12206, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12206, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12206, 3) -&gt; (12206, 5)\n  Merging dataframe 3: (12206, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12206, 5) -&gt; (12206, 7)\n  Merging dataframe 4: (12206, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12206, 7) -&gt; (12206, 11)\n  Merging dataframe 5: (12206, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12206, 11) -&gt; (12206, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12206 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12206/12206 non-NaN values\n    right_index_y_P1: 12206/12206 non-NaN values\n    right_index_z_P1: 12206/12206 non-NaN values\n    right_index_x_P2: 12206/12206 non-NaN values\n    right_index_y_P2: 12206/12206 non-NaN values\n    right_index_z_P2: 12206/12206 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/VisionNoMovement_Trial2.csv\n  Shape: (12206, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _Vision_NoMovement _3\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195460 samples\n✓ Prepared heart rate data: 24436 samples\n✓ Prepared respiration data: 24436 samples\n✓ Prepared EMG data: 24436 samples\nMotion tracking P1 shape: (685, 99)\nMotion tracking P2 shape: (731, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5935, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5882, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.204\nLSL time ranges - P2: 0.000 to 12.204\nLSL frame ranges - P1: 0.0 to 638.0\nLSL frame ranges - P2: 0.0 to 733.0\nUnique LSL mappings - P1: 639, P2: 734\nMotion frame ranges - P1: 0 to 684, P2: 0 to 730\nAfter LSL merge - P1: (685, 101), P2: (731, 101)\nValid timestamps after merge - P1: 639/685, P2: 731/731\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 639, P2: 731\nTime ranges after filtering - P1: 0.000 to 12.198\nTime ranges after filtering - P2: 0.000 to 12.150\nAfter participant merge: (1369, 7)\nValid times in merged data: 1369/1369\n✓ Prepared motion tracking data and found LSL: 1369 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195460, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.216\n    Sample times: [0.0, 6.250031976015431e-05, 0.0001250006395203]\n  Modality 2: (24436, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.215\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 3: (24436, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.215\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 4: (24436, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.215\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722154366784]\n  Modality 5: (1369, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.198\n    Sample times: [0.0, 0.006169939684696146, 0.00830083503296919]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12218 samples from 0.000s to 12.217s\n  Resampling modality 1...\n    Input: (195460, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12218, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12218/12218 non-NaN rows\n  Resampling modality 2...\n    Input: (24436, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12218, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12218/12218 non-NaN rows\n  Resampling modality 3...\n    Input: (24436, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12218, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12218/12218 non-NaN rows\n  Resampling modality 4...\n    Input: (24436, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12218, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12218/12218 non-NaN rows\n  Resampling modality 5...\n    Input: (1369, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12218, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12218/12218 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12218, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12218, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12218, 3) -&gt; (12218, 5)\n  Merging dataframe 3: (12218, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12218, 5) -&gt; (12218, 7)\n  Merging dataframe 4: (12218, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12218, 7) -&gt; (12218, 11)\n  Merging dataframe 5: (12218, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12218, 11) -&gt; (12218, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12218 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12218/12218 non-NaN values\n    right_index_y_P1: 12218/12218 non-NaN values\n    right_index_z_P1: 12218/12218 non-NaN values\n    right_index_x_P2: 12218/12218 non-NaN values\n    right_index_y_P2: 12218/12218 non-NaN values\n    right_index_z_P2: 12218/12218 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/VisionNoMovement_Trial3.csv\n  Shape: (12218, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\n\nProcessing _Vision_NoMovement _4\nFound files - Envelope: 2, HR: 2, Resp: 2, EMG: 2, MT: 2\n✓ Prepared envelope data: 195273 samples\n✓ Prepared heart rate data: 24412 samples\n✓ Prepared respiration data: 24412 samples\n✓ Prepared EMG data: 24412 samples\nMotion tracking P1 shape: (684, 99)\nMotion tracking P2 shape: (734, 99)\n✓ Found LSL files - processing with time alignment...\nLSL P1 shape: (5937, 2), columns: ['LSL_Time', 'Video_P1_Channel 1']\nLSL P2 shape: (5884, 2), columns: ['LSL_Time', 'Video_P2_Channel 1']\nLSL time ranges - P1: 0.000 to 12.208\nLSL time ranges - P2: 0.000 to 12.208\nLSL frame ranges - P1: 0.0 to 614.0\nLSL frame ranges - P2: 0.0 to 733.0\nUnique LSL mappings - P1: 615, P2: 734\nMotion frame ranges - P1: 0 to 683, P2: 0 to 733\nAfter LSL merge - P1: (684, 101), P2: (734, 101)\nValid timestamps after merge - P1: 615/684, P2: 734/734\nAvailable motion positions - P1: ['right_index_x', 'right_index_y', 'right_index_z']\nAvailable motion positions - P2: ['right_index_x', 'right_index_y', 'right_index_z']\nAfter time filtering - P1: 615, P2: 734\nTime ranges after filtering - P1: 0.000 to 12.196\nTime ranges after filtering - P2: 0.000 to 12.198\nAfter participant merge: (1348, 7)\nValid times in merged data: 1348/1348\n✓ Prepared motion tracking data and found LSL: 1348 samples\n  Motion columns: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n\n🔍 FINAL MERGE DEBUG START\nNumber of modalities to merge: 5\n  Modality 1: (195273, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Time range: 0.000 to 12.205\n    Sample times: [0.0, 6.250032006636896e-05, 0.0001250006401327]\n  Modality 2: (24412, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Time range: 0.000 to 12.203\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722145271837]\n  Modality 3: (24412, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Time range: 0.000 to 12.203\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722145271837]\n  Modality 4: (24412, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Time range: 0.000 to 12.203\n    Sample times: [0.0, 0.0009997719225793844, 0.0009997722145271837]\n  Modality 5: (1348, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Time range: 0.000 to 12.198\n    Sample times: [0.0, 0.00830083503296919, 0.012339879369392293]\n\nCreating common time grid for 5 modalities...\n✓ Common time grid created: 12206 samples from 0.000s to 12.205s\n  Resampling modality 1...\n    Input: (195273, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Output: (12206, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n    Data quality: 12206/12206 non-NaN rows\n  Resampling modality 2...\n    Input: (24412, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Output: (12206, 3) - Columns: ['Time', 'Filtered_ECG_P1', 'Filtered_ECG_P2']\n    Data quality: 12206/12206 non-NaN rows\n  Resampling modality 3...\n    Input: (24412, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Output: (12206, 3) - Columns: ['Time', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    Data quality: 12206/12206 non-NaN rows\n  Resampling modality 4...\n    Input: (24412, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Output: (12206, 5) - Columns: ['Time', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    Data quality: 12206/12206 non-NaN rows\n  Resampling modality 5...\n    Input: (1348, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Output: (12206, 7) - Columns: ['Time', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    🎯 Motion columns found: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    Data quality: 12206/12206 non-NaN rows\n\n📊 Starting final merge of 5 resampled dataframes...\n  Base dataframe: (12206, 3) - Columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2']\n  Merging dataframe 2: (12206, 3)\n    Columns to add: ['Filtered_ECG_P1', 'Filtered_ECG_P2']\n    After merge: (12206, 3) -&gt; (12206, 5)\n  Merging dataframe 3: (12206, 3)\n    Columns to add: ['Filtered_Respiration_P1', 'Filtered_Respiration_P2']\n    After merge: (12206, 5) -&gt; (12206, 7)\n  Merging dataframe 4: (12206, 5)\n    Columns to add: ['Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2']\n    After merge: (12206, 7) -&gt; (12206, 11)\n  Merging dataframe 5: (12206, 7)\n    Columns to add: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    After merge: (12206, 11) -&gt; (12206, 17)\n    🎯 Motion columns in merged data: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎉 Final merged data: 12206 samples, 17 columns\nAll columns: ['Time', 'Amplitude_Envelope_P1', 'Amplitude_Envelope_P2', 'Filtered_ECG_P1', 'Filtered_ECG_P2', 'Filtered_Respiration_P1', 'Filtered_Respiration_P2', 'Filtered_EMG_Bicep_P1', 'Filtered_EMG_Tricep_P1', 'Filtered_EMG_Bicep_P2', 'Filtered_EMG_Tricep_P2', 'right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n🎯 MOTION TRACKING COLUMNS FOUND: ['right_index_x_P1', 'right_index_y_P1', 'right_index_z_P1', 'right_index_x_P2', 'right_index_y_P2', 'right_index_z_P2']\n    right_index_x_P1: 12206/12206 non-NaN values\n    right_index_y_P1: 12206/12206 non-NaN values\n    right_index_z_P1: 12206/12206 non-NaN values\n    right_index_x_P2: 12206/12206 non-NaN values\n    right_index_y_P2: 12206/12206 non-NaN values\n    right_index_z_P2: 12206/12206 non-NaN values\n✓ Trial data saved to: ../merged_filteredtimeseries/VisionNoMovement_Trial4.csv\n  Shape: (12206, 20)\n  Time alignment check - First 5 timestamps: [0.0, 0.001, 0.002, 0.003, 0.004]\n  Time step consistency: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n🔍 FINAL MERGE DEBUG END\n\nProcessing complete!\n\n\n\n# analyze and produce plots\n\nSeries([], Name: Time, dtype: object)"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#define-variables",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_Movement\\trial_1\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#d-plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_1/trial_1.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#define-variables",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_Movement\\trial_3\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#d-plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_Movement/trial_3/trial_3.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html",
    "title": "Load necessary libraries",
    "section": "",
    "text": "First, we are importing libraries (also known as “packages”) that we will use throughout this notebook. In this case, we are using pathlib, numpy, and plotly. If they are not installed, we will install them.\nPathlib provides methods to handle file and directory paths, numpy allows us to handle arrays, and plotly is the graphing library we will use for visualization.\nIf any of these libraries are not installed, we install them directly from this notebook.\nfrom pathlib import Path\n\ntry:\n    import numpy as np\nexcept Exception as e:\n    print(e)\n    %pip install numpy\n    import numpy as np\n\n\ntry:\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go\nexcept Exception as e:\n    print(e)\n    %pip install plotly\n    from plotly.subplots import make_subplots\n    import plotly.graph_objects as go"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#define-variables",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#define-variables",
    "title": "Load necessary libraries",
    "section": "Define variables",
    "text": "Define variables\nNext, we’re defining a few things we’ll need to use later:\n\npath_to_recording: This is a path to the folder where the mocap data for this recording is stored\njoint_to_plot: We need to tell the program which joint we want to visualize. By default, we select the ‘nose’.\nmediapipe_indices: These are the possible joints that can be visualized.\n\nYou can select a different joint from mediapipe_indices to view the plot for that - for example, you could replace joint_to_plot = 'nose' with joint_to_plot = 'left_elbow' to view the trajectory visualization for the left elbow.\nWe’re also getting the path to two types of data we need - ‘center of mass’ and ‘3D body data’. We will be loading and using these datasets later on.\n\npath_to_recording = \"F:\\Mobile-Multimodal-Lab\\3_MOTION_TRACKING\\3_freemocap\\marker_MULTIPLEpairs\\P1\\NoVision_NoMovement\\trial_4\"\n\n\n#pick a joint from the mediapipe indices list to plot\njoint_to_plot = 'nose'\n\n\n\nmediapipe_indices = ['nose',\n    'left_eye_inner',\n    'left_eye',\n    'left_eye_outer',\n    'right_eye_inner',\n    'right_eye',\n    'right_eye_outer',\n    'left_ear',\n    'right_ear',\n    'mouth_left',\n    'mouth_right',\n    'left_shoulder',\n    'right_shoulder',\n    'left_elbow',\n    'right_elbow',\n    'left_wrist',\n    'right_wrist',\n    'left_pinky',\n    'right_pinky',\n    'left_index',\n    'right_index',\n    'left_thumb',\n    'right_thumb',\n    'left_hip',\n    'right_hip',\n    'left_knee',\n    'right_knee',\n    'left_ankle',\n    'right_ankle',\n    'left_heel',\n    'right_heel',\n    'left_foot_index',\n    'right_foot_index']\n\njoint_to_plot_index = mediapipe_indices.index(joint_to_plot)\n\n\npath_to_recording = Path(path_to_recording)\npath_to_center_of_mass_npy = path_to_recording/'output_data'/'center_of_mass'/'total_body_center_of_mass_xyz.npy'\npath_to_freemocap_3d_body_data_npy = path_to_recording/'output_data'/'mediapipe_body_3d_xyz.npy'\n\nfreemocap_3d_body_data = np.load(path_to_freemocap_3d_body_data_npy)\ntotal_body_com_data = np.load(path_to_center_of_mass_npy)\n\nfreemocap_3d_body_data_to_plot = freemocap_3d_body_data[:,joint_to_plot_index,:]"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#plotting",
    "title": "Load necessary libraries",
    "section": "Plotting",
    "text": "Plotting\nAfter loading our data, we are going to create some plots to better visualize it. Specifically, we are plotting the trajectory of the total body center of mass and the trajectory of the chosen joint (nose by default, but you can replace that and rerun this notebook to plot a different trajectory).\nThe first three plots (in column 1) represent the X, Y, and Z trajectories of the total body center of mass. The next three plots (in column 2) represent the X, Y, and Z trajectories of the chosen joint.\nNote: The X, Y, and Z values refer to the three dimensions in space.\nYou can click and drag on the plots below to interact with them and zoom into certain areas. When hovering over the plot, you can see additional options in the top right to pan, zoom, reset, and download the plots.\n\n\nfig = make_subplots(rows=3, cols=2, subplot_titles=('total body center of mass trajectory',f'{joint_to_plot} trajectory'))\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,0]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,1]),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y = total_body_com_data[:,2]),\n    row=3, col=1\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,0]),\n    row=1, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,1]),\n    row=2, col=2\n)\n\nfig.add_trace(\n        go.Scatter(y = freemocap_3d_body_data_to_plot[:,2]),\n    row=3, col=2\n)\n\n#COM plot axes labels \nfig['layout']['yaxis']['title']='X Axis (mm)'\nfig['layout']['yaxis3']['title']='Y Axis (mm)'\nfig['layout']['yaxis5']['title']='Z Axis (mm)'\nfig['layout']['xaxis5']['title']='Frame #'\nfig['layout']['xaxis6']['title']='Frame #'\n\n\n\n\nfig.update_layout(height=600, width=800,showlegend=False)\nfig.show()"
  },
  {
    "objectID": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#d-plotting",
    "href": "4a_PROCESSED/raw_trials/marker_MULTIPLEpairs/P1/NoVision_NoMovement/trial_0/trial_4.html#d-plotting",
    "title": "Load necessary libraries",
    "section": "3D Plotting",
    "text": "3D Plotting\nFinally, we are creating a 3D plot of the skeleton movement, tracking all the joints in the mediapipe_indices list over time.\nThe ‘Play’ button at the bottom allows you to watch the motion as if it were a video. Before pressing play, you can manually click and drag the plot around to orient the view of the plot.\n\n\ndef calculate_axes_means(skeleton_3d_data):\n    mx_skel = np.nanmean(skeleton_3d_data[:,0:33,0])\n    my_skel = np.nanmean(skeleton_3d_data[:,0:33,1])\n    mz_skel = np.nanmean(skeleton_3d_data[:,0:33,2])\n\n    return mx_skel, my_skel, mz_skel\n\nax_range = 1500\n\nmx_skel, my_skel, mz_skel = calculate_axes_means(freemocap_3d_body_data)\n\n# Create a list of frames\nframes = [go.Frame(data=[go.Scatter3d(\n    x=freemocap_3d_body_data[i, :, 0],\n    y=freemocap_3d_body_data[i, :, 1],\n    z=freemocap_3d_body_data[i, :, 2],\n    mode='markers',\n    marker=dict(\n        size=2,  # Adjust marker size as needed\n    )\n)], name=str(i)) for i in range(freemocap_3d_body_data.shape[0])]\n\n# Define axis properties\naxis = dict(\n    showbackground=True,\n    backgroundcolor=\"rgb(230, 230,230)\",\n    gridcolor=\"rgb(255, 255, 255)\",\n    zerolinecolor=\"rgb(255, 255, 255)\",\n)\n\n# Create a figure\nfig = go.Figure(\n    data=[go.Scatter3d(\n        x=freemocap_3d_body_data[0, :, 0],\n        y=freemocap_3d_body_data[0, :, 1],\n        z=freemocap_3d_body_data[0, :, 2],\n        mode='markers',\n        marker=dict(\n            size=2,  # Adjust marker size as needed\n        )\n    )],\n    layout=go.Layout(\n        scene=dict(\n            xaxis=dict(axis, range=[mx_skel-ax_range, mx_skel+ax_range]), # Adjust range as needed\n            yaxis=dict(axis, range=[my_skel-ax_range, my_skel+ax_range]), # Adjust range as needed\n            zaxis=dict(axis, range=[mz_skel-ax_range, mz_skel+ax_range]),  # Adjust range as needed\n            aspectmode='cube'\n        ),\n        updatemenus=[dict(\n            type='buttons',\n            showactive=False,\n            buttons=[dict(\n                label='Play',\n                method='animate',\n                args=[None, {\"frame\": {\"duration\": 30}}]\n            )]\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()"
  },
  {
    "objectID": "5_ANIMATIONS/Donders_MML_Multimodal_Animations.html",
    "href": "5_ANIMATIONS/Donders_MML_Multimodal_Animations.html",
    "title": "DONDERS MML: Multimodal Animations",
    "section": "",
    "text": "import cv2 #opencv\nimport math #basic operations\nimport numpy as np #basic operations\nimport pandas as pd #data wrangling\nimport csv #csv saving\nimport os #some basic functions for inspecting folder structure etc.\nfrom os import listdir\nfrom os.path import isfile, join\nimport glob as glob\nimport moviepy.editor as mop\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.animation import FuncAnimation\nimport tkinter # GUI toolkit to open and save files\nfrom tkinter import filedialog, messagebox  # GUI toolkit to open and save files\nfrom scipy.signal import butter, filtfilt, iirnotch\nimport librosa\nimport librosa.display\nimport tempfile\nimport shutil\nimport tqdm\n\n\nprint(\"Everything imported successfully\")\n\nEverything imported successfully",
    "crumbs": [
      "Visualization",
      "DONDERS MML: Multimodal Animations"
    ]
  },
  {
    "objectID": "5_ANIMATIONS/Donders_MML_Multimodal_Animations.html#extract-amplitude-envelope-and-plot-audio-signal",
    "href": "5_ANIMATIONS/Donders_MML_Multimodal_Animations.html#extract-amplitude-envelope-and-plot-audio-signal",
    "title": "DONDERS MML: Multimodal Animations",
    "section": "4. Extract Amplitude Envelope and Plot Audio Signal",
    "text": "4. Extract Amplitude Envelope and Plot Audio Signal\n\n# Define the bandpass filter\ndef butter_bandpass(lowcut, highcut, fs, order=2):\n    nyquist = 0.5 * fs\n    low = lowcut / nyquist\n    high = highcut / nyquist\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\ndef butter_bandpass_filtfilt(data, lowcut, highcut, fs, order=2):\n    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n    y = filtfilt(b, a, data)\n    return y\n\n# Define the lowpass filter\ndef butter_lowpass(cutoff, fs, order=2):\n    nyquist = 0.5 * fs\n    normal_cutoff = cutoff / nyquist\n    b, a = butter(order, normal_cutoff, btype='low')\n    return b, a\n\ndef butter_lowpass_filtfilt(data, cutoff, fs, order=2):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = filtfilt(b, a, data)\n    return y\n\n# Function to extract amplitude envelope\ndef amp_envelope(audiofilename):\n    # load audio with librosa\n    audio, sr = librosa.load(audiofilename, sr=None)\n    # Bandpass filter 400-4000Hz\n    data = butter_bandpass_filtfilt(audio, 400, 4000, sr, order=2)\n    # Lowpass filter 10Hz\n    data = butter_lowpass_filtfilt(np.abs(data), 10, sr, order=2)\n    # scale from 0 to 1\n    data = (data - np.min(data)) / (np.max(data) - np.min(data))\n    return data, sr\n\n# Get the amplitude envelope\nampv, sr = amp_envelope(audio_input)\n# Plot the filtered signal and the original signal\nplt.figure(figsize=(14, 5))\nplt.subplot(2, 1, 1)\nplt.plot(ampv)\nplt.title('Vocalic energy envelope')\n\n# Extract and plot the original signal with the amplitude envelope\nrawaudio, sr = librosa.load(audio_input, sr=None)\nplt.subplot(2, 1, 2)\nplt.plot(rawaudio, label = 'Original Signal', color='blue', linewidth=0.2)\nplt.plot(ampv, label='Amplitude Envelope', color='red', linewidth=1)\nplt.title('Original Signal with Vocalic Energy Envelope')\n\nplt.subplots_adjust(hspace=0.5)  # Increase the space between subplots to move title down\nplt.show()\n\n\n# Create a Audio DataFrame with time (in secs, starting from 0) raw audio and amplitude envelope for later use\naudio_df = pd.DataFrame({'time': np.arange(len(ampv)) / sr, 'audio': rawaudio, 'envelope': ampv})\naudio_df.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime\naudio\nenvelope\n\n\n\n\n0\n0.000000\n-0.000031\n0.003748\n\n\n1\n0.000063\n-0.000031\n0.003749\n\n\n2\n0.000125\n-0.000031\n0.003749\n\n\n3\n0.000188\n-0.000031\n0.003749\n\n\n4\n0.000250\n-0.000031\n0.003749",
    "crumbs": [
      "Visualization",
      "DONDERS MML: Multimodal Animations"
    ]
  },
  {
    "objectID": "5_ANIMATIONS/Donders_MML_Multimodal_Animations.html#filter-and-plot-physiological-signal-emg-ecg-and-resp",
    "href": "5_ANIMATIONS/Donders_MML_Multimodal_Animations.html#filter-and-plot-physiological-signal-emg-ecg-and-resp",
    "title": "DONDERS MML: Multimodal Animations",
    "section": "5. Filter and Plot Physiological Signal: EMG, ECG and RESP",
    "text": "5. Filter and Plot Physiological Signal: EMG, ECG and RESP\n\nPLUX_data = pd.read_csv(PLUX_input)\nprint(PLUX_data.head())  # To see the first few rows of the DataFrame\nprint(PLUX_data.columns)  # To list the column names\n\nsampling_rate = 1000 # Sampling rate of 1000 Hz for our PLUX stream \n\n## ----------------  1. EMG DATA PROCESSING ---------------- ## \n\n# Define Butterworth filter function\ndef butter_filter(data, cutoff, fs, order=4, filter_type='low'):\n    nyquist = 0.5 * fs  # Nyquist frequency\n    normal_cutoff = cutoff / nyquist\n    b, a = butter(order, normal_cutoff, btype=filter_type, analog=False)\n    # Apply zero-phase filtering with padding to prevent edge effects\n    padded_data = np.pad(data, (1000, 1000), 'edge')\n    filtered_data = filtfilt(b, a, padded_data)\n    return filtered_data[1000:-1000]  # Remove padding\n\n# High-pass filter, rectify, and then low-pass filter EMG signals\ndef process_emg(emg_signal, fs, cutoff_high, cutoff_low):\n    # Apply high-pass filter\n    high_passed = butter_filter(emg_signal, cutoff_high, fs, order=4, filter_type='high')\n    # Rectify (full-wave rectification)\n    rectified = np.abs(high_passed)\n    # Apply low-pass filter\n    low_passed = butter_filter(rectified, cutoff_low, fs, order=4, filter_type='low')\n    return low_passed\n\n# Your EMG data and sampling rate\nEMG_bicep = PLUX_data.iloc[:, 3].values\nEMG_tricep = PLUX_data.iloc[:, 4].values\nsampling_rate = 1000  # Replace with your actual sampling rate\n\n# Process the EMG data for each muscle\nEMG_bicep_processed = process_emg(EMG_bicep, sampling_rate, 2, 20)     # 2 Hz high-pass, 20 Hz low-pass\nEMG_tricep_processed = process_emg(EMG_tricep, sampling_rate, 2, 20)   # 2 Hz high-pass, 20 Hz low-pass\n\n# Compute frequency spectra for EMG bicep\nfreqs_bicep = np.fft.rfftfreq(len(EMG_bicep), d=1/sampling_rate)\nfft_bicep = np.abs(np.fft.rfft(EMG_bicep))\nfft_bicep_processed = np.abs(np.fft.rfft(EMG_bicep_processed))\n\n# Compute frequency spectra for EMG tricep\nfreqs_tricep = np.fft.rfftfreq(len(EMG_tricep), d=1/sampling_rate)\nfft_tricep = np.abs(np.fft.rfft(EMG_tricep))\nfft_tricep_processed = np.abs(np.fft.rfft(EMG_tricep_processed))\n\n# Visualization: Create a figure with 4 subplots\nplt.figure(figsize=(12, 12))\n\n# Subplot 1: Time-domain EMG data before filtering\nplt.subplot(4, 1, 1)\nplt.plot(PLUX_data['LSL_Time'], EMG_bicep, label='Bicep (Unfiltered)', color='blue', linewidth=0.5)\nplt.plot(PLUX_data['LSL_Time'], EMG_tricep, label='Tricep (Unfiltered)', color='red', linewidth=0.5)\nplt.legend()\nplt.xlabel('LSL Time (ms)')\nplt.ylabel('Amplitude')\nplt.title('EMG Data Unfiltered: Bicep and Tricep')\nplt.grid(True)\n\n# Subplot 2: Time-domain EMG data after filtering\nplt.subplot(4, 1, 2)\nplt.plot(PLUX_data['LSL_Time'], EMG_bicep_processed, label='Bicep (Filtered)', color='blue', linewidth=0.5)\nplt.plot(PLUX_data['LSL_Time'], EMG_tricep_processed, label='Tricep (Filtered)', color='red', linewidth=0.5)\nplt.legend()\nplt.xlabel('LSL Time (ms)')\nplt.ylabel('Amplitude')\nplt.title('EMG Data Filtered: Bicep and Tricep')\nplt.grid(True)\n\n# Subplot 3: Frequency spectrum for Bicep EMG\nplt.subplot(4, 1, 3)\nplt.plot(freqs_bicep, fft_bicep, label='Bicep Unfiltered', color='blue', linewidth=0.5)\nplt.plot(freqs_bicep, fft_bicep_processed, label='Bicep Filtered', color='red', linewidth=0.5)\nplt.xlim(0, 250)  # Adjust frequency range as needed\nplt.ylim(0, max(fft_bicep))  # Adjust y-axis limits as needed\nplt.legend()\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Magnitude')\nplt.title('Frequency Spectrum before and after Filtering - Bicep EMG')\nplt.grid(True)\n\n# Subplot 4: Frequency spectrum for Tricep EMG\nplt.subplot(4, 1, 4)\nplt.plot(freqs_tricep, fft_tricep, label='Tricep Unfiltered', color='blue', linewidth=0.5)\nplt.plot(freqs_tricep, fft_tricep_processed, label='Tricep Filtered', color='red', linewidth=0.5)\nplt.xlim(0, 250)\nplt.ylim(0, max(fft_tricep))  # Adjust y-axis limits as needed\nplt.legend()\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Magnitude')\nplt.title('Frequency Spectrum before and after Filtering- Tricep EMG')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Create a new ECG DataFrame with time (in secs) and the processed EMG signal for Bicep and Tricep\nEMG_df = pd.DataFrame({'time': np.arange(len(EMG_bicep)) / sampling_rate, 'emg_bicep': EMG_bicep_processed, 'emg_tricep': EMG_tricep_processed})\n\n\n\n## ----------------  2. ECG DATA PROCESSING ---------------- ##\n\n# Define the notch filter function to remove powerline interference\ndef notch_filter(signal, fs, notch_freq=50, quality_factor=30):\n    b, a = iirnotch(notch_freq / (fs / 2), quality_factor)\n    filtered_signal = filtfilt(b, a, signal)\n    return filtered_signal\n\n# Updated ECG processing function with notch filter\ndef process_ecg(ecg_signal, fs, cutoff_high, cutoff_low, notch_freq, quality_factor):\n    # Apply high-pass filter\n    high_passed = butter_filter(ecg_signal, cutoff_high, fs, order=4, filter_type='high')\n    # Apply low-pass filter\n    low_passed = butter_filter(high_passed, cutoff_low, fs, order=4, filter_type='low')\n    # Apply notch filter\n    filtered = notch_filter(low_passed, fs, notch_freq, quality_factor)\n    return filtered\n\n# Your ECG data and sampling rate\nECG = PLUX_data.iloc[:,2]\n\n# Process the ECG signal with the notch filter\nECG_processed = process_ecg(ECG, sampling_rate, 0.5, 40, 50, 30)  # 0.5 Hz high-pass, 40 Hz low-pass, 50 Hz notch, 30 Q-factor\n\n\n# Visualtiation with matplotlib: ECG data before and after filtering + Furier Transform frequency (3 subplots)\nplt.figure(figsize=(12, 8))\n\nplt.subplot(3, 1, 1)\nplt.plot(PLUX_data['LSL_Time'], ECG, label='ECG (unfiltered)', color='blue', linewidth=0.5)\nplt.legend()\nplt.xlabel('LSL Time (ms)')\nplt.title('ECG Data Unfiltered')\n\nplt.subplot(3, 1, 2)\nplt.plot(PLUX_data['LSL_Time'], ECG_processed, label='ECG (filtered with Notch)', color='green', linewidth=0.5)\nplt.legend()\nplt.xlabel('LSL Time (ms)')\nplt.title('ECG Data Filtered with Notch Filter')\n\n# Optional: Plot frequency spectrum to visualize the effect of the notch filter\nfreqs = np.fft.rfftfreq(len(ECG), d=1/sampling_rate)\nfft_ecg = np.abs(np.fft.rfft(ECG))\nfft_ecg_processed = np.abs(np.fft.rfft(ECG_processed))\n\nplt.subplot(3, 1, 3)\nplt.plot(freqs, fft_ecg, label='Original ECG Spectrum', color='blue', linewidth=0.5)\nplt.plot(freqs, fft_ecg_processed, label='Filtered ECG Spectrum', color='green', linewidth=0.5)\nplt.xlim(0, 100)  # Focus on frequencies up to 100 Hz\nplt.legend()\nplt.xlabel('Frequency (Hz)')\nplt.title('Frequency Spectrum Before and After Bandpass + Notch Filter')\n\nplt.tight_layout()\nplt.show()\n\n\n## Create a new ECG DataFrame with time (in secs) and the processed ECG signal\nECG_df = pd.DataFrame({'time': np.arange(len(ECG_processed))/ sampling_rate, 'ecg': ECG_processed})\n\n\n## ----------------  3. RESPIRATION DATA PROCESSING ---------------- ##\n\nRSP = PLUX_data.iloc[:,5]\n\n\n# visualtiation with matplotlib: Unfiltered respiration data\nplt.figure(figsize=(12, 6))\nplt.plot(PLUX_data['LSL_Time'], RSP, label='Respiration (unfiltered)', color='blue', linewidth=0.5)\nplt.legend()\nplt.xlabel('LSL Time (ms seconds)')\nplt.title('Respiration Data Unfiltered')\nplt.show()\n\n# Create a new Respiration DataFrame with time (in secs) and the raw respiration signal\nRESP_df = pd.DataFrame({'time': np.arange(len(RSP)) / sampling_rate, 'resp': RSP})\nprint(RESP_df.head())\n\n\n      LSL_Time  PLUX_P1_nSeq  PLUX_P1_ECG0  PLUX_P1_EMG1  PLUX_P1_EMG2  \\\n0  6700.956065     1383405.0       0.04393       0.00477      -0.00291   \n1  6700.957064     1383406.0       0.04236       0.00118      -0.00259   \n2  6700.958064     1383407.0       0.04142       0.00095       0.00195   \n3  6700.959064     1383408.0       0.03994       0.00009       0.00254   \n4  6700.960064     1383409.0       0.03306       0.00722       0.00023   \n\n   PLUX_P1_RESPIRATION3  \n0               0.28427  \n1               0.28230  \n2               0.28038  \n3               0.28725  \n4               0.29068  \nIndex(['LSL_Time', 'PLUX_P1_nSeq', 'PLUX_P1_ECG0', 'PLUX_P1_EMG1',\n       'PLUX_P1_EMG2', 'PLUX_P1_RESPIRATION3'],\n      dtype='object')\n    time     resp\n0  0.000  0.28427\n1  0.001  0.28230\n2  0.002  0.28038\n3  0.003  0.28725\n4  0.004  0.29068",
    "crumbs": [
      "Visualization",
      "DONDERS MML: Multimodal Animations"
    ]
  },
  {
    "objectID": "5_ANIMATIONS/Donders_MML_Multimodal_Animations.html#create-the-multimodal-animation",
    "href": "5_ANIMATIONS/Donders_MML_Multimodal_Animations.html#create-the-multimodal-animation",
    "title": "DONDERS MML: Multimodal Animations",
    "section": "5. Create the Multimodal Animation",
    "text": "5. Create the Multimodal Animation\n\n# what is the window size in seconds\nwindow = 4\n\n\ndef plot_multimodal(audio, emg, ecg, resp, midpoint, window=4):\n    # Temporary folder for saving the plot\n    tempfolder = tempfile.mkdtemp()\n\n    # Create 4 subplots for the different data streams\n    fig, ax = plt.subplots(4, 1, figsize=(14, 16))  # 4 rows (Audio, EMG, ECG, RESP)\n\n    # Define the time window for all plots\n    start = -window / 2\n    end = window / 2\n\n    # Create local time variables adjusted for the current midpoint\n    audio_time = audio['time'] - midpoint\n    emg_time = emg['time'] - midpoint\n    ecg_time = ecg['time'] - midpoint\n    resp_time = resp['time'] - midpoint\n\n    # Filter the data to the time window\n    audio_mask = (audio_time &gt;= start) & (audio_time &lt;= end)\n    emg_mask = (emg_time &gt;= start) & (emg_time &lt;= end)\n    ecg_mask = (ecg_time &gt;= start) & (ecg_time &lt;= end)\n    resp_mask = (resp_time &gt;= start) & (resp_time &lt;= end)\n\n    # Plot 1: Audio envelope\n    ax[0].plot(audio_time.loc[audio_mask], audio['audio'].loc[audio_mask], label='Raw Audio', linewidth=0.5)\n    ax[0].plot(audio_time.loc[audio_mask], audio['envelope'].loc[audio_mask], label='Envelope', linewidth=1.5)\n    ax[0].set_xlim(start, end)\n    ax[0].set_ylim(min(audio['audio']), max(audio['envelope']))  # Adjust as needed based on your signal\n    ax[0].legend(prop={'size': 18})\n    ax[0].set_ylabel('Amplitude')\n\n    # Plot 2: Filtered EMG signal\n    ax[1].plot(emg_time.loc[emg_mask], emg['emg_bicep'].loc[emg_mask], label='EMG Bicep', color='green', linewidth=1)\n    ax[1].plot(emg_time.loc[emg_mask], emg['emg_tricep'].loc[emg_mask], label='EMG Tricep', color='red', linewidth=1)\n    ax[1].set_xlim(start, end)\n    ax[1].set_ylim(min(min(emg['emg_bicep']), min(emg['emg_tricep'])), max(max(emg['emg_bicep']), max(emg['emg_tricep'])))\n    ax[1].legend(prop={'size': 18})\n    ax[1].set_ylabel('Amplitude')\n\n    # Plot 3: Filtered ECG signal\n    ax[2].plot(ecg_time.loc[ecg_mask], ecg['ecg'].loc[ecg_mask], label='ECG Signal', color='blue', linewidth=1)\n    ax[2].set_xlim(start, end)\n    ax[2].set_ylim(min(ecg['ecg']), max(ecg['ecg']))  # Adjust as needed based on your signal\n    ax[2].legend(prop={'size': 18})\n    ax[2].set_ylabel('Amplitude')\n\n    # Plot 4: Respiratory signal\n    ax[3].plot(resp_time.loc[resp_mask], resp['resp'].loc[resp_mask], label='Respiration', color='orange', linewidth=1)\n    ax[3].set_xlim(start, end)\n    ax[3].set_ylim(min(resp['resp']), max(resp['resp']))  # Adjust as needed based on your signal\n    ax[3].legend(prop={'size': 18})\n    ax[3].set_ylabel('Respiration')\n    ax[3].set_xlabel('Time (s)')\n\n\n    #Apply a vertical line at time 0 for all subplots\n    ax[0].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n    ax[1].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n    ax[2].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n    ax[3].axvline(x=0, color='r', linestyle='--', linewidth=4, alpha=0.8)\n\n    # Apply tight layout and save plot\n    plt.tight_layout()\n    tpf = tempfolder + '/tempfig.png'\n    plt.savefig(tpf)\n    plt.close()\n\n    # Load and return the image as a frame for the video\n    img = cv2.imread(tpf)\n    shutil.rmtree(tempfolder)\n    #show image in the terminal\n    return img\n\n\ndef process_video_with_multimodal(videos_2D, animation_3D, audio, emg, ecg, resp, output_filename):\n    # Open the three 2D videos\n    captures_2D = [cv2.VideoCapture(vf) for vf in videos_2D]\n    \n    # Open the 3D animation video\n    capture_3D = cv2.VideoCapture(animation_3D)\n    \n    # Check if all videos opened successfully\n    if not all([cap.isOpened() for cap in captures_2D]) or not capture_3D.isOpened():\n        print(\"Error opening one or more video files.\")\n        return\n    \n    # Get video properties of 2D videos (assuming all videos have the same properties)\n    fps = captures_2D[0].get(cv2.CAP_PROP_FPS)\n    frame_count = int(captures_2D[0].get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_width = int(captures_2D[0].get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(captures_2D[0].get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Get properties of the 3D animation video\n    frame_width_3D = int(capture_3D.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height_3D = int(capture_3D.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps_3D = capture_3D.get(cv2.CAP_PROP_FPS)\n    frame_count_3D = int(capture_3D.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # Ensure synchronization (assuming same fps and frame count)\n    if fps != fps_3D or frame_count != frame_count_3D:\n        print(\"Mismatch in FPS or frame count between videos.\")\n        return\n    \n    # Calculate dimensions for the video grid\n    # Top: 3 videos (side by side)\n    video_top_width = frame_width * 3\n    # video_top_height = frame_height - (frame_height // 3) # Reduce height by 1/3\n    video_top_height = frame_height  # Keep the same height\n\n    # Set the desired width of the 3D animation (e.g., half of video_top_width)\n    animation_width = int(video_top_width)  # Half the width of the top videos\n    # Maintain aspect ratio\n    scale_factor = animation_width / frame_width_3D\n    animation_height = int(frame_height_3D * scale_factor)\n    \n    # Video combined height\n    video_height = video_top_height + animation_height  # Top videos + 3D animation\n\n    # Create a sample plot to determine plot height\n    tempfolder = tempfile.mkdtemp()\n    fig, ax = plt.subplots(4, 1, figsize=(14, 16))\n    plt.tight_layout()\n    tpf = tempfolder + '/tempfig.png'\n    plt.savefig(tpf)\n    plt.close()\n    plot_img = cv2.imread(tpf)\n    plot_height, plot_width, _ = plot_img.shape\n    shutil.rmtree(tempfolder)\n    \n    # Total output frame dimensions\n    total_width = video_top_width\n    total_height = video_height + plot_height\n\n    print(f\"Total video dimensions: {total_width}x{total_height}\")\n    \n    # Set up video writer for the output video\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_filename, fourcc, fps, (total_width, total_height))\n\n    if not out.isOpened():\n        print(\"Error: VideoWriter not initialized correctly.\")\n        return\n    \n    frame_number = 0\n    for i in tqdm.tqdm(range(frame_count)):\n        # Read frames from the three 2D videos\n        frames_2D = []\n        for cap in captures_2D:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frames_2D.append(frame)\n        if len(frames_2D) != len(captures_2D):\n            break  # Not enough frames\n        \n        # Read frame from the 3D animation video\n        ret_3D, frame_3D = capture_3D.read()\n        if not ret_3D:\n            break\n        \n        # Resize 2D frames if needed\n        resized_frames_2D = [cv2.resize(f, (frame_width, frame_height)) for f in frames_2D]\n        \n        # Arrange the 3 2D videos side by side (top)\n        video_top = np.hstack(resized_frames_2D)\n        \n        # Resize the 3D animation frame\n        frame_3D_resized = cv2.resize(frame_3D, (animation_width, animation_height))\n        \n        # Create a blank image for the bottom part (same width as video_top)\n        bottom_image = np.zeros((animation_height, video_top_width, 3), dtype=np.uint8)\n        \n        # Center the 3D animation in the bottom_image\n        x_offset = (video_top_width - animation_width) // 2\n        y_offset = 0  # Top of bottom_image\n        bottom_image[y_offset:y_offset+animation_height, x_offset:x_offset+animation_width] = frame_3D_resized\n        \n        # Combine top and bottom images vertically\n        video_combined = np.vstack((video_top, bottom_image))\n        \n        # Generate the multimodal plot for the current window\n        midpoint = frame_number / fps\n        multimodal_img = plot_multimodal(audio, emg, ecg, resp, midpoint)\n        \n        # Resize the plot to match the width of the video_combined\n        multimodal_img = cv2.resize(multimodal_img, (total_width, plot_height))\n        \n        # Combine the video_combined and the plot vertically\n        combined_frame = np.vstack((video_combined, multimodal_img))\n        \n        # Write the combined frame to the output video\n        out.write(combined_frame.astype(np.uint8))\n        \n        frame_number += 1\n\n        ##show the frame (otpional)\n        #cv2.imshow('Frame', combined_frame)\n    \n    # Release video objects\n    for cap in captures_2D:\n        cap.release()\n    capture_3D.release()\n    out.release()\n    cv2.destroyAllWindows()\n\n\nanimation_input_3D = output_file\n\n\noutput_folder = os.path.abspath('./animation_videos_temp_x')\noutput_filename = os.path.join(output_folder, 'multimodal_video_5.mp4')\n\n#make sure the folder exists\nos.makedirs(output_folder, exist_ok=True)\n\n\nprocess_video_with_multimodal(videos_input_2D, animation_input_3D, audio_df, EMG_df, ECG_df, RESP_df, output_filename)\n\n\n\n\nTotal video dimensions: 1620x3532\n\n\n100%|██████████| 681/681 [07:31&lt;00:00,  1.51it/s]\n\n\n\nprint('audio start')\nprint(audio_df.head())\nprint('audio end')\nprint(audio_df.tail())\n\nprint('emg start')\nprint(EMG_df.head())\nprint('emg end')\nprint(EMG_df.tail())\n\nprint('ecg start')\nprint(ECG_df.head())\nprint('ecg end')\nprint(ECG_df.tail())\n\nprint('resp start')\nprint(RESP_df.head())\nprint('resp end')\nprint(RESP_df.tail())\n\n# Open one of your videos\ncap = cv2.VideoCapture(videos_input_2D[0])\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nvideo_duration = frame_count / fps\ncap.release()\n\nprint(f\"Video Duration: {video_duration} seconds\")\n\n\naudio start\n       time     audio  envelope\n0  0.000000 -0.000031  0.002007\n1  0.000063 -0.000031  0.002008\n2  0.000125 -0.000031  0.002008\n3  0.000188 -0.000031  0.002008\n4  0.000250 -0.000031  0.002009\naudio end\n             time     audio  envelope\n195400  12.212500  0.000153  0.002122\n195401  12.212563  0.000153  0.002122\n195402  12.212625  0.000153  0.002122\n195403  12.212687  0.000122  0.002122\n195404  12.212750  0.000122  0.002122\nemg start\n    time  emg_bicep  emg_tricep\n0  0.000   0.004012    0.004489\n1  0.001   0.004397    0.005017\n2  0.002   0.004785    0.005577\n3  0.003   0.005170    0.006165\n4  0.004   0.005551    0.006778\nemg end\n         time  emg_bicep  emg_tricep\n12210  12.210   0.003504    0.005443\n12211  12.211   0.003353    0.005263\n12212  12.212   0.003198    0.005067\n12213  12.213   0.003040    0.004857\n12214  12.214   0.002881    0.004635\necg start\n    time       ecg\n0  0.000 -0.037212\n1  0.001 -0.036089\n2  0.002 -0.034548\n3  0.003 -0.032619\n4  0.004 -0.030350\necg end\n         time       ecg\n12210  12.210 -0.051759\n12211  12.211 -0.050409\n12212  12.212 -0.048846\n12213  12.213 -0.047064\n12214  12.214 -0.045072\nresp start\n    time     resp\n0  0.000  0.49232\n1  0.001  0.49690\n2  0.002  0.49072\n3  0.003  0.48798\n4  0.004  0.49489\nresp end\n         time     resp\n12210  12.210 -0.09320\n12211  12.211 -0.09096\n12212  12.212 -0.09201\n12213  12.213 -0.09467\n12214  12.214 -0.09050\nVideo Duration: 12.25925925925926 seconds\n\n\n\n(min(audio_df['audio']), max(audio_df['envelope']))\n\n(-0.1820068359375, 1.0)\n\n\n\n# Get video properties of 2D videos (assuming all videos have the same properties)\ncaptures_2D = [cv2.VideoCapture(vf) for vf in videos_input_2D]\nfps = captures_2D[0].get(cv2.CAP_PROP_FPS)\nframe_count = int(captures_2D[0].get(cv2.CAP_PROP_FRAME_COUNT))\nframe_width = int(captures_2D[0].get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(captures_2D[0].get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nprint(frame_height)\n\n960",
    "crumbs": [
      "Visualization",
      "DONDERS MML: Multimodal Animations"
    ]
  },
  {
    "objectID": "index.html#about-project",
    "href": "index.html#about-project",
    "title": "Mobile Multimodal Lab",
    "section": "",
    "text": "MobileMultimodalLab (MMLab) is a project initiated by researchers at Donders Center for Cognition. It aims to provide a lab setup for anyone interested in studying multimodal interactive behaviour - including acoustics, body movement, muscle activity, eye movements, and so on.\nTo achieve this, we are working on a comprehensive coding library, accompanied by a practical manual, that shall help researchers to build their own MobileMultimodalLab. Our guiding principles are:\n\nOpen-source resources - All code and documentation is freely available to everyone\nLow-cost equipment - We want to build the setup with as little monetary cost as possible (i.e., less than 10K)\nPortable setup - The setup should be easily transportable across locations\n\n\n\n\nMobileMultimodalLab\n\n\nThe MML setup originally consists of\n\nmultiple frame-synced 2D cameras that allow for 3D motion tracking\nmultiple microphones for acoustic analysis\nmultiple physiological sensors for measuring heart rate, muscle activity, and respiration\n\nTo ensure that all the signals are synchronized, we use the Lab streaming layer (https://github.com/sccn/labstreaminglayer), a software that synchronizes different data streams with sub-millisecond precision, crucially simplifying the data collection process and subsequent processing.\n\n\n\nSetup\n\n\nAdditionally, the setup is build in a modular way, so that anyone can add or remove equipment and recording from the default setup as long as these devices are LSL compatible",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Mobile Multimodal Lab",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank …",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Mobile Multimodal Lab",
    "section": "Contact",
    "text": "Contact",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#how-to-cite",
    "href": "index.html#how-to-cite",
    "title": "Mobile Multimodal Lab",
    "section": "How to cite",
    "text": "How to cite",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#funding-and-support",
    "href": "index.html#funding-and-support",
    "title": "Mobile Multimodal Lab",
    "section": "Funding and support",
    "text": "Funding and support",
    "crumbs": [
      "Home"
    ]
  }
]